{"seg_0": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 1, "bbox": []}, "seg_1": {"summary": "Summary: This text segment lists the authors and their affiliations. Claudia Flores-Saviaga, Kashif Imteyaz, and Saiph Savage are affiliated with Northeastern University in the USA, while Benjamin V. Hanrahan and Steven Clarke are affiliated with Microsoft in the USA and United Kingdom, respectively.", "text": "CLAUDIA FLORES-SAVIAGA, Northeastern University, USA BENJAMIN V. HANRAHAN, Microsoft, USA KASHIF IMTEYAZ, Northeastern University, USA STEVEN CLARKE, Microsoft, United Kingdom SAIPH SAVAGE, Northeastern University, USA", "page": 1, "bbox": []}, "seg_3": {"summary": "Summary: Figure 1 provides an overview of research focused on understanding how developers with visual impairments interact with AI coding assistants.  This research aims to analyze the specific ways visually impaired developers use and engage with AI coding assistants.", "text": "Fig. 1. Overview of our research to analyze how developers who are visually impaired interact with AI coding assistants.", "page": 1, "bbox": []}, "seg_4": {"summary": "A study exploring the use of AI coding assistants by developers with visual impairments found that while beneficial, these tools also presented accessibility challenges, such as overwhelming suggestions and difficulty switching between AI-generated and user code.  Despite these issues, participants were optimistic about the potential of AI assistants, highlighting the need for activity-centered design to improve inclusivity and effectiveness for developers with visual impairments.  This approach can lead to more intuitive and accessible AI tools for software development.", "text": "The rapid adoption of generative AI in software development has impacted the industry, yet its effects on developers with visual impairments remain largely unexplored. To address this gap, we used an Activity Theory framework to examine how developers with visual impairments interact with AI coding assistants. For this purpose, we conducted a study where developers who are visually impaired completed a series of programming tasks using a generative AI coding assistant. We uncovered that, while participants found the AI assistant beneficial and reported significant advantages, they also highlighted accessibility challenges. Specifically, the AI coding assistant often exacerbated existing accessibility barriers and introduced new challenges. For example, it overwhelmed users with an excessive number of suggestions, leading developers who are visually impaired to express a desire for \u201cAI timeouts.\u201d Additionally, the generative AI coding assistant made it more difficult for developers to switch contexts between the AI-generated content and their own code. Despite these challenges, participants were optimistic about the potential of AI coding assistants to transform the coding experience for developers with visual impairments. Our findings emphasize the need to apply activity-centered design principles to generative AI assistants, ensuring they better align with user behaviors and address specific accessibility needs. This approach can enable the assistants to provide more intuitive, inclusive, and effective experiences, while also contributing to the broader goal of enhancing accessibility in software development.", "page": 1, "bbox": []}, "seg_5": {"summary": "Insufficient content for summary.", "text": "ACM Reference Format:", "page": 1, "bbox": []}, "seg_6": {"summary": "Summary: This research paper, authored by Flores-Saviaga, Hanrahan, Imteyaz, Clarke, and Savage, investigates the impact of generative AI coding assistants on developers with visual impairments.  Published in March 2025, the 25-page study is accessible via the provided DOI link.", "text": "Claudia Flores-Saviaga, Benjamin V. Hanrahan, Kashif Imteyaz, Steven Clarke, and Saiph Savage. 2025. The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired. 1, 1 (March 2025), 25 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn", "page": 1, "bbox": []}, "seg_7": {"summary": "Summary: This text segment lists the authors of a publication, along with their affiliations at Northeastern University and Microsoft, and their respective email addresses.  It provides contact information for each author.", "text": "Authors\u2019 addresses: Claudia Flores-Saviaga, floressaviaga.c@northeastern.edu, Northeastern University, USA; Benjamin V. Hanrahan, benhanrahan@ microsoft.com, Microsoft, USA; Kashif Imteyaz, imteyaz.k@northeastern.edu, Northeastern University, USA; Steven Clarke, stevencl@microsoft.com, Microsoft, United Kingdom; Saiph Savage, s.savage@northeastern.edu, Northeastern University, USA.", "page": 1, "bbox": []}, "seg_8": {"summary": "Summary: This text segment is a copyright notice from the Association for Computing Machinery (ACM) regarding permitted uses of their work.  It grants free permission for personal and classroom use, provided it is not for profit and includes proper citation.  Any other use, such as commercial redistribution or republication, requires explicit permission and potentially a fee, obtainable from ACM.", "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2025 Association for Computing Machinery.", "page": 1, "bbox": []}, "seg_9": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 1, "bbox": []}, "seg_10": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 1, "bbox": []}, "seg_13": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 2, "bbox": []}, "seg_14": {"summary": "Insufficient content for summary.", "text": "1 INTRODUCTION", "page": 2, "bbox": []}, "seg_15": {"summary": "Generative AI is transforming software development through coding assistants like GitHub Copilot, which offer functionalities such as auto-completion and code generation.  This technological shift raises an important question regarding the impact of these AI tools on developers with visual impairments. The text segment highlights the need to understand how this emerging technology affects accessibility and inclusivity in coding.", "text": "The integration of generative artificial intelligence (AI) into software development is fundamentally transforming coding practices [ 55 ]. AI coding assistants, such as GitHub Copilot [ 34 ], provide functionalities like auto-completion, code generation, and test generation [ 76 ], which have the potential to revolutionize how developers work [ 73 ]. Amidst this surge of new capabilities driven by AI, an important question emerges: How do these generative AI coding assistants impact developers with visual impairments?", "page": 2, "bbox": []}, "seg_16": {"summary": "AI's integration into coding offers opportunities to improve accessibility for developers with visual impairments by automating tasks and providing intelligent support.  However, if AI coding assistants are not designed with accessibility in mind, they risk creating new obstacles, especially for screen reader users.  Therefore, careful consideration of accessibility is crucial in the development of AI-powered coding tools.", "text": "The convergence of AI and accessibility in coding presents both tremendous opportunities and intricate challenges [ 55 , 67 ]. On one hand, AI has the potential to make coding more accessible for developers with visual impairments by reducing manual coding tasks and providing intelligent assistance [ 89 ]. On the other hand, if these AI coding assistants are not designed with accessibility in mind, they may inadvertently create new barriers or worsen existing ones [ 67 , 103 ]. This risk is especially high if the AI coding assistants depend heavily on visual cues or employ interaction methods incompatible with screen readers [ 61 , 86 , 109 ].", "page": 2, "bbox": []}, "seg_17": {"summary": "The accessibility challenges in AI assistants mirror those in web accessibility, particularly for visually impaired users, due to dynamic content and complex interactions.  Traditional accessibility guidelines may not fully address these issues in AI coding assistants, where AI-generated suggestions and interfaces can create new hurdles for visually impaired developers.  Further research is needed to understand and address these difficulties to make AI coding assistants truly accessible and user-friendly for all developers.", "text": "In this context, it is important to understand that the relationship between accessibility and usability in AI assistants mirrors longstanding challenges in web accessibility [ 38 , 51 ], particularly for visually impaired users [ 14 , 43 ]. Similar to rich internet applications [ 9 , 72 ], AI assistants often introduce dynamic content and complex interactions that traditional accessibility approaches may not fully address [ 39 , 47 ]. For instance, Petrie and Kheir [ 74 ] found that existing accessibility guidelines fail to capture many of the usability problems encountered by visually impaired users when interacting with dynamic content. This gap could be especially exacerbated in AI coding assistants [ 2 , 65 , 71 ], where AI-generated suggestions, real-time code generation, and sophisticated user interfaces can present new hurdles [ 39 , 41 ]. Just as screen readers struggle with dynamic web content, AJAX updates, and automatic refreshes [ 20 ], developers who are visually impaired may experience similar difficulties when interacting with rapidly changing AI-generated code suggestions and interface elements [ 54 ]. Consequently, simply following basic accessibility rules might not be enough to make AI coding assistants truly easy to use. We might need to rethink how to make these assistants more accessible for everyone. The challenge is that we do not fully understand the difficulties and benefits that visually impaired developers experience when using AI coding assistants. Closing this knowledge gap is important\u2014not just for accessibility, but also for making AI-assisted coding tools more user-friendly and effective for all developers, including those with different levels of vision.", "page": 2, "bbox": []}, "seg_18": {"summary": "Summary: This study investigates the unique challenges and opportunities that AI-assisted coding tools present for developers with visual impairments.  The research is driven by a question focused on understanding these challenges and opportunities.", "text": "Our study addresses this research gap by investigating what unique challenges and opportunities AI-assisted coding tools present for developers with visual impairments. Our research is driven by the following research question:", "page": 2, "bbox": []}, "seg_19": {"summary": "Summary: This research question explores the challenges that AI coding tools present to visually impaired developers.  It also investigates the potential of these tools to create new opportunities and improve the work of developers with visual impairments.", "text": "\u2022 RQ1: What challenges do AI-assisted coding tools pose for developers who are visually impaired, and what opportunities do they offer to empower and enhance their work?", "page": 2, "bbox": []}, "seg_20": {"summary": "This study uses the Activity Theory framework to investigate how visually impaired developers interact with the AI coding assistant GitHub Copilot. Researchers observed ten developers with varying experience levels as they completed a coding task using Copilot and conducted interviews afterwards. The study aimed to identify both the challenges and opportunities these developers encountered when using the AI assistant.", "text": "To study this question, we use the Activity Theory framework [ 44 ], which helps analyze how tools influence human activity and identifies contradictions that occur when a tool\u2019s design does not fit well with users\u2019 workflows and needs [ 12 ]. We applied this framework to examine a qualitative study we conducted with 10 visually impaired developers of varying experience levels. These developers used an AI coding assistant, GitHub Copilot, to complete a coding task. During the study, we observed their real-time interactions with the AI assistant, noting both challenges and opportunities. After the task, we conducted interviews to gain deeper insights into their experiences and the difficulties they faced while using the AI coding assistant.", "page": 2, "bbox": []}, "seg_21": {"summary": "Summary: This study reveals that AI coding assistants have a dual impact, enhancing coding efficiency while simultaneously posing new accessibility challenges.  Based on these findings, the authors propose a roadmap for developing the next generation of accessible AI tools. This research contributes to the understanding of the complex relationship between AI in coding and accessibility.", "text": "Our findings reveal a complex landscape where AI coding assistants can both improve coding efficiency and introduce new accessibility challenges. Based on our findings, we outline a roadmap for the next generation of accessible AI Manuscript submitted to ACM", "page": 2, "bbox": []}, "seg_23": {"summary": "Summary: This text segment will explore how generative AI coding assistants affect developers who are visually impaired. It will likely examine the specific ways these tools impact their coding experience.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 3, "bbox": []}, "seg_24": {"summary": "Summary: This text segment discusses research focused on the experiences of visually impaired developers using AI coding assistants. The goal of this research is to derive key design insights that can improve accessibility in AI-driven software development. Ultimately, these insights are intended to reshape how accessibility is approached in the development of AI-driven software.", "text": "coding assistants. By analyzing the experiences of visually impaired developers using AI-assisted coding tools, we derive key design insights that can reshape how accessibility is approached in AI-driven software development.", "page": 3, "bbox": []}, "seg_25": {"summary": "Insufficient content for summary.", "text": "The contributions of this paper are twofold:", "page": 3, "bbox": []}, "seg_26": {"summary": "Summary: This research offers a thorough examination of the accessibility of AI coding assistants, specifically looking at the challenges and advantages for developers who are visually impaired. The study is based on practical experiences and perspectives from visually impaired developers themselves.", "text": "\u2022 We conduct a comprehensive analysis of the accessibility challenges and benefits of AI coding assistants, using real-world insights from developers who are visually impaired.", "page": 3, "bbox": []}, "seg_27": {"summary": "Summary: This segment introduces design recommendations aimed at improving the accessibility and inclusivity of AI coding assistants.  The recommendations are specifically targeted towards developers with visual impairments.", "text": "\u2022 We propose a set of design recommendations to make AI coding assistants more accessible and inclusive for developers with visual impairments.", "page": 3, "bbox": []}, "seg_28": {"summary": "Insufficient content for summary.", "text": "2 RELATED WORK", "page": 3, "bbox": []}, "seg_29": {"summary": "Insufficient content for summary.", "text": "2.1 Accessibility and Tools for Developers who are Visually Impaired", "page": 3, "bbox": []}, "seg_30": {"summary": "Studies have consistently shown that visually impaired developers face significant challenges in coding. These challenges span multiple areas, including code navigation, comprehension, editing, debugging, and skimming, as identified by Mountapmbeme et al. Further research by Albusays and Ludi highlights ongoing difficulties in code navigation, diagrams, debugging, and UI layouts, leading to a preference for text editors over IDEs due to accessibility concerns.", "text": "Accessibility in general has been a subject of ongoing research [ 17 , 35 , 36 , 85 ]. Several studies have shed light on the significant challenges developers who are visually impaired face with coding and the techniques these programmers employ to overcome these challenges. Mountapmbeme et al. [ 63 ] categorized these challenges into five main areas: code navigation, code comprehension, code editing, code debugging, and code skimming. Albusays and Ludi [ 5 ] found persistent challenges in code navigation, accessing diagrams, debugging, and UI layout. Their study highlighted a strong preference for text editors over IDEs due to accessibility issues, reduced complexity, and better compatibility with assistive technologies.", "page": 3, "bbox": []}, "seg_31": {"summary": "Visually impaired developers often favor text editors over IDEs due to accessibility limitations, leading to unique coding practices.  Challenges include navigating code structure and debugging with visually oriented tools. To mitigate these issues, researchers have developed tools like StructJumper for code navigation and audio-based debuggers like Wicked Audio Debugger and CodeTalk, as well as collaborative tools like CodeWalk and Grid-Coding to improve coding environment accessibility.", "text": "Further studies by Mealin and Murphy-Hill [ 59 ] and Baker et al. [ 11 ] explored specific tools and techniques that developers who are visually impaired use. Mealin and Murphy-Hill found that many developers who are visually impaired rely on text editors rather than IDEs due to accessibility issues employing unique practices like \u201cout-of-context editing,\u201d where blocks of code are copied, edited separately, and pasted back. This preference for text editors has been corroborated by other researchers [ 6 , 63 ], who attribute it to specific challenges such as navigating line-by-line, understanding indentation, and managing nested code structures [ 56 , 99 ]. To address this issue, Baker et al. [ 11 ] created StructJumper, a plugin that generates a hierarchical tree structure of code for easier navigation. Debugging, presents another significant challenge for developers who are visually impaired, largely due to the reliance on visual interfaces in debugging tools, which screen readers struggle to interpret effectively [ 5 , 78 ]. These challenges have led the development of tools like Wicked Audio Debugger (WAD) [ 94 ] , a tool that provides audio descriptions of programs during execution; CodeTalk [ 78 ], a Visual Studio plugin that introduces \u201cTalkPoints\u201d for audio-based debugging; and CodeWalk, a tool by Potluri et al. [ 77 ], which facilitates accessible, remote, synchronous code review and refactoring activities by tethering collaborators\u2019 cursors with the host of a Live Share session. Additionally, Haque et al. [ 27 ] introduced Grid-Coding, a paradigm designed to improve the accessibility of coding environments by representing code in a structured 2D grid, allowing developers who are visually impaired to navigate and edit code more effectively.", "page": 3, "bbox": []}, "seg_32": {"summary": "Summary: Conversational interfaces, especially speech-based ones, show promise in improving comprehension and navigation. Studies have explored their effectiveness in reducing cognitive load through voice commands and their application in development environments using audio cues for tasks like debugging and file navigation.", "text": "Conversational interfaces have also shown promise [ 15 ], Ludi et al. [ 57 ], found that speech-based cues generally provided the best performance for comprehension and navigation tasks. Phutane et al. [ 75 ] explored the use of voice commands to reduce cognitive load. Meanwhile, Stefik et al. [ 95 ] created Sodbeans, an IDE that relies on audio cues for debugging, while Smith et al. [ 90 ] developed a tool to allow developers to navigate the tree structure of files in Eclipse.", "page": 3, "bbox": []}, "seg_33": {"summary": "Summary: Existing research on accessibility for visually impaired developers may not fully address the new challenges and opportunities presented by generative AI coding tools like GitHub Copilot.  The impact of these tools on accessible programming remains largely unexamined as most current studies predate their emergence.", "text": "While previous research has significantly advanced accessibility for developers who are visually impaired, these findings may not fully apply to the new landscape shaped by generative AI-assisted coding tools such as GitHub Copilot. The introduction of these tools brings new challenges and opportunities in accessible programming, an area that remains largely unexamined. The majority of current studies were conducted before the emergence of generative", "page": 3, "bbox": []}, "seg_34": {"summary": "Summary: A manuscript has been submitted to the ACM, likely for review and potential publication.", "text": "Manuscript submitted to ACM", "page": 3, "bbox": []}, "seg_36": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 4, "bbox": []}, "seg_37": {"summary": "Summary:  A significant knowledge gap exists concerning the effects of AI coding tools on developers with visual impairments.  This lack of understanding hinders our ability to assess the impact and possibilities of these tools for visually impaired programmers.", "text": "AI in coding environments, resulting in a critical knowledge gap regarding the impact and potential of these advanced tools for developers who are visually impaired.", "page": 4, "bbox": []}, "seg_38": {"summary": "Insufficient content for summary.", "text": "2.2 AI-assisted Coding Environments", "page": 4, "bbox": []}, "seg_39": {"summary": "AI-assisted coding tools like GitHub Copilot are shown to increase developer productivity and speed up programming tasks. However, studies also reveal challenges, as developers can struggle with understanding, editing, and debugging AI-generated code, impacting task effectiveness.  The tool's utility is also dependent on the developer's expertise level, as less experienced developers may find it harder to assess AI suggestions.", "text": "The integration of AI into software development tools has significantly impacted coding practices, with AI-assisted coding assistants like GitHub Copilot showing promise in improving developer productivity. Ziegler et al. [ 108 ] found that Copilot increases users\u2019 feelings of productivity, with almost a third of its proposed code completions being accepted. In a controlled experiment, Peng et al.[ 73 ] demonstrated that software developers using Github Copilot were able to complete programming tasks significantly faster than those without such assistance. However, Vaithilingam et al. [ 100 ] noted that while most participants preferred using Copilot in daily programming tasks, they often faced difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. This was confirmed by Dakhel et al [ 26 ], who noted that the effectiveness of tools like Github Copilot depends on the developer\u2019s level of expertise, as less experienced developers often lack the necessary skills to effectively evaluate AI-generated code suggestions.", "page": 4, "bbox": []}, "seg_40": {"summary": "Summary: According to a survey by Liang et al., developers primarily use AI programming assistants to reduce keystrokes, speed up task completion, and aid in syntax recall. The survey also indicated that developers are utilizing these tools for a significant portion of their coding, with a median of 30.5% of code being generated with AI assistance.", "text": "A recent survey of developers by Liang et al. [ 55 ] revealed that the primary motivations for using AI programming assistants include reducing keystrokes, finishing programming tasks quickly, and recalling syntax. Developers reported that a median of 30.5% of their code was written with help from tools like Copilot.", "page": 4, "bbox": []}, "seg_41": {"summary": "Studies have explored how developers interact with AI coding assistants, identifying modes like \"acceleration\" and \"exploration\" and comparing human-AI to human-human pair programming.  These studies reveal insights into collaboration dynamics, but a key gap remains in understanding the impact of AI coding assistants on developers with accessibility needs. Further research is needed to address this underrepresented perspective.", "text": "Studies like those by Barke et al.[ 66 ] and Wu et al. [ 58 ] provide insight into the varying modes of interaction with AI coding assistants. Barke et al. [ 66 ] identified \u201cacceleration mode\u201d and \u201cexploration mode\u201d as two broad categories of Copilot use, while Wu et al. [ 58 ] compared human-human pair programming with human-AI pair programming, highlighting differences in collaboration dynamics. However, there is still a significant gap in understanding how these AI tools impact developers with accessibility needs.", "page": 4, "bbox": []}, "seg_42": {"summary": "Insufficient content for summary.", "text": "2.3 Activity Theory", "page": 4, "bbox": []}, "seg_43": {"summary": "A visually impaired software developer uses Github Copilot as a tool to assist with coding tasks. This highlights the use of the tool by a specific user group for a particular purpose.", "text": "Tool ( Github Copilot ) Subject (Software developer who is visually impaired) Object ( Coding task )", "page": 4, "bbox": []}, "seg_44": {"summary": "Summary: Fig. 2 illustrates the relationship between visually impaired software developers (subject), coding tasks (object), and GitHub Copilot (mediating tool). This relationship is presented within the Activity Theory framework.", "text": "Fig. 2. Diagram illustrating the relationship between software developers who are visually impaired (subject), coding tasks (object), and GitHub Copilot (mediating tool) within the Activity Theory framework.", "page": 4, "bbox": []}, "seg_45": {"summary": "Summary: Activity Theory is a conceptual framework used to guide research and problem-solving.  It centers on the concept of \"activity,\" defined as goal-directed actions mediated by artifacts to achieve specific objectives. These actions are carried out through routinized operations.", "text": "Activity Theory is a type of \u201cconceptual frame- work\u201d [ 80 , 82 , 88 , 91 ]. A conceptual framework is an analytical tool or structure used to organize and guide research, projects, or problem-solving efforts [ 40 , 60 ]. The foundational concept of Activity The- ory is the \u201cactivity\u201d, a series of goal directed actions [ 53 ] that aim to achieve specific objectives [ 52 ]. These actions are mediated by artifacts, the instru- ments through which individuals interact with their objectives. Actions themselves are performed via routinized operations, in which individuals are not conscious of or focused on these operations. This process is illustrated in Figure 2 . For example, devel- opers who are visually impaired (the subjects) may have specific goals (objects) related to completing", "page": 4, "bbox": []}, "seg_46": {"summary": "Summary: In the context of programming tasks, an AI programming tool is defined as a mediating artifact, essentially functioning as a tool itself.", "text": "programming tasks. In this context, an AI programming tool acts as the mediating artifact (tool). Manuscript submitted to ACM", "page": 4, "bbox": []}, "seg_48": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 5, "bbox": []}, "seg_49": {"summary": "Summary: B\u00f8dker introduced Activity Theory as a foundational framework for HCI, emphasizing the dynamic interplay between users, their goals, and the tools they use, advocating for tool evolution to meet user needs.  Within this theory, contradictions between users, goals, and tools are seen as drivers of change, while misalignments are identified as localized usability issues that may not require systemic changes. Activity Theory, therefore, guides the design of adaptable interactive systems by addressing these tensions and mismatches.", "text": "B\u00f8dker proposed Activity Theory as a foundational framework for HCI [ 18 ], emphasizing its potential to guide the design of interactive systems by focusing on the dynamic interplay between users, their goals, and the tools users employ (interactive systems) [ 19 ]. B\u00f8dker emphasized that tools should evolve over time to meet users\u2019 needs and adapt to the activities they mediate[ 18 ], highlighting the importance of designing systems that adjust to changing workflows and contexts to remain effective[ 19 ]. Within B\u00f8dker\u2019s definition of Activity Theory for HCI [ 18 ], contradictions are discrepancies or tensions that arise between the tools, the goals, and the users, that must be addressed [ 28 ]. These contradictions are not obstacles but drivers of change, pointing out areas where tools or processes need to evolve to better meet user needs [ 29 ]. Within this space, Activity Theory also introduces the concept of misalignments , which are localized issues or practical mismatches. Unlike contradictions , misalignments hinder usability and effectiveness for end-users but may not necessitate systemic changes.", "page": 5, "bbox": []}, "seg_50": {"summary": "This paper uses Activity Theory to investigate the design of AI-assisted coding tools for visually impaired developers.  Activity Theory is presented as a suitable framework due to its established history in studying technologies designed for diverse groups, including individuals with disabilities. Prior research has successfully applied Activity Theory to analyze various technologies for people with disabilities, such as tactile devices and audio-haptic tools.", "text": "In this paper, we use Activity Theory as a framework to examine the design of AI-assisted coding tools for developers who are visually impaired. We chose this approach because Activity Theory has been widely used for decades to study tools and technologies designed for diverse groups, including individuals with disabilities [ 12 , 18 , 28 , 44 , 45 , 81 , 97 ]. For example, Baldwin et al. [ 12 ] applied Activity Theory to investigate the design of tactile devices and enhanced auditory tools, examining whether these technologies align with the unique workflows of users who have no or low-vision. Similarly, Szymczak [ 97 ] applied Activity Theory to analyze how audio-haptic technologies mediated interactions and supported the goals of individuals who are visually impaired, focusing on how these technologies could assist users in interacting with 2D representations, such as maps and drawings. Tlili et al. [ 98 ], also applied Activity Theory to review the use of game-based learning for learners with disabilities and identify inconsistencies in stakeholder involvement, variability in the use of educational technology, and difficulties in standardizing performance measures. Robins [ 81 ] applied Activity Theory to analyze and design accessibility in video games, focusing on the interaction between visually impaired players, game goals, and mediating tools like audio-haptic technologies and game mechanics.", "page": 5, "bbox": []}, "seg_51": {"summary": "Summary: The table presents data for ten male participants with either low vision or no vision who participated in a study.  These participants possess diverse coding experience and have utilized various AI coding tools such as GitHub Copilot and ChatGPT.  Python and C# were the preferred programming languages for the study task, which was successfully completed by all participants.", "text": "P# Gender Age Exp.(yrs) Vision Status AI-Coding Tool Experience Coding Proficiency Pref. Lang. For Study Task Completed P1 Male 32 8 No vision GitHub Copilot C#, JavaScript, Python, Ruby Python Yes P2 Male 34 12 Low vision GitHub Copilot PHP, C#, JavaScript C# Yes P3 Male 38 9 Low vision CodeWhisperer, Codellama, ChatGPT C++, Python Python Yes P4 Male 43 22 Low vision ChatGPT JavaScript, TypeScript, C# C# Yes P5 Male 26 6 No vision GitHub Copilot, ChatGPT Python Python Yes P6 Male 36 17 Low vision GitHub Copilot PHP, Ruby, Swift, Go, Python, JavaScript, Kotlin Python Yes P7 Male 58 23 No vision GitHub Copilot Python, SQL, PowerShell Python Yes P8 Male 54 32 No vision ChatGPT C, R, Ruby, TypeScript, Swift, Python, Kotlin Python Yes P9 Male 42 4 No vision ChatGPT JavaScript, PHP, Python, HTML, CSS Python Yes P10 Male 24 2 No vision Claude, ChatGPT, Gemini C#, HTML, PHP C# Yes", "page": 5, "bbox": []}, "seg_52": {"summary": "Insufficient content for summary.", "text": "Table 1. Participant Demographics, Vision Status, Experience with AI Coding Assistants, and Programming Background.", "page": 5, "bbox": []}, "seg_53": {"summary": "This paper utilizes Activity Theory to study the impact of AI-assisted coding tools, specifically GitHub Copilot, on visually impaired developers.  The analysis investigates how these tools mediate developer tasks and identifies both systemic contradictions and usability misalignments.  Ultimately, the research aims to provide design recommendations to enhance AI coding tools to better meet the needs of developers with visual impairments.", "text": "Building on this foundation, our paper applies Activity Theory to examine how AI-assisted coding tools, particularly GitHub Copilot, mediate and influence the tasks of developers who are visually impaired. Activity Theory provides a particularly useful analysis framework since the introduction of AI coding assistants may operationalize some actions that have not yet become routine for developers. Additionally, we use Activity Theory to guide design recommendations aimed at improving these tools for this population. To achieve this, we analyze both contradictions \u2014systemic tensions within the activity system\u2014and misalignments \u2014localized issues that disrupt usability. By addressing these challenges, we ensure AI tools meet developers\u2019 needs.", "page": 5, "bbox": []}, "seg_54": {"summary": "Summary: Manuscript submission to ACM has occurred.", "text": "Manuscript submitted to ACM", "page": 5, "bbox": []}, "seg_56": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 6, "bbox": []}, "seg_57": {"summary": "Summary: This text segment lists various chat functionalities.  It includes features such as inline chat and ghost text suggestions, as well as different types of chat window implementations, specifically floating and embedded panes.", "text": "a.Inline chat b. Ghost text suggestions c. Chat Window (floating element) d. Chat window (embedded pane)", "page": 6, "bbox": []}, "seg_58": {"summary": "Summary: GitHub Copilot offers multiple interaction interfaces to assist users, including an inline chat for quick commands, dynamic ghost text suggestions as users type, a floating chat window for temporary interactions, and an embedded pane chat for more extensive and ongoing conversations. These interfaces cater to different user needs and interaction styles.", "text": "Fig. 3. GitHub Copilot interaction interfaces. (a) Inline chat window for quick command input and AI engagement. (b) Ghost text suggestions dynamically generated as the user types. (c) Floating chat window for temporary interactions, ideal for quick-access queries. (d) Embedded pane chat window for ongoing, more extensive conversations with AI, allowing for sustained reference and deeper coding assistance.", "page": 6, "bbox": []}, "seg_59": {"summary": "Insufficient content for summary.", "text": "2.4 Copilot", "page": 6, "bbox": []}, "seg_60": {"summary": "Summary: GitHub Copilot's interface includes an inline chat feature, enabling developers to interact with the AI coding assistant directly within the code editor. This method allows for seamless and quick communication without interrupting the coding process.", "text": "The GitHub Copilot interface provides multiple ways for developers to interact with its AI-driven coding assistant. One primary method is through its inline chat (Fig. 3 a), where users can type commands or questions directly within the code editor. This allows for seamless, quick interactions without disrupting the coding workflow.", "page": 6, "bbox": []}, "seg_61": {"summary": "Developers can interact with Copilot through a chat window that can be either a floating element or an embedded pane within the interface.  Floating windows are suitable for quick queries, while embedded panes are better for extended interactions and referencing past exchanges. This provides developers with flexible interaction styles to suit their workflow.", "text": "Additionally, developers can engage with Copilot using the chat window , which can appear either as a floating element (Fig. 3 c) or as an embedded pane within the interface (Fig. 3 d). An embedded pane refers to a UI component that is fixed within the main window, allowing for continuous visibility and interaction without obstructing other elements on the screen. The floating element is useful for quick, temporary queries, while the embedded pane is better suited for extended interactions where users may need to frequently reference past exchanges. This flexibility enables developers to choose an interaction style that best fits their workflow and preferences.", "page": 6, "bbox": []}, "seg_62": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 6, "bbox": []}, "seg_64": {"summary": "This text segment discusses the effects of generative AI coding assistants on developers who are visually impaired.  It likely explores how these tools impact their coding experience, accessibility, and productivity.  The content will likely delve into both the benefits and challenges presented by AI assistance for this specific group of developers.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 7, "bbox": []}, "seg_65": {"summary": "Summary: Copilot offers ghost text suggestions, providing real-time, semi-transparent code completions as developers type, which can be accepted, modified, or ignored.  Additionally, developers can request code completions by writing natural language comments describing the desired functionality, which Copilot then translates into code snippets. These features aim to enhance developer efficiency.", "text": "Another key interaction method is through ghost text suggestions (Fig. 3 b). As the developer types, Copilot continuously analyzes the context and generates relevant code suggestions in real-time. These suggestions appear as semi-transparent text within the editor, allowing developers to seamlessly integrate them into their code. Developers can choose to accept, modify, or ignore these suggestions based on their needs. Beyond real-time suggestions, developers can also request code completions by writing natural language comments that describe the desired functionality. Copilot then processes these descriptions and generates corresponding code snippets, helping developers implement features more efficiently.", "page": 7, "bbox": []}, "seg_66": {"summary": "Summary: GitHub Copilot is a Visual Studio Code extension, not a standalone editor, and its chat windows are features of the extension itself, not default VS Code functionality.  These chat interfaces enable users to engage with AI-generated coding suggestions directly within their VS Code environment.", "text": "It is important to note that GitHub Copilot is not a standalone code editor but an extension designed to work within Visual Studio Code. The chat windows, including both the floating and embedded pane versions, are features introduced by the GitHub Copilot extension and are not part of Visual Studio Code by default. These chat interfaces allow users to interact directly with AI-generated coding suggestions within their coding environment.", "page": 7, "bbox": []}, "seg_67": {"summary": "Researchers selected GitHub Copilot as the primary AI coding assistant for their study due to its widespread use at the time of research.  Visual Studio Code was chosen as the development environment because of its popularity, accessibility, and seamless integration with GitHub Copilot. This setup allowed the researchers to investigate the influence of these tools on the coding experience of developers who are visually impaired.", "text": "GitHub Copilot was selected as the primary AI coding assistant for this study because, at the time of research, it was the most widely used tool of its kind [ 33 , 92 ]. We chose Visual Studio Code as the development environment due to its popularity and accessibility, as well as its seamless integration with GitHub Copilot [ 93 , 101 ]. This setup enabled us to explore how these tools influence the coding experience of developers who are visually impaired.", "page": 7, "bbox": []}, "seg_68": {"summary": "Insufficient content for summary.", "text": "2.5 Research Gap", "page": 7, "bbox": []}, "seg_69": {"summary": "Summary:  While accessibility has improved for visually impaired developers using traditional coding tools, AI coding assistants like GitHub Copilot present new, under-researched challenges.  Existing research mainly focuses on traditional tools, neglecting the unique complexities of AI-assisted coding, highlighting the need to examine and ensure these new AI tools are inclusive.", "text": "Despite progress in accessibility for developers who are visually impaired [ 13 , 23 , 65 ], generative AI coding assistants like GitHub Copilot introduce new challenges and opportunities that remain under-explored. Most existing research focuses on traditional coding tools [ 1 , 49 ], overlooking complexities unique to AI-assisted coding. Our study examines these accessibility issues to ensure AI assistants like GitHub Copilot are inclusive and support equal access for visually impaired developers.", "page": 7, "bbox": []}, "seg_71": {"summary": "Researchers conducted a study to understand the experiences and challenges faced by visually impaired developers when using GitHub Copilot. Utilizing Activity Theory, the study analyzed the interaction between these developers and the AI tool in coding tasks. The findings revealed design limitations of Copilot in meeting the specific needs of visually impaired developers, alongside the identification of potential opportunities.", "text": "To study the relationship between AI coding assistants and accessibility, we conducted a study with developers who are visually impaired. Our goal was to understand their experiences, challenges, and strategies when using an AI coding assistant, specifically GitHub Copilot. Using Activity Theory as our framework, we analyzed how Copilot (tool) mediates the interaction between visually impaired developers (participants) and their coding tasks (object). Our analysis highlights the contradictions and misalignments that occur when Copilot\u2019s design falls short of meeting the unique needs of developers who are visually impaired, while also uncovering new opportunities that emerge in this environment.", "page": 7, "bbox": []}, "seg_72": {"summary": "Insufficient content for summary.", "text": "3.1 Participant Recruitment", "page": 7, "bbox": []}, "seg_73": {"summary": "Summary: This study recruited ten visually impaired male software developers with varying levels of vision and professional experience (2-32 years) to participate in research.  Participants were sourced through specialized networks and had some familiarity with AI coding assistants. The study acknowledges the limited recruitment pool but justifies the sample size by referencing similar HCI studies on underrepresented groups.", "text": "We recruited 10 software developers who were visually impaired, including 4 with low vision and 6 with no vision, with experience ranging from 2 to 32 years in the field (see Table 1 ). Participants were sourced through professional networks, accessibility-focused online forums, and organizations supporting professionals in tech with visual impairments. To participate, developers needed some familiarity with AI coding assistants, though extensive experience with Copilot was not required. All participants identified as male. Given the specialized nature of developers who are visually impaired, our recruitment pool was naturally limited. However, our sample size aligns with prior HCI studies on underrepresented populations [ 25 , 30 , 69 , 79 , 83 ]. Despite these constraints, our study provides valuable insights into the accessibility", "page": 7, "bbox": []}, "seg_74": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 7, "bbox": []}, "seg_76": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 8, "bbox": []}, "seg_77": {"summary": "Summary: This text segment refers to an underrepresented group and mentions challenges and opportunities they face.  Participants from this group received $50 USD as compensation for their time, suggesting involvement in a study or event.", "text": "challenges and opportunities for this underrepresented group. Each participant received $50 USD (or its equivalent in local currency) as compensation for their time.", "page": 8, "bbox": []}, "seg_78": {"summary": "Insufficient content for summary.", "text": "3.2 Study setup", "page": 8, "bbox": []}, "seg_79": {"summary": "Summary: This study was conducted remotely, allowing participants to utilize their preferred accessible setups.  Each session was approximately 90 minutes long and followed four sequential phases.", "text": "We conducted our study remotely, ensuring that participants could use their preferred accessible setup. Each session lasted approximately 90 minutes and followed four sequential phases:", "page": 8, "bbox": []}, "seg_80": {"summary": "Summary: This section of the study focuses on participant backgrounds and their experience with AI coding assistants.  Researchers collected information on participants' professional and technical backgrounds and their prior use of tools like GitHub Copilot.", "text": "(1) Participant Background and Copilot Experience: We asked participants to describe their professional and technical backgrounds and share their prior experience with AI coding assistants like GitHub Copilot.", "page": 8, "bbox": []}, "seg_81": {"summary": "Summary: Participants attended a training session to learn about GitHub Copilot and its features. The session also detailed how Copilot works with accessible development environments and assistive technologies that participants were already familiar with. This training was designed to prepare participants to effectively use Copilot for upcoming coding tasks.", "text": "(2) Training Session: We provided participants with a training session that introduced GitHub Copilot and its key features. The session also covered how Copilot integrates with accessible development environments and assistive technologies participants were already using. This ensured that all participants had a solid understanding of how to interact with Copilot before starting the coding tasks.", "page": 8, "bbox": []}, "seg_82": {"summary": "Participants in the study engaged in practical coding and debugging exercises.  These tasks were performed using GitHub Copilot within the Visual Studio Code environment. This hands-on component aimed to assess participant interaction with the tool in realistic software development scenarios.", "text": "(3) Hands-on Coding and Debugging Tasks: We instructed participants to complete a coding task followed by a debugging task using GitHub Copilot in Visual Studio Code.", "page": 8, "bbox": []}, "seg_83": {"summary": "Summary: Participants were tasked with writing a program to calculate the days until a user's next birthday, requiring specific date input formats and class implementation with unit tests.  They were instructed to use the \"think-aloud\" protocol and utilize Copilot while completing the programming task. This programming assignment was designed following prior research methods.", "text": "\u2022 Programming Task: Following prior work [ 46 ], we asked participants to write a program that calculates the number of days until the user\u2019s next birthday. The program required a birthday string input in either DDMMYY or DDMMYYYY format and a class implementation to handle date calculations. We also asked participants to write unit tests for their class. Throughout the task, we encouraged them to use the \"think-aloud\" protocol, verbalizing their thought process while interacting with Copilot.", "page": 8, "bbox": []}, "seg_84": {"summary": "Insufficient content for summary.", "text": "We selected this task because:", "page": 8, "bbox": []}, "seg_85": {"summary": "Summary: This tool integrates familiar programming concepts like string manipulation, date calculations, and object-oriented principles.  These features ensure relevance and applicability to participants' standard coding practices.", "text": "\u2013 It incorporates common programming concepts such as string manipulation, date calculations, and object- oriented principles, making it relevant to participants\u2019 typical coding workflows.", "page": 8, "bbox": []}, "seg_86": {"summary": "Summary: The subject matter does not necessitate specific programming framework expertise. This simplifies the process of recruiting participants.", "text": "\u2013 It does not require specialized knowledge of specific programming frameworks, simplifying participant recruitment.", "page": 8, "bbox": []}, "seg_87": {"summary": "Participants participated in a debugging task where they had to find and correct errors in code produced by Copilot.  This session involved them discussing the debugging process as they worked to fix the code.", "text": "\u2022 Debugging Task: We then asked participants to engage in a debugging session, where they identified and fixed errors in the code generated by Copilot. During this session, we prompted them to discuss:", "page": 8, "bbox": []}, "seg_88": {"summary": "Summary: Insufficient content for summary.", "text": "\u2013 Their usual approach to evaluating code accuracy.", "page": 8, "bbox": []}, "seg_89": {"summary": "Insufficient content for summary.", "text": "\u2013 The methods they used to handle encountered issues.", "page": 8, "bbox": []}, "seg_90": {"summary": "Insufficient content for summary.", "text": "\u2013 Whether Copilot simplified or complicated their debugging process.", "page": 8, "bbox": []}, "seg_91": {"summary": "Summary: Following the programming tasks, semi-structured interviews were conducted where participants detailed their code organization, adjustments made due to Copilot, and challenges encountered.  Participants also reflected on Copilot's impact on task complexity, highlighting instances of its helpfulness or difficulty.  Further details about the interview questions can be found in the Appendix.", "text": "(4) Post-Interview: After completing the programming and debugging tasks, we conducted a semi-structured interview with each participant. We asked them to explain their approach to organizing their code and describe any adjustments they made based on Copilot\u2019s suggestions. They also shared the challenges they encountered and the solutions they implemented. Additionally, we encouraged them to reflect on how Copilot influenced the complexity of their tasks, providing examples of when it was particularly helpful or challenging. Our Appendix contains further details on the post-study interview questions.", "page": 8, "bbox": []}, "seg_92": {"summary": "Summary: Figure 1 illustrates the study setup with a diagram.", "text": "A diagram illustrating our study setup is shown in Figure 1 .", "page": 8, "bbox": []}, "seg_93": {"summary": "Insufficient content for summary.", "text": "3.3 Data Collection and Analysis", "page": 8, "bbox": []}, "seg_94": {"summary": "The researchers collected data through interviews, recording and transcribing both participant verbal responses and audio cues from assistive technologies.  They then analyzed this data using thematic coding to identify key patterns and insights. This methodology allowed for a comprehensive understanding of the participant experience.", "text": "We recorded and transcribed all interviews, capturing both verbal responses and relevant audio cues from participants\u2019 screen readers and other assistive technologies. We analyzed the data using thematic coding, focusing on significant Manuscript submitted to ACM", "page": 8, "bbox": []}, "seg_96": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 9, "bbox": []}, "seg_97": {"summary": "This study analyzed the experiences of visually impaired developers using GitHub Copilot by qualitatively coding interview transcripts to identify key themes and patterns.  Researchers focused on critical incidents to understand how Copilot impacted accessibility and aligned with or diverged from non-visual coding strategies. Activity Theory was used to further analyze the opportunities and challenges encountered by these developers when using the AI coding assistant.", "text": "events that participants experienced while using GitHub Copilot. Two of us independently coded the transcripts, identifying emerging themes and patterns. We paid special attention to pivotal moments that either enhanced or hindered participants\u2019 coding processes. These critical incidents provided concrete examples of how Copilot mediated accessibility, revealing specific ways in which the tool aligned with or diverged from the non-visual coding strategies used by developers who are visually impaired. To deepen our analysis, we integrated Activity Theory , which allowed us to examine the opportunities and contradictions that visually impaired developers encountered when interacting with an AI coding assistant. In particular, Activity Theory helped us explore within our themes:", "page": 9, "bbox": []}, "seg_98": {"summary": "Researchers applied Activity Theory to study how visually impaired developers used GitHub Copilot for coding tasks.  The study mapped the interactions within this activity system to understand how Copilot impacted developer workflows and to identify any resulting challenges or misalignments. This framework helped analyze the dynamics between developers, Copilot, and their coding activities.", "text": "(1) Activity-Centric Lens: how developers who are visually impaired ( subjects ) used GitHub Copilot ( tool ) to complete their coding tasks ( object ). By applying Activity Theory, we mapped the observed dynamics within the activity system, identifying key interactions and tensions among its components. This framework helped us understand how Copilot influenced developers\u2019 workflows and where misalignments or challenges emerged.", "page": 9, "bbox": []}, "seg_99": {"summary": "Visually impaired developers encounter challenges due to systemic contradictions in AI tools like Copilot.  Specifically, context switching and dynamic behaviors within these tools disrupt the sequential workflows crucial for these developers. These disruptions necessitate extra navigation adaptations to maintain productivity.", "text": "(2) Systemic Contradictions: challenges that developers who are visually impaired faced, particularly those arising from systemic contradictions . For example, some participants struggled with context switching imposed by the AI, while others had difficulty navigating Copilot\u2019s dynamic behaviors, which disrupted the sequential workflows that are critical for developers who are visually impaired. These disruptions required additional navigation adaptations to maintain productivity.", "page": 9, "bbox": []}, "seg_100": {"summary": "Summary: The text segment discusses adaptive solutions to identified challenges and mentions the application of the Activity Theory framework. This framework provides insights into how AI assistants can be improved to better suit the workflows of visually impaired developers. The ultimate goal is to create more inclusive and efficient coding environments.", "text": "(3) Adaptive Solutions: possible adaptive solutions to the challenges we identified. By applying the Activity Theory framework, we gained insights into how AI assistants could be better aligned with the specific workflows of developers who are visually impaired, ultimately fostering more inclusive and efficient coding environments.", "page": 9, "bbox": []}, "seg_101": {"summary": "Summary: This study used Activity Theory to analyze how visually impaired developers interact with AI coding assistants like Copilot. The analysis revealed key accessibility themes and design elements within Copilot that either helped or hindered these developers' workflows.  Ultimately, the findings advocate for a re-evaluation of accessibility standards in AI-assisted coding environments.", "text": "By integrating Activity Theory into our thematic analysis, we gained a deeper understanding of how developers who are visually impaired engage with AI coding assistants. This approach allowed us to identify key accessibility themes and design elements of Copilot that either supported or hindered participants\u2019 work processes. Our findings contribute to the broader goal of rethinking accessibility paradigms in AI-assisted coding environments.", "page": 9, "bbox": []}, "seg_103": {"summary": "Summary: Research involving interviews and observations has identified key themes regarding the intricate connection between AI coding assistants, accessibility requirements, and coding methods. These themes shed light on the challenges and opportunities encountered by visually impaired developers when utilizing AI-assisted coding tools such as GitHub Copilot. This research directly addresses the question of how these tools impact accessibility and coding practices for this specific group of developers.", "text": "Through our interviews and observations, we identified key themes that highlight the complex relationship between AI coding assistants, accessibility needs, and coding practices. These themes address our research question by revealing the challenges and opportunities that developers who are visually impaired encounter when using AI-assisted coding tools like GitHub Copilot.", "page": 9, "bbox": []}, "seg_104": {"summary": "This section presents themes from interviews and critical incidents to illustrate how generative AI coding assistants impact visually impaired developers, both positively and negatively.  Framed within Activity Theory, the findings reveal systemic misalignments between users, tools, and coding environments. The study aims to identify opportunities to improve the accessibility and inclusivity of AI coding assistants.", "text": "In this section, we present these themes. For each theme, we include illustrative quotes and describe critical incidents from our interviews. These incidents provide insight into the lived experiences of our participants, offering concrete examples of how generative AI coding assistants can both empower and hinder developers who are visually impaired. We frame our findings within the Activity Theory framework to contextualize these challenges as systemic misalignments between users, their tools, and the coding environments they work in. This approach allows us to examine how AI coding assistants mediate the development process and where breakdowns occur, helping us identify opportunities for more accessible and inclusive AI coding assistants.", "page": 9, "bbox": []}, "seg_105": {"summary": "Summary: This section, labeled as Research Question 1 (RQ1), explores the challenges and opportunities that AI coding assistants present for developers who are visually impaired. It aims to understand the specific implications of these tools for this user group.", "text": "4.1 RQ1: Challenges and Opportunities of AI Coding Assistants for Developers Who Are Visually Impaired", "page": 9, "bbox": []}, "seg_106": {"summary": "AI coding assistants were found to enhance the sense of control for visually impaired developers.", "text": "4.1.1 AI and Control . Our interviews revealed that AI coding assistants contributed to a dynamic sense of control among developers who are visually impaired. This enhanced control manifested in several positive ways. Participants", "page": 9, "bbox": []}, "seg_107": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 9, "bbox": []}, "seg_109": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 10, "bbox": []}, "seg_110": {"summary": "Summary: AI assistants are shifting developers' focus towards strategic control in coding by enabling them to delegate routine tasks. This allows developers to concentrate on high-level decisions and code structure, ultimately increasing their control over the development process. For example, Copilot assists with tedious tasks like generating documentation, freeing developers to focus on more strategic aspects of coding.", "text": "described experiencing a shift toward strategic control over the coding process. Rather than manually performing every task, they found that AI assistant allowed them to delegate routine or less engaging tasks, enabling them to focus on high-level decision-making and overall code structure. This ability to offload work gave them greater control in shaping their development process while maintaining oversight of their projects. For instance, P7 highlighted how Copilot eased their workload and gave them more control over their coding process by handling tasks they found tedious, such as generating docstrings\u2014special multiline comments in programming that explain the functionality of a function, class, or module:", "page": 10, "bbox": []}, "seg_111": {"summary": "Summary: The developer reports a positive experience with Copilot, particularly for its ability to automate tasks they find unenjoyable, such as generating documentation.  They appreciate Copilot's assistance with these less desirable aspects of development, even while planning to write the majority of their code themselves.", "text": "\u201cOverall, my experience with Copilot is positive, especially because it helps me with tasks I don\u2019t enjoy. I will write the majority of my own code [...], but where it [Copilot] can help me, I\u2019ll definitely let it [Copilot] do it. I love having it generate docstrings for me. That\u2019s cool, because frankly, I don\u2019t like writing documentation. I\u2019d rather code and let it do the heavy lifting for me...\u201d - P7, Developer with no vision.", "page": 10, "bbox": []}, "seg_112": {"summary": "Summary: AI empowers developers by automating routine tasks, freeing them to focus on complex challenges and increasing their workflow control.  One participant compared using AI to piloting an aircraft, highlighting this enhanced control.", "text": "This experience demonstrates how AI empowers developers by taking over tedious tasks they prefer to avoid. By automating routine work, AI enables developers to maintain greater control over their workflow and focus on more complex and engaging coding challenges. Similarly, participant P1, who had prior experience with AI coding assistants, compared coding with AI to piloting an aircraft, emphasizing how these assistants increase their sense of control:", "page": 10, "bbox": []}, "seg_113": {"summary": "Summary: This developer likens using Copilot to pair programming and flying a plane, where Copilot handles the repetitive coding tasks, similar to an autopilot.  The developer's role then shifts to a higher-level perspective, focusing on the overall direction and identifying potential problems, much like a pilot monitoring a plane's course.", "text": "\u201cI sort of compare it [Copilot] to pair programming almost, where [...] I\u2019m sort of driving, but the other person is doing all the sort of drudgery work. And what I\u2019m really mostly doing now is almost like what a pilot does when they\u2019re flying a plane. They\u2019re [...] not flying the plane physically for most of it. Most of it, the plane\u2019s flying itself, but the pilot does have to keep an eye on. Are we still going in the right direction? What\u2019s that big thing coming towards us really fast? Should we avoid that?\u201d - P1, Developer with no vision.", "page": 10, "bbox": []}, "seg_114": {"summary": "Summary: Activity Theory suggests AI tools like Copilot mediate the coding process, transforming how developers work. Copilot acts as an intermediary, shifting developer control and enabling them to concentrate on strategic, high-level decisions instead of lower-level coding tasks. This reallocation of focus is exemplified by developers P7 and P1.", "text": "From the perspective of Activity Theory, these findings illustrate the transformative role of AI tools as mediators in the coding process. In this context, Copilot functions as an intermediary that actively shapes a developer\u2019s control over their work, allowing developers to reallocate their focus toward higher-level strategic decisions, as demonstrated by P7 and P1.", "page": 10, "bbox": []}, "seg_115": {"summary": "Visually impaired developers can utilize AI coding assistants in a supervisory control model, where they oversee and guide the AI's output, correcting mistakes and ensuring alignment with their coding objectives. This approach shifts the developer's role from manual code writing to strategically guiding and refining AI-generated code, similar to a pilot managing an aircraft's automated systems.  This allows developers to focus on high-level project direction while delegating implementation details to the AI.", "text": "This shift in control also aligns with the concept of supervisory control [ 24 , 87 ], in which humans oversee and direct automated systems rather than executing tasks manually. In this case, the developers who are visually impaired guide the AI coding assistants by monitoring their outputs, making corrections when necessary, and ensuring alignment with their broader coding goals. Just as a pilot monitors and manages an aircraft\u2019s automated systems while remaining responsible for high-level navigation and safety decisions, developers can delegate routine implementation details to Copilot while maintaining oversight and control over the overall project direction. This dynamic shifts the developer\u2019s role from manually writing every line of code to curating, refining, and strategically guiding the AI\u2019s output.", "page": 10, "bbox": []}, "seg_116": {"summary": "Summary: In this evolving coding paradigm, developer control shifts from writing individual lines of code to a more strategic role of guiding the overall direction and making critical decisions. Developers now oversee AI-generated suggestions, ensuring they align with the broader project architecture and goals, while still retaining ultimate authority. This represents a move towards a more abstract and high-level form of control.", "text": "In this new paradigm, control manifests as the ability to guide the overall direction of the code, make critical decisions, and intervene when necessary. While the developer retains ultimate authority over the coding process, the nature of that control shifts to a more abstract and strategic level. Rather than focusing solely on writing individual lines of code, developers can now oversee and direct AI-generated suggestions to ensure they align with the broader project architecture and goals.", "page": 10, "bbox": []}, "seg_117": {"summary": "Summary: The increasing control developers have with AI tools demands they acquire new skills. These skills include prompt engineering to guide AI outputs, evaluating the quality of AI-generated code, and maintaining a high-level perspective on software design.", "text": "However, this shift in control also requires developers to develop new skills, such as crafting effective prompts to generate useful AI outputs, quickly assessing the quality and relevance of the AI-generated code, and maintaining a high-level understanding of the overall software design. As highlighted by P1:", "page": 10, "bbox": []}, "seg_118": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 10, "bbox": []}, "seg_120": {"summary": "This text segment likely discusses the effects of generative AI coding assistants on developers who are visually impaired. It probably explores how these AI tools influence the workflows, accessibility, and productivity of visually impaired programmers.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 11, "bbox": []}, "seg_121": {"summary": "Summary: A developer using Copilot found that the AI assistant made a mistake in a day counting function by not considering different years, a type of error they had observed previously. This experience underscores the need for careful prompting and user vigilance when working with AI coding assistants to avoid such oversights.", "text": "\u201c...when I was checking that initial version of the day counting function, I wasn\u2019t sure if it [Copilot] realized that it wasn\u2019t always going to be the same year, and it [Copilot] didn\u2019t realize that in this case. But I\u2019ve seen it make that kind of mistake before [...] You just need to be careful with your prompts [communication with the AI assistant].\u201d - P1, Developer with no vision.", "page": 11, "bbox": []}, "seg_122": {"summary": "AI-assisted coding is changing the role of visually impaired developers, allowing them to focus on strategic oversight rather than manual coding tasks.  This shift presents a challenge to design accessible AI coding environments that empower these developers with control over their workflows.  Ensuring accessibility in these new environments is crucial for visually impaired developers to maintain command over their coding processes.", "text": "This shift in AI-assisted coding, where developers focus on strategic oversight rather than manually completing every coding task, marks an evolution in how developers who are visually impaired maintain control over their coding workflows [ 8 ]. The challenge lies in designing AI coding environments that empower developers with this level of control while ensuring accessibility.", "page": 11, "bbox": []}, "seg_123": {"summary": "Summary: Designers of AI coding environments must recognize the distinct needs of visually impaired developers, who rely on predictable and structured interactions rather than the visual cues preferred by sighted developers.  Integrating generative AI, which is inherently unpredictable, presents a challenge to creating accessible coding environments that meet these needs.  Therefore, careful design is crucial to balance AI assistance with the accessibility requirements of visually impaired developers.", "text": "To achieve this, designers of AI coding environments must recognize that the needs of developers who are visually impaired often differ from those of sighted developers [ 8 , 27 , 107 ]. While sighted developers frequently rely on visual cues and dynamic, exploratory interfaces, visually impaired developers usually require predictable, structured interactions with clear, interpretable feedback [ 63 ]. This requirement is not merely a preference but a necessity that allows them to effectively understand, navigate, and maintain control over their workflow [ 48 , 62 ]. Now, a key consideration in designing AI coding environments for visually impaired developers is enabling them to anticipate the AI\u2019s actions and seamlessly integrate its assistance into their coding tasks [ 64 ]. However, generative AI inherently introduces a degree of unpredictability, making this integration complex. As a result, developing AI-driven coding environments that balance strategic oversight with accessibility is not a trivial task and requires careful, thoughtful design.", "page": 11, "bbox": []}, "seg_124": {"summary": "AI-assisted interfaces like Copilot introduce context switching challenges due to their dynamic suggestions, forcing developers to frequently shift focus between writing code and reviewing AI outputs. This disruption is particularly problematic for visually impaired developers, as it interferes with their structured navigation strategies and makes it harder to maintain workflow continuity.", "text": "4.1.2 Context Switching Difficulties in AI-assisted Interfaces . Although Copilot provided a sense of control and empowerment, it also introduced new challenges related to context switching. Our findings highlight that the dynamic nature of AI-generated suggestions and the frequent need to shift focus between writing code and reviewing AI outputs created disruptions. For developers who are visually impaired, this shift interfered with the structured navigation strategies they had developed for traditional coding environments [ 8 ], making it harder to maintain workflow continuity.", "page": 11, "bbox": []}, "seg_125": {"summary": "The AI assistant's unexpected view changes disrupted developer workflows by forcing them to switch focus and lose context.  For example, interacting with code suggestions automatically shifted the view, often without notification. This made it challenging for developers to maintain their flow and seamlessly integrate AI-generated code.", "text": "The AI assistant frequently triggered unexpected view changes, forcing developers to shift their focus to different tasks or sections of the coding environment. For example, as developers interacted with Copilot\u2019s suggestions, the system would automatically switch their view to the newly generated code. This disrupted their workflow, especially because they were not always notified about these sudden shifts. As a result, developers struggled to maintain context, making it even more challenging to navigate and integrate AI-generated code seamlessly.", "page": 11, "bbox": []}, "seg_126": {"summary": "Summary: Context switching in AI-assisted coding environments creates significant accessibility barriers for visually impaired developers, especially those using screen readers.  This underscores the necessity for AI coding assistants to be designed with improved accessibility to better support screen reader users' reliance on sequential navigation.", "text": "The challenges of context switching in AI-assisted coding environments pose a significant accessibility barrier for developers who are visually impaired. These difficulties highlight the need for AI coding assistants to be designed with greater attention to the needs of screen reader users, who rely on sequential navigation [ 37 , 109 ].", "page": 11, "bbox": []}, "seg_127": {"summary": "Summary: User P9 faced usability issues with Copilot's embedded chat feature.  After posing a question, P9 had trouble locating and navigating to the answer provided by Copilot within the text. This difficulty hindered workflow and increased mental effort.", "text": "This challenge became evident with P9, who encountered difficulties after typing a question in the embedded chat window. When Copilot generated a response, he struggled to locate and navigate to the text where the answer was displayed, disrupting his workflow and adding unnecessary cognitive load:", "page": 11, "bbox": []}, "seg_128": {"summary": "Summary: The user, a developer with no vision (P9), is confused about how to navigate the interface to find the AI assistant's response. They are unsure if they need to use the up and down keys and suspect the focus might still be on their initial question, preventing them from seeing the AI's answer.  This indicates a potential usability issue for users with no vision in accessing the AI's output.", "text": "\u201cOkay, so do I need, I\u2019m just, I\u2019m trying to understand if we need to press the up and down [the participant repeatedly pressed keys to locate the AI assistant\u2019s response]. Seems like it still, the focus is still on my question that I typed [and not on the AI\u2019s response]...\u201d - P9, Developer with no vision.", "page": 11, "bbox": []}, "seg_129": {"summary": "Summary: Blind users rely on stable context for efficient screen reader use, but AI coding introduces frequent and abrupt context switching. This can negatively impact the workflow of blind users who depend on consistent context.", "text": "P1 emphasized that blind users depend on maintaining a stable and consistent context to work efficiently, largely due to how screen readers are designed. However, the frequent and abrupt context switching introduced by AI coding", "page": 11, "bbox": []}, "seg_130": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 11, "bbox": []}, "seg_132": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 12, "bbox": []}, "seg_133": {"summary": "Assistants hinder visually impaired developers by disrupting the continuous workflow necessary for orientation and task management in coding. This disruption negatively impacts their ability to code effectively.", "text": "assistants disrupts this continuity, making it difficult for developers who are visually impaired to stay oriented and manage their coding tasks effectively:", "page": 12, "bbox": []}, "seg_134": {"summary": "Summary: Screen readers present information sequentially, meaning blind users can only access one element at a time, such as a single window or line of text. This sequential nature contrasts with visual interfaces where sighted users can perceive multiple elements simultaneously.  This limitation is a key aspect of how blind individuals interact with digital content.", "text": "\u201cA blind person really only has one thing they can see at any given time. It would be that one window, one line of text, one line of whatever it is [...] because screen readers, that\u2019s just how they work. You can only see one thing at a time. They work sequentially and not parallel.\u201d - P1, Developer with no vision.", "page": 12, "bbox": []}, "seg_135": {"summary": "Summary: The user, P5, found that navigating between coding and Copilot's chat window required too many keystrokes, which exacerbated context-switching issues. This excessive effort for minor transitions caused task disruption and frustration, hindering workflow and focus during coding.", "text": "P5 explained that the context-switching issue worsened due to the many keystrokes required to navigate between different contexts, such as reaching the Copilot chat window after writing code. The need for multiple keystrokes or clicks for minor transitions caused him to lose track of his original task, leading to frustration. This additional effort further amplified the challenge, making it harder to maintain workflow and stay focused on coding tasks:", "page": 12, "bbox": []}, "seg_136": {"summary": "Summary: A developer with no vision expresses frustration with coding due to the heavy reliance on memorization and frequent need to reference files.  Using Copilot, while helpful, also introduces frustration because navigating to and from the Copilot chat window disrupts their train of thought and causes them to lose context, requiring them to re-examine code. This context switching makes it harder to remember their original coding intentions.", "text": "\u201c...when I\u2019m coding, I have to memorize so much... It\u2019s a lot of, I\u2019m going to reference this file again because I can\u2019t 100% remember what I said...I was going to do. And you know when I\u2019m going to Copilot, when I\u2019m pressing F6 a couple of times and I\u2019m pressing tab a couple of times, that creates a little frustration almost, because I want to get back to the task [The participant used multiple key presses to navigate to the Copilot chat window and then return to their code]. And that frustration makes it hard to remember what I was thinking about, to begin with. And then...I forget the context...now I have to go back and check again.\u201d - P5, Developer with no vision.", "page": 12, "bbox": []}, "seg_137": {"summary": "Unexpected AI responses in AI-assisted coding environments worsen context switching challenges.  For example, a developer with no vision, P5, found that navigating the coding interface triggered unintended AI actions, which further complicated navigation.", "text": "The challenges of context switching in AI-assisted coding environments were further exacerbated by unexpected AI responses that caused sudden and confusing shifts in context. P5, a developer with no vision, described a particularly frustrating experience where merely navigating through the coding interface unintentionally triggered AI actions, making navigation even more difficult:", "page": 12, "bbox": []}, "seg_138": {"summary": "Summary: A developer with no vision, P5, describes accidentally accepting unwanted Copilot suggestions due to the UI design.  The issue arises because Copilot immediately suggests code when the cursor moves, and visually impaired developers often use the tab key for navigation, making accidental acceptance easy. This highlights a usability problem for developers who navigate interfaces with the tab key.", "text": "\u201cDid I accidentally just accept [accept the AI\u2019s suggestion] I don\u2019t, I think I hit tab. See, this is part of the problem too. It\u2019s so easy to accidentally accept a suggestion because when you move the cursor, Copilot immediately is like, oh, I have a suggestion for this and I\u2019m just going to suggest it to you. But you know, again, the way a lot of us [developers who are visually impaired] navigate the UI is usually with the tab key. And so it\u2019s so easy to just accidentally insert a suggestion\u201d - P5, Developer with no vision.", "page": 12, "bbox": []}, "seg_139": {"summary": "Summary: Users of screen magnification software encountered issues with the AI assistant because it would sometimes display information outside of their zoomed-in view. This meant that users were often unaware of suggestions provided by the AI assistant as the content was not visible on their screen.  This poses a challenge for accessibility when using AI assistants with screen magnification.", "text": "Similarly, P2 highlighted a significant challenge for users who rely on screen magnification: the AI assistant sometimes displayed information in areas of the screen that were out of view. Because these users were zoomed into a specific section of the screen, they often remained unaware when the AI assistant provided suggestions, as the content appeared outside their visible area:", "page": 12, "bbox": []}, "seg_140": {"summary": "Summary: A developer with low vision reports that zooming in on their screen to improve visibility obstructs their view of AI assistant suggestions, which are placed in an area outside of their zoomed-in view. This makes it difficult to access the AI assistant's features and suggests that for users who utilize screen zoom, the AI assistant's interface should be positioned more centrally. The user implies that the current placement hinders accessibility for zoomed-in users.", "text": "\u201c...If I, for example, zoom in like this [the participant zoomed into a specific screen area], I often don\u2019t get any information if there\u2019s some programming error or something going on in this area [the participant pointed to another part of the screen]. When I look at this [the zoomed-in area] and I just remember every time I have to go this way [The participant pointed to the unseen screen area where Copilot placed suggestions.] Sometimes I don\u2019t see that [AI suggestion] if I zoom in. So for people who are using Zoom, it will be better that it [AI assistant] will show up in the middle of the program because I don\u2019t see it [AI assistant] when it\u2019s in this area [zoomed-in area of the screen]. If there\u2019s something I have to install, for example, to run anything.\u201d - P2, Developer with low vision.", "page": 12, "bbox": []}, "seg_141": {"summary": "Summary: This text segment indicates that the study's participant experiences corroborate previous research on the difficulties visually impaired developers face when context-switching.  Specifically, the findings are presented as an extension of the work conducted by Albusays et al. [6].", "text": "These participant experiences align with and extend prior research on context-switching challenges for developers who are visually impaired. Our findings build upon the work of Albusays et al. [ 6 ], who conducted interviews and Manuscript submitted to ACM", "page": 12, "bbox": []}, "seg_143": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 13, "bbox": []}, "seg_144": {"summary": "Summary: A study on blind software developers found that AI-assisted coding environments exacerbate code navigation difficulties compared to traditional IDEs.  While previous research focused on challenges in traditional IDEs, this study reveals that AI introduces additional interface elements and interaction modes that complicate navigation and workflow.  This increased complexity stems from the need for frequent context switching between manual code and AI-generated content, disrupting established navigation strategies for visually impaired developers.", "text": "observations with blind software developers to examine code navigation difficulties in traditional IDEs. Their study identified the challenges blind developers face when moving between different sections of a program. Our research expands on these insights by showing how AI-assisted coding environments amplify these difficulties by introducing additional interface elements and new interaction modes, further complicating navigation and workflow continuity. While Albusays et al. [ 6 ] focused on traditional IDEs, our study reveals that AI-assisted coding environments introduce an additional layer of complexity by requiring developers to frequently switch contexts within the same application. This added complexity exacerbates the already challenging task of code navigation for developers who are visually impaired. The dynamic nature of AI-generated suggestions and the frequent need to shift focus between manually written code and AI-generated content disrupt the mental models and navigation strategies that blind developers have developed.", "page": 13, "bbox": []}, "seg_145": {"summary": "Summary: Activity Theory suggests that context switching challenges for screen reader users of AI coding assistants arise from contradictions in user-AI interaction. This is mainly because AI coding assistants are designed for visual interfaces, which is incompatible with the nonvisual navigation methods employed by screen reader users.  Further factors also contribute to this mismatch.", "text": "From an Activity Theory perspective, the challenges of context switching reveal contradictions in the interaction between the user and the AI coding assistant. These contradictions likely arise because AI coding assistants are primarily designed for visual interaction, which conflicts with the sequential, nonvisual navigation methods used by screen reader users. However, several additional factors contribute to this misalignment:", "page": 13, "bbox": []}, "seg_146": {"summary": "Summary: Navigating different contexts within AI-assisted environments requires multiple keystrokes. This necessity could potentially slow down user workflows and make context switching less efficient.", "text": "\u2022 The need for multiple keystrokes to move between different contexts within the AI-assisted environment.", "page": 13, "bbox": []}, "seg_147": {"summary": "Summary: Unexpected behaviors from AI systems are disrupting the established navigation patterns of developers who are visually impaired. This interference poses challenges for these developers in their work.", "text": "\u2022 Unexpected AI behaviors that interfere with established navigation patterns of developers who are visually impaired.", "page": 13, "bbox": []}, "seg_148": {"summary": "Summary: Screen magnification can lead to information being displayed outside of the visible screen area. This makes the information inaccessible to the user who is relying on magnification.", "text": "\u2022 Information being displayed in inaccessible areas due to screen magnification.", "page": 13, "bbox": []}, "seg_149": {"summary": "Summary: The design of the tool is not suitable for visually impaired developers. This incompatibility negatively impacts their concentration and ability to work efficiently.", "text": "This mismatch between the tool\u2019s design and the needs of developers who are visually impaired disrupts their ability to stay focused and maintain a smooth workflow.", "page": 13, "bbox": []}, "seg_150": {"summary": "AI coding assistants, despite offering greater control and efficiency, can overwhelm developers with constant suggestions, leading to cognitive strain.  This constant stream of AI-generated \"ghost text\" creates a tension between leveraging AI and maintaining mental focus. Consequently, developers express a need for \"AI timeouts,\" periods of coding without AI intervention, to regain control and focus.", "text": "4.1.3 AI Timeouts . While AI coding assistants gave developers greater control over their code and reduced the need for manually completing every task, they also introduced cognitive challenges. Participants specifically expressed a need for moments of disconnection from the AI. The constant stream of AI-generated suggestions\u2014especially the frequent appearance of ghost text \u2014created a trade-off between maintaining control and mental focus. This tension led to a key theme in our study: participants\u2019 desire for what we call \"AI timeouts\" \u2014periods of uninterrupted coding without AI intervention. For instance, participant P1, a braille display user, stressed the importance of having control over when AI suggestions appear. They found that frequent AI interventions disrupted their thought process, stating:", "page": 13, "bbox": []}, "seg_151": {"summary": "Summary: A developer with no vision states that they turn off speech functionality when ghost text appears. This allows them time to think and process the information presented.", "text": "\u201cWhat I usually do when this happens [when ghost text is presented] is I\u2019ll turn speech off for a bit so I can think....\u201d - P1, Developer with no vision.", "page": 13, "bbox": []}, "seg_152": {"summary": "Participant P1's strategy of turning off speech to think exemplifies the problem of information overload for screen-reader users.  Research by Ahmed et al. [3] indicates that users with visual impairments often experience cognitive overload because they must listen to content to assess its relevance, a challenge in non-visual interfaces. This highlights the difficulty screen-reader users face in efficiently processing information.", "text": "Participant P1\u2019s strategy of turning off speech to think highlights the issue of information overload , a challenge explored by Ahmed et al. [ 3 ] in their research on non-visual interfaces. Ahmed et al. explain that screen-reader users often struggle to determine the relevance or importance of content without first listening to at least some of it. This necessity frequently results in cognitive overload for users who are visually impaired.", "page": 13, "bbox": []}, "seg_153": {"summary": "AI-assisted coding environments can overwhelm users with continuous AI-generated suggestions, particularly through speech output, as illustrated by one user turning off speech to manage the influx of information. This strategy of creating an \"AI timeout\" allows users to pause and process information effectively.  This observation aligns with research suggesting that slower-paced interactions can improve usability.", "text": "In AI-assisted coding environments, this issue becomes even more pronounced. P1\u2019s experience illustrates how the continuous stream of AI-generated suggestions, conveyed through speech output, can overwhelm cognitive capacity. By choosing to \u201cturn speech off for a bit,\u201d P1 effectively creates a temporary barrier\u2014an AI timeout \u2014allowing them to pause, process information, and make decisions without the constant influx of new suggestions. This aligns with findings by Bigham et al. [ 16 ], who suggest that slower-paced interactions can enhance usability for visually impaired users.", "page": 13, "bbox": []}, "seg_154": {"summary": "Summary: A manuscript has been submitted to ACM for review.", "text": "Manuscript submitted to ACM", "page": 13, "bbox": []}, "seg_156": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 14, "bbox": []}, "seg_157": {"summary": "Summary: AI timeouts are not just for managing ghost text suggestions, but also for enabling periods of coding without any AI interference.  Participants, like P7, highlighted the need for extended uninterrupted coding sessions free from AI input. This allows developers to focus and work without AI distractions.", "text": "The concept of AI timeouts extends beyond merely managing ghost text suggestions. Some participants expressed a need for extended periods of uninterrupted coding without any AI input. For instance, P7 emphasized the importance of having the option to engage in uninterrupted coding sessions, stating:", "page": 14, "bbox": []}, "seg_158": {"summary": "Summary: This developer desires a coding mode that allows for uninterrupted coding within the IDE, even if it results in buggy code initially.  They prioritize getting their thoughts down and plan to address errors later using shortcuts, rather than having AI assistance interrupt their flow during the initial coding process.  Essentially, they want a \"get it done now, fix it later\" mode without AI interference.", "text": "\u201cWell, hey, can we get a mode that says, hey, I just want to get all my code done. I just want to write code. I don\u2019t care right now. I don\u2019t care how buggy it is right now. I really don\u2019t care. But I want to stay in the IDE because when I\u2019m done, I\u2019m going to use a shortcut and I\u2019m going to go through, I\u2019m going to find all my errors and I\u2019ll fix them, but I don\u2019t want it [AI assistant] to get in the way [...] But let me just get my thoughts out. \u201d - P7, Developer with no vision.", "page": 14, "bbox": []}, "seg_159": {"summary": "Summary: This text segment explores the issue of cognitive dissonance arising when AI assistance operates too quickly for the user's cognitive pace.  Participant P7's experience illustrates this point, suggesting a need for AI timeouts to avoid disrupting the user's thought process, while participant P1 offers a contrasting view of AI as a learning catalyst.  The segment highlights differing user experiences with AI assistance speed.", "text": "This experience highlights the potential for cognitive dissonance when AI assistance operates at a faster pace than the developer\u2019s thought process. Although the AI\u2019s suggestion may have been useful, it momentarily disrupted P7\u2019s train of thought, creating a mismatch between the AI\u2019s proactive assistance and the participant\u2019s cognitive workflow. While P7\u2019s experience underscores the need for AI timeouts, it is important to note that not all participants shared this sentiment. P1\u2019s reflection, for example, illustrates a different perspective\u2014one where AI acts as a catalyst for learning and expanding problem-solving approaches:", "page": 14, "bbox": []}, "seg_160": {"summary": "Summary: The developer was surprised by the AI assistant's approach to parsing and converting the date in their coding task, as it preempted a step they had planned to do later.  Initially, the developer focused on extracting the date and then calculating a year later, but the AI assistant first converted the date into a standard format, which the developer realized was a logical and efficient step.  This experience highlighted the AI assistant's ability to anticipate necessary steps in the coding process.", "text": "\u201cI think it [AI assistant] was a couple steps ahead of me now and again because I was sort of just expecting, okay, I need to get that date [date requested in their coding task], and then I\u2019m going to need to somehow get a year later out of it. And I didn\u2019t actually really consider that you\u2019d have to sort of parse that date and convert it into an actual date first. [...] So when it [AI assistant] made a constructor and just transformed it into month, day, year, I\u2019m like, okay, well, that\u2019s not immediately what I had in mind, but on second thought, that does make a lot of sense. [...] I probably would have done that as a next step myself, but it just sort of preempted me on there.\u201d - P1, Developer with no vision.", "page": 14, "bbox": []}, "seg_161": {"summary": "Summary: This text highlights the connection between AI coding assistants and Vygotsky's Zone of Proximal Development, emphasizing that AI should guide users to more advanced solutions.  It stresses the importance of dynamically adjusting AI intervention to balance cognitive challenge and prevent user overwhelm.  Adaptive AI assistance is crucial for creating an enriching coding experience that supports problem-solving without causing excessive cognitive load.", "text": "This experience aligns with Vygotsky\u2019s concept of the Zone of Proximal Development [ 102 ], which suggests that learning is most effective when guidance bridges the gap between what a learner can do independently and what they can achieve with support. In this case, the AI guided P1 toward a more advanced solution. However, maintaining a balance between beneficial cognitive challenges and overwhelming disruptions is crucial. While this instance demonstrated a positive outcome, it highlights the need for AI systems that can dynamically adjust their level of intervention based on the user\u2019s expertise and cognitive load. By incorporating adaptive assistance, AI coding assistants can create a more effective and enriching coding experience\u2014one where developers are both supported and challenged in a way that enhances their problem-solving abilities without causing excessive cognitive overload.", "page": 14, "bbox": []}, "seg_162": {"summary": "Summary: P4's response reveals that AI assistants can promote the early adoption of best coding practices among developers. Additionally, AI timeouts can be beneficial by providing developers with time to consider various aspects of their code before applying AI suggestions. These insights highlight the potential of AI to improve coding workflows and developer reflection.", "text": "Conversely, P4\u2019s response to how AI suggestions influenced their planning highlights two key insights. First, AI assistants can encourage developers to adopt best coding practices earlier in the development process. Second, AI timeouts could play a role in giving developers the necessary time to reflect on different aspects of their program before implementing AI-generated suggestions:", "page": 14, "bbox": []}, "seg_163": {"summary": "Summary: A developer with low vision appreciated the AI assistant suggesting guard clauses, which are best practices often considered during unit testing.  Inspired by Bob Martin's unit testing hat analogy, the developer indicated that the AI helped them think about these important edge cases earlier in the development process than they normally would.  The developer welcomed this proactive assistance from the AI.", "text": "\u201c...[I would] not have remembered to put those guard clauses in until I got, like, my unit testing hat on [the participant was pointing to AI-generated guard clauses, a best practice for handling edge cases]. I\u2019ve seen some videos of like, I think Uncle Bob, Bob Martin did some unit testing videos where he literally had a hat that he would flip around and it was two hats put together and he\u2019d be like, coder, unit test, or coder and those kind of things of like, oh, I wasn\u2019t there yet [was not thinking about guard clauses yet], but if you [AI assistant] want to help me get there early, that\u2019s fine. That\u2019s great.\u201d - P4, Developer with low vision.", "page": 14, "bbox": []}, "seg_164": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 14, "bbox": []}, "seg_166": {"summary": "This text segment discusses the impact of generative AI coding assistants on developers who are visually impaired.  It likely explores how these tools affect their coding practices and workflows, potentially highlighting both benefits and challenges.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 15, "bbox": []}, "seg_167": {"summary": "Summary: AI has the potential to merge traditional coding phases by offering proactive suggestions, as seen with guard clauses eliminating the need for timeouts between coding and testing.  However, developers have varied workflow preferences, necessitating flexible AI timeouts that allow control over AI interventions rather than complete disengagement.  This adaptability is crucial for accommodating diverse coding styles and needs, particularly for developers with visual impairments who may rely on structured routines.", "text": "This reflection highlights how AI can blur the traditional boundaries between different coding phases, potentially reducing the need for certain AI timeouts. In this case, the AI\u2019s proactive suggestions for guard clauses eliminated the need for a \u201ctimeout\u201d between the coding and testing phases by integrating best practices early in the development process. However, this scenario also emphasizes the importance of \u201cflexible AI timeouts\u201d. While P4 valued the early inclusion of guard clauses, other developers might prefer to maintain distinct mental phases in their coding workflow. In this context, AI timeouts do not necessarily mean disengaging from AI assistance entirely but rather enabling developers to control when and how AI interventions occur. By allowing developers to schedule AI timeouts or receive advanced suggestions at specific points in their workflow, AI-assisted coding tools can accommodate different coding styles and personal preferences. This flexibility could be particularly beneficial for developers who are visually impaired, as they may have structured routines or mental models for managing distinct phases of the coding process [ 42 , 50 ]. Providing adaptive AI interaction could enhance productivity without disrupting established workflows.", "page": 15, "bbox": []}, "seg_168": {"summary": "Summary: This text highlights the importance of carefully managing AI timeouts to achieve a balance.  The goal is for AI assistance to be beneficial and supportive, as illustrated by P3's experience, without becoming intrusive or hindering the user.  Therefore, a balanced approach is crucial to optimize AI's helpfulness and minimize potential disruptions.", "text": "P3\u2019s experience further illustrates the need for a balanced approach to AI timeouts, ensuring that AI assistance is helpful without becoming disruptive:", "page": 15, "bbox": []}, "seg_169": {"summary": "Summary: A developer with low vision finds the AI assistant very helpful, especially for tasks like datetime manipulation, which they often struggle to remember.  They appreciate the AI assistant's ability to understand the context of their programming language and quickly provide solutions, saving them time spent searching documentation. This allows them to bypass the need to repeatedly consult online resources.", "text": "\u201cHonestly, I think it\u2019s [AI assistant] super helpful because like, I\u2019m going to be honest, like you were saying before, I always forget datetime manipulation. [...] So having the ability to just say what I want, have it [AI assistant] understand the context of the programming language I\u2019m in and help me kind of get there quicker without having to kind of start searching around the web, figuring out the reading documentation again for the datetime methods. Like I have to all the time.\u201d - P3, developer with low vision.", "page": 15, "bbox": []}, "seg_170": {"summary": "Developers find AI useful for tasks like syntax recall and code manipulation, providing helpful support during coding.  However, for complex problem-solving and creative tasks, developers may prefer to temporarily disable AI to work independently. This suggests a nuanced approach to AI adoption, leveraging its strengths while maintaining autonomy for certain tasks.", "text": "As the quote illustrates, developers may choose to keep the AI when it provides helpful support, such as recalling correct syntax or assisting with complex code manipulations. However, for tasks requiring deep problem-solving or creative thinking, they may prefer to use AI timeouts to independently work through solutions.", "page": 15, "bbox": []}, "seg_171": {"summary": "The concept of \"AI timeouts\" emerges from contradictions in developer workflows when using tools like Copilot.  While intended to assist coding, Copilot's constant interventions can disrupt developers who need uninterrupted periods for processing and structuring information. This highlights the need for AI systems to be more adaptable, calibrating their assistance based on individual user needs for periods of focused work.", "text": "The concept of \u201cAI timeouts\u201d reflects contradictions within the activity system, arising from misalignments between the subject (developers), the object (coding tasks), and the tool (Copilot). While Copilot serves as a mediating artifact designed to facilitate coding, its dynamic and sometimes intrusive interventions create tensions that can disrupt developers\u2019 workflows. This contradiction likely arises because the tool\u2019s design assumes that continuous AI assistance improves productivity, whereas developers might actually need uninterrupted periods to process and structure information. These tensions highlight the necessity of adaptable AI systems that calibrate their level of intervention based on the user\u2019s needs.", "page": 15, "bbox": []}, "seg_172": {"summary": "Summary:  The study found that AI coding assistants enhance coding efficiency for visually impaired developers, despite challenges like context switching. This efficiency improvement is attributed to two main factors.  These findings highlight a key benefit of AI tools in coding accessibility.", "text": "4.1.4 AI-Assisted Coding Efficiency . While our study highlighted challenges like context switching, it also revealed key benefits of AI coding assistants, particularly in enhancing coding efficiency for developers who are visually impaired. This efficiency improvement stemmed from two main factors:", "page": 15, "bbox": []}, "seg_173": {"summary": "Summary: Copilot offers proactive code generation, minimizing manual coding efforts.  Additionally, it serves as an accessible and consistently available coding partner, providing immediate and non-judgmental assistance.", "text": "(1) Copilot\u2019s Proactive Code Generation , which anticipated and generated relevant code, reducing manual effort. (2) Copilot as an Accessible, Always-Available Coding Partner , providing instant non-judgmental support.", "page": 15, "bbox": []}, "seg_174": {"summary": "Proactive code generation in the AI assistant significantly boosts user productivity by anticipating their coding needs.  Participants noted that the AI proactively generated code to resolve issues or introduce new features, sometimes even before the user explicitly considered them.  For example, one participant described the AI intelligently parsing and formatting a date, which streamlined their workflow.", "text": "Proactive Code Generation . Participants highlighted how the AI assistant boosted their productivity by anticipating their needs\u2014proactively generating code that resolved existing issues or introduced features they had not initially considered. For instance, P7 described a moment when the AI assistant demonstrated foresight by automatically parsing and transforming a date into a usable format. Upon reflection, P7 realized that this step aligned perfectly with their next intended action, streamlining their workflow and reducing the need for manual adjustments:", "page": 15, "bbox": []}, "seg_175": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 15, "bbox": []}, "seg_177": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 16, "bbox": []}, "seg_178": {"summary": "Summary: A developer using an AI assistant for a coding task realized the AI was more efficient by parsing the user-provided date and converting it into a date object, a step the developer had not initially considered.  The developer acknowledged the AI's approach was logical and something they would have likely done later, but the AI preempted them in the process. This highlights the AI's ability to anticipate necessary steps in coding tasks.", "text": "\u201cIt [AI assistant] was a couple steps ahead of me. [The participant\u2019s coding task involved calculating the number of days until a birthday provided by an end-user.] I was initially focused on getting the date and calculating a year later, but I didn\u2019t actually really consider that you\u2019d have to sort of parse that date [date given by the end-user] and convert it into an actual date first [converting to an actual date was something the generative AI did for them]. That makes perfect sense in hindsight [...] So when it [AI assistant] made it a constructor and just transformed it into month, day, year, I\u2019m like, okay, well, that\u2019s not immediately what I had in mind, but on second thought, that does make a lot of sense. [...] I probably would have done that as a next step myself, but it just sort of preempted me on there.\u201d - P7, Developer with no vision.", "page": 16, "bbox": []}, "seg_179": {"summary": "Summary: Participant P1 valued the AI's proactive code generation capabilities.  They specifically highlighted the AI's ability to anticipate their development steps and align with their intended coding goals. This proactive nature of the AI was seen as a significant benefit.", "text": "Similarly, P1 emphasized their appreciation for the AI\u2019s ability to proactively generate code, particularly in how it anticipated their next steps and aligned with their intentions as a developer:", "page": 16, "bbox": []}, "seg_180": {"summary": "Summary: This developer is experimenting with Copilot's \"auto-modify\" feature and is impressed with the AI's code generation capabilities.  Upon reviewing the AI-generated code, the developer expresses astonishment, stating it is \"scarily good\" and exactly what they would have done themselves. This highlights the perceived accuracy and usefulness of the AI tool in coding tasks.", "text": "\u201cI\u2019m gonna try something else. I\u2019m actually very curious [...] I\u2019ll auto modify that [The participant used a Copilot command to \u2019auto-modify\u2019 their code, allowing the AI to proactively edit and expand their initial work]. That sounds good [The participant was reviewing the code generated by the AI\u201d]. There are days until your next birthday. Yeah, format days until next birthday [The participant continued reviewing the code generated by the AI.] That [code generated by the AI] is scarily good. Actually that [AI generated code] is exactly what I was going to do.\u201d - P1, Developer with no vision.", "page": 16, "bbox": []}, "seg_181": {"summary": "Developers with visual impairments face challenges in traditional coding environments, requiring them to navigate complex code structures and documentation. AI-driven code generation offers a promising solution by proactively predicting coding needs and generating relevant snippets. This technology can significantly reduce cognitive load and improve efficiency for visually impaired developers by minimizing the need for extensive manual code exploration.", "text": "Prior research has documented the challenges that developers with visual impairments face when navigating and understanding code structures in traditional development environments [ 5 , 11 , 78 ]. Given these challenges, we argue that AI\u2019s proactive code generation holds potential for this population. Developers who are visually impaired often need to traverse multiple documentation pages or code files to construct appropriate solutions\u2014a process that is not only time-consuming but also more prone to errors when relying on screen readers [ 6 , 20 ]. In this context, AI\u2019s ability to predict coding needs and generate relevant code snippets is especially valuable, as it can reduce cognitive load and enhance efficiency for developers who are visually impaired.", "page": 16, "bbox": []}, "seg_182": {"summary": "Summary: Participants viewed the AI assistant as a helpful and always-available coding partner that supported them in their coding tasks and boosted their productivity. They appreciated the AI's reliability in providing quick answers and relevant code snippets, likening it to a knowledgeable and readily accessible colleague.", "text": "AI as an Accessible, Always-Available Coding Partner. Some participants described the AI assistant as a supportive \u2019buddy\u2019 that guided them through their coding tasks, providing assistance and enhancing their productivity. They appreciated having an AI-powered companion they could rely on for coding assistance, providing quick answers and relevant code snippets when needed. For example, Participant P3 likened the AI assistant to a knowledgeable colleague who is readily available to offer solutions and assist with coding challenges:", "page": 16, "bbox": []}, "seg_183": {"summary": "Summary: A developer with low vision (P3) describes using an AI coding assistant as a helpful \"buddy\" for quickly answering coding questions. They use it to ask about specific functions or methods needed for tasks, such as converting strings for date-related coding.", "text": "\u201c[with the AI coding assistant] you have that kind of buddy to kind of ask real quick, you know, what was that? You know, what was the daytime method that I needed to use to convert that particular string into the right thing? [The participant refers to asking the AI about functions and methods needed for the date-related coding task]\u201d - P3, Developer with low vision.", "page": 16, "bbox": []}, "seg_184": {"summary": "Summary: Participants generally agreed that the AI assistant was helpful for software development.  However, individuals relied on it to different extents, with some, such as P6, using it for nearly all of their work.", "text": "All participants agreed that the AI assistant was beneficial in helping them complete their software development tasks. However, the degree of reliance on the assistant varied among individuals. Some, like P6, depended on it extensively, using it to complete nearly their entire assigned workload:", "page": 16, "bbox": []}, "seg_185": {"summary": "Summary: A study participant described using Copilot to complete a coding task by simply pasting the instructions into the chat interface.  The AI assistant provided a nearly complete implementation, requiring only very minor tweaks.  This suggests Copilot's effectiveness in generating code solutions with minimal user effort.", "text": "\u201cI just copy and pasted the whole task [instructions for a coding task assigned in our study] into chat [Copilot\u2019s chat interface], and it [AI assistant] gave me the whole implementation that we had to do. Very, very minor tweaks left to do. I mean, the date formats worked out, right? [The participant was assigned a coding task", "page": 16, "bbox": []}, "seg_186": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 16, "bbox": []}, "seg_188": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 17, "bbox": []}, "seg_189": {"summary": "Summary: A developer with low vision utilized an AI assistant for coding tasks, primarily around date formats.  The developer reported that the AI assistant handled the majority of the coding, requiring them only to add comments and make minor adjustments. This highlights the AI assistant's efficiency in code generation.", "text": "involving date formats.] Essentially we just added comments and tinkered with the code [...] But otherwise, it [AI assistant] just did everything for us.\u201d - P6, Developer with low vision.", "page": 17, "bbox": []}, "seg_190": {"summary": "Summary: Participants with limited prior experience with AI assistants recognized the technology's potential to speed up coding tasks.  This positive experience increased their interest in learning more about and using AI assistants in the future.", "text": "Even participants with limited experience using AI assistants, such as P9, acknowledged its potential to accelerate coding tasks. Their experience in our study sparked greater interest in exploring the technology further:", "page": 17, "bbox": []}, "seg_191": {"summary": "Summary: The developer is interested in exploring AI coding assistants further and believes they have potential, especially for quick tasks such as writing functions. They highlight the time-saving aspect of using these tools for development work.", "text": "\u201cI would love to try and explore more [about AI coding assistants]. But, it [the AI coding assistant] has definitely a potential [...] just doing quick work, especially writing quick functions and doing also all those tasks [...] It\u2019s [the AI assistant is] bringing a lot of time saving.\u201d - P9, Developer with no vision.", "page": 17, "bbox": []}, "seg_192": {"summary": "Generative AI assistants offer significant advantages for visually impaired developers by providing continuous, non-judgmental support in software development tasks. This technology promotes greater independence and efficiency, allowing developers to overcome potential hesitations about seeking help from colleagues in the workplace.  Ultimately, generative AI has the potential to make software development more accessible to this group.", "text": "For developers who are visually impaired, this aspect of AI assistance could be particularly valuable, as it offers constant, non-judgmental support without requiring face-to-face interaction. This can be especially beneficial in workplace environments where they may feel hesitant to frequently ask colleagues for help [ 6 ]. Generative AI assistants have the potential to make software development more accessible and efficient, enabling developers to work more independently without relying on coworkers for assistance.", "page": 17, "bbox": []}, "seg_193": {"summary": "Copilot, viewed through the lens of Activity Theory, functions as a mediating artifact that transforms coding by making the AI assistant an active collaborator. This shift creates a dynamic interaction between the developer, coding task, and AI, where Copilot acts as a \"buddy\" to assist with complex problems and reduce cognitive load.  By integrating AI as an active partner, developers can navigate programming tasks with increased confidence and independence.", "text": "From the perspective of Activity Theory, Copilot serves as a mediating artifact that reshapes the coding activity by shifting its role from a passive tool to an active collaborator. Within the framework of Activity Theory, the interaction between the subject (developer), object (coding task), and tool (AI assistant) is no longer a linear process but a dynamic, adaptive exchange. Instead of developers following a rigid, step-by-step workflow, Copilot enables a more fluid interaction where AI-generated suggestions proactively shape the problem-solving process. Acting as a readily available and non-judgmental coding companion, Copilot can function as a \u201cbuddy\u201d that developers can turn to at any moment to clarify uncertainties, propose solutions, and reduce cognitive load. By integrating AI as an active collaborator within the activity system, Copilot can help developers navigate complex programming tasks with greater confidence and independence.", "page": 17, "bbox": []}, "seg_194": {"summary": "Insufficient content for summary.", "text": "5 DISCUSSION", "page": 17, "bbox": []}, "seg_195": {"summary": "AI coding assistants offer significant advantages for visually impaired developers, including enhanced capabilities and control.  However, these tools also introduce new accessibility challenges like context switching and managing AI-generated suggestions.  The authors argue for a new approach to accessibility in AI coding environments and propose design recommendations to ensure these tools are beneficial for all developers.", "text": "Our findings show that AI coding assistants provide powerful capabilities, new opportunities, and greater control for developers who are visually impaired, but they also introduce new accessibility challenges. Issues such as context switching and managing AI-generated suggestions highlight the need for a new approach to accessibility in AI coding environments. Building on Nielsen\u2019s argument that generative AI has introduced a new paradigm for HCI [ 68 ] and the design principles for generative AI tools outlined by Weisz et al. [ 104 ], we propose design recommendations to ensure AI coding assistants are accessible and beneficial for all developers.", "page": 17, "bbox": []}, "seg_196": {"summary": "Summary: This section will likely explore how AI coding assistants manage control and context switching.  It is expected to cover methods for controlling the AI's actions and strategies for managing different programming contexts within these assistants. Understanding these aspects is crucial for the effective operation of AI coding tools.", "text": "5.1 AI Control and Context-Switching Management in AI Coding Assistants", "page": 17, "bbox": []}, "seg_197": {"summary": "AI coding assistants offer visually impaired developers the advantage of focusing on strategic aspects of coding; however, they also present usability challenges.  Specifically, navigating between static code and dynamic AI windows causes frustrating context switching, disrupting workflow and cognitive focus for some developers.", "text": "Our study highlights the mixed experiences of developers who are visually impaired when using AI coding assistants. While these assistants empower developers by allowing them to focus on higher-level strategic thinking, they also introduce new challenges. Participants, such as P5 and P9, reported difficulties in navigating between their static code and the dynamic windows used for AI interactions. This constant context switching led to frustration, disrupting their workflow and breaking their cognitive focus.", "page": 17, "bbox": []}, "seg_198": {"summary": "Visually oriented IDEs pose challenges for visually impaired developers, causing a loss of control.  Screen readers, which read code sequentially, exacerbate difficulties with context switching for these developers. Tools like StructJumper and CodeTalk have been developed to address navigation issues in static coding environments.", "text": "Previous research highlights that the visually oriented nature of Integrated Development Environments (IDEs) presents challenges for developers who are visually impaired, often leading to a loss of control [ 6 , 22 , 96 ]. P1 also noted that screen readers require users to read code sequentially, making context switching particularly difficult [ 5 \u2013 7 , 59 , 90 ]. Several tools, such as StructJumper and CodeTalk, have been designed to aid navigation in traditional static coding", "page": 17, "bbox": []}, "seg_199": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 17, "bbox": []}, "seg_201": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 18, "bbox": []}, "seg_202": {"summary": "AI-assisted coding environments introduce challenges for visually impaired developers due to the dynamic nature of AI suggestions, contrasting with existing static code tools.  The need to constantly switch focus between manual code and AI-generated content increases cognitive load and disrupts workflow. This fragmented attention can negatively impact productivity, code quality, and trust in AI assistance for these developers.", "text": "environments [ 10 , 11 , 77 , 78 ]. However, these tools primarily focus on static code elements. In contrast, AI-assisted coding environments introduce new challenges by incorporating dynamic code elements [ 73 ]. Our findings reveal that developers who are visually impaired must now manage the additional complexity of interacting with AI-generated suggestions, making context switching even more difficult. Navigating between static code and AI-generated content requires frequent shifts in focus across different interface elements [ 66 , 73 ]. Developers must assess their own code, review AI-generated suggestions, and interact with various UI windows required for AI engagement [ 105 ]. This continuous back-and-forth process can increase cognitive load, forcing developers to constantly evaluate AI outputs while maintaining an understanding of their overall code structure [ 84 ]. As a result, the interplay between AI assistance and manual coding can fragment attention, disrupt workflow continuity, and impact productivity, code quality, and trust in the AI-generated content [ 55 , 73 , 103 ].", "page": 18, "bbox": []}, "seg_203": {"summary": "Summary: The research emphasizes the critical need for accessible coding tools tailored for dynamic environments, particularly for visually impaired developers.  These tools should streamline workflows, reduce AI-related interruptions, and enhance support for developers with visual impairments.  Addressing context switching and control will improve accessibility and effectiveness in AI-assisted coding.", "text": "Our findings highlight the urgent need for accessible tools specifically designed for dynamic coding environments. These tools should aim to streamline workflows, minimize AI disruptions, and provide better support for developers who are visually impaired. By addressing the challenges of context switching and maintaining control, such tools can help developers navigate AI-assisted coding environments more effectively and improve overall accessibility.", "page": 18, "bbox": []}, "seg_204": {"summary": "AI coding assistants present a dual impact for visually impaired developers, offering empowerment by enabling supervisory control over code, yet introducing challenges.  Specifically, context switching and navigation complexity emerged as significant workflow disruptions.  These findings highlight both the potential and the obstacles in leveraging AI coding assistants for accessibility in software development.", "text": "5.1.1 Addressing the Research Gap. Our research revealed both new accessibility opportunities and challenges in AI coding assistants for developers who are visually impaired. On one hand, AI coding assistants empowered developers by shifting their role toward supervisory control over the code. However, these benefits were accompanied by significant challenges, especially related to context switching, which disrupted their workflow and made navigation more complex.", "page": 18, "bbox": []}, "seg_205": {"summary": "AI-driven coding environments worsen existing accessibility barriers found in traditional IDEs due to their dynamic nature, creating challenges for visually impaired developers.  Current accessibility tools designed for static code are insufficient for these new AI-assisted workflows. This research highlights the urgent need for AI coding tools to be designed with accessibility at their core to support equitable participation in evolving coding practices.", "text": "While previous studies [ 6 , 22 , 96 ] have identified accessibility barriers in traditional IDEs\u2014such as visually-oriented interfaces and linear screen reader navigation for static code [ 5 , 7 , 59 , 90 ]\u2014our findings reveal that AI-driven coding environments exacerbate these issues. The dynamic nature of AI-assisted coding introduces additional complexities that amplify accessibility barriers. Existing tools like StructJumper and CodeTalk [ 10 , 11 , 77 , 78 ], designed for static code, may not fully support AI-driven workflows. Furthermore, prior research on AI-assisted coding has examined cognitive strain and productivity for general developers [ 55 , 66 , 73 , 105 ]. But, our work introduces an accessibility perspective. Frequent context switching and fragmented workflows present significant challenges for developers who are visually impaired, emphasizing the urgent need for AI tools designed with accessibility at their core to ensure equitable participation in evolving coding practices.", "page": 18, "bbox": []}, "seg_206": {"summary": "Summary: Based on research findings, this section proposes design recommendations to enhance AI-assisted coding environments, focusing on interaction continuity, personalization, and information management.  These recommendations are informed by principles like \"Design for Generative Variability\" and Activity Theory, aiming to better support developers who are visually impaired in coding tasks.", "text": "5.1.2 Design Recommendations. Based on our findings, we propose a set of design recommendations aimed at improving interaction continuity, personalization, and information management in AI-assisted coding environments. These recommendations build on established principles such as Design for Generative Variability by Weisz et al. [ 104 ], which emphasizes the importance of visualizing the user\u2019s journey to accommodate diverse needs. From an Activity Theory perspective, our recommendations seek to realign the AI coding assistant ( tool ) with developers who are visually impaired ( subject ) to help them successfully complete coding tasks ( object ).", "page": 18, "bbox": []}, "seg_207": {"summary": "Summary: Implementing a context history log is recommended to address workflow disruptions from AI-driven context switching. This feature would record user interactions with the AI, allowing developers to review previous steps.  It is especially beneficial for visually impaired users to maintain workflow continuity.", "text": "First, we recommend implementing a context history log [ 21 ] to mitigate workflow disruptions caused by AI-driven context switching. This feature would provide a sequential record of user interactions with the AI, allowing developers to review and revisit previous steps in their coding process. For visually impaired users, a structured history log could be especially valuable for maintaining workflow continuity and recovering from unexpected focus shifts.", "page": 18, "bbox": []}, "seg_208": {"summary": "Summary: Logging user behavior can enable the system to learn user preferences and adapt to individual needs.  For example, if a developer frequently uses inline chat for code explanations, the system can prioritize and streamline access to this feature. This adaptation aims to improve efficiency and reduce cognitive load for the user.", "text": "Additionally, this log could enhance the system\u2019s adaptability by learning from user behavior. For example, if a developer frequently accesses the inline chat for code explanations, the AI could prioritize and streamline access to this feature, reducing cognitive load and improving efficiency. By addressing workflow contradictions , a context history Manuscript submitted to ACM", "page": 18, "bbox": []}, "seg_210": {"summary": "Summary: This text segment discusses the impact of generative AI coding assistants. It specifically focuses on the effects these tools have on developers who are visually impaired.  The topic suggests an exploration of how AI coding assistants are changing the workflows and experiences for visually impaired programmers.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 19, "bbox": []}, "seg_211": {"summary": "A log system would empower developers to oversee their interactions with AI coding tools. This control would lead to more predictable and user-friendly AI-assisted coding experiences.  Ultimately, logging enhances the developer's ability to understand and manage the AI's role in the coding process.", "text": "log would ensure that developers maintain control over their interactions with the AI, making AI-assisted coding environments more predictable and user-friendly.", "page": 19, "bbox": []}, "seg_212": {"summary": "Visually impaired developers using screen readers struggle to efficiently navigate AI-generated suggestions due to the sequential nature of screen readers, unlike the visual scanning capabilities of sighted developers. To address this, the authors propose an interaction organizer. This tool would enable users to structure AI-generated suggestions more effectively, improving navigation.", "text": "Second, we propose integrating an interaction organizer to help mitigate the challenges caused by the sequential navigation constraints of screen readers. Unlike sighted developers, who can visually scan and filter information quickly, developers who are visually impaired must process content in a linear fashion, which can make it difficult to efficiently navigate AI-generated suggestions [ 20 ]. The interaction organizer would allow users to structure AI-generated suggestions more effectively by enabling them to:", "page": 19, "bbox": []}, "seg_213": {"summary": "Summary: Organize AI suggestions by grouping related items into folders or categories for better management.", "text": "\u2022 Group related AI suggestions into folders or categories.", "page": 19, "bbox": []}, "seg_214": {"summary": "Summary: Assigning meaningful labels and keywords to information facilitates quick and efficient retrieval. This practice ensures that users can easily locate and access the information they need when searching.", "text": "\u2022 Assign meaningful labels and keywords for quick retrieval.", "page": 19, "bbox": []}, "seg_215": {"summary": "Summary: This text segment recommends adding personal notes or comments to code that is generated by AI. This practice can help with understanding, modification, and future reference of the AI-produced code.", "text": "\u2022 Attach personal notes or comments to AI-generated code.", "page": 19, "bbox": []}, "seg_216": {"summary": "Summary: The interaction organizer streamlines developer workflows by managing and retrieving AI-generated outputs, reducing cognitive strain.  This is particularly beneficial for visually impaired developers by resolving conflicts between dynamic AI suggestions and sequential workflows.  Ultimately, this tool enhances the efficiency and accessibility of AI-assisted development by structuring and personalizing AI outputs.", "text": "By incorporating these capabilities, the interaction organizer would help developers streamline their workflow, making it easier to manage and retrieve AI-generated outputs while reducing cognitive strain. From an Activity Theory perspective, this feature helps resolve contradictions between the dynamic and non-linear nature of AI-generated suggestions and the sequential workflows that visually impaired developers rely on. By providing structured ways to categorize and personalize AI outputs, the interaction organizer improves the tool\u2019s role as a mediator in the coding process, making AI-assisted development more efficient and accessible.", "page": 19, "bbox": []}, "seg_217": {"summary": "Insufficient content for summary.", "text": "5.2 The Need for \"AI Timeouts\" in", "page": 19, "bbox": []}, "seg_218": {"summary": "Insufficient content for summary.", "text": "AI-Assisted Coding", "page": 19, "bbox": []}, "seg_219": {"summary": "AI assistance in coding environments for visually impaired developers can increase productivity by anticipating coding needs. However, this increased assistance can also lead to cognitive overload. To mitigate this, developers require AI timeouts to manage information and maintain focus during their workflow.", "text": "Our study uncovered a complex relationship between increased productivity and cognitive overload in AI-assisted coding environments for developers who are visually impaired. While participants reported productivity gains\u2014such as AI proactively anticipating their coding needs\u2014these benefits also introduced a challenge: the need for AI timeouts . These timeouts represent intentional breaks from AI interventions, allowing developers to manage information overload and maintain focus during their workflow.", "page": 19, "bbox": []}, "seg_220": {"summary": "AI-generated suggestions, especially inline ghost text, contributed to cognitive overload for users. This overload resulted in the necessity for AI timeouts.  These timeouts are temporary pauses in AI assistance designed to reduce mental strain and enable focused work.", "text": "The continuous stream of AI-generated suggestions, particularly inline ghost text , seemed to have contributed to cognitive overload among participants. This overload created a need for AI timeouts \u2014temporary pauses in AI interventions to allow for uninterrupted focus and reduce mental strain.", "page": 19, "bbox": []}, "seg_221": {"summary": "Summary: Activity Theory highlights a contradiction where AI assistants, intended to boost productivity, can disrupt focus, especially for visually impaired developers.  This aligns with existing research on information overload and is intensified by AI-generated content in coding environments.  The study emphasizes the need for controlled and customizable AI interactions to support focus and effective workflows for developers with visual impairments.", "text": "Activity Theory provides a useful framework for understanding this challenge, as it reveals a contradiction between the AI assistant\u2019s intended role of enhancing productivity and its unintended effect of disrupting focus. This finding aligns with prior research on information overload in visual interfaces [ 3 , 32 ], which shows that excessive information can overwhelm users, particularly those who are visually impaired. Our study extends this research by demonstrating how AI-generated content can intensify cognitive overload in AI-assisted coding environments. This underscores the importance of designing AI interactions that are more controlled and customizable, ensuring that developers who are visually impaired can maintain focus and effectively manage their workflows.", "page": 19, "bbox": []}, "seg_222": {"summary": "The need for AI timeouts in coding assistants differs among visually impaired developers, indicating the complexity of designing these tools.  Some users prefer proactive AI help, while others desire more control over AI interventions during coding. This variability emphasizes the necessity for flexible and personalized customization in AI coding assistants for visually impaired developers, rather than a standardized approach.", "text": "The need for AI timeouts varied among participants, highlighting the complexity of designing AI coding assistants for developers who are visually impaired. While some participants valued proactive AI assistance, others wanted greater control over when and how AI intervened in their coding process. This variation underscores the importance of flexible, personalized customization rather than a one-size-fits-all approach, which is often emphasized in existing research on accessible development tools [ 10 , 11 , 77 , 78 ].", "page": 19, "bbox": []}, "seg_223": {"summary": "Summary: A manuscript has been submitted to ACM, indicating it is being considered for publication or review.", "text": "Manuscript submitted to ACM", "page": 19, "bbox": []}, "seg_225": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 20, "bbox": []}, "seg_226": {"summary": "Summary:  AI-assisted coding environments could benefit from adaptive features that learn user preferences and dynamically adjust AI suggestions based on task complexity.  Providing developers with control over enabling or disabling AI assistance would further enhance autonomy and prevent cognitive overload. This approach ensures AI serves as a supportive tool aligned with user goals.", "text": "Building on Gajos et al.\u2019s adaptive interfaces concept [ 31 ], our findings suggest that AI-assisted coding environments could benefit from adaptive features. From an Activity Theory perspective, these adaptive interfaces help resolve contradictions by aligning AI mediation with user goals and preferences. Specifically, AI coding assistants could learn individual preferences for AI intervention, dynamically adjusting the frequency and type of AI suggestions based on task complexity. Additionally, providing easily accessible controls for enabling and disabling AI assistance would grant developers greater autonomy over their workflow. This approach ensures that AI remains a supportive tool rather than a source of cognitive overload.", "page": 20, "bbox": []}, "seg_227": {"summary": "Summary: This research introduces AI timeouts as a novel strategy to address cognitive overload experienced by visually impaired developers using AI coding assistants.  While AI enhances productivity, continuous AI-generated inputs can cause cognitive strain and disrupt focus. AI timeouts are proposed as an accessibility solution to balance AI assistance with the need to mitigate cognitive overload in dynamic AI-driven development environments.", "text": "5.2.1 Addressing the Research Gap. Our research introduces AI timeouts as a strategy to mitigate cognitive overload for developers who are visually impaired using AI coding assistants. Prior work highlights AI\u2019s ability to enhance productivity by automating repetitive tasks and offering anticipatory suggestions [ 4 , 70 , 106 ]. However, our findings reveal new challenges, such as cognitive strain from continuous AI-generated inputs like inline ghost text . Building on studies of information overload for visually impaired users [ 3 , 32 , 73 , 84 ], we extend this discussion to dynamic AI-driven environments, where proactive AI mediation can disrupt cognitive coherence [ 73 , 84 ]. Using Activity Theory, we frame this tension as a contradiction between AI assistance and focus disruption, proposing AI timeouts as a novel accessibility strategy to balance intervention and cognitive load.", "page": 20, "bbox": []}, "seg_228": {"summary": "Summary: Design recommendations for AI coding assistants for visually impaired developers emphasize aligning AI behavior with the user's mental model to improve accessibility and usability.  Customization is key, allowing users to adjust the AI's level of detail based on their needs and preferences, such as offering \"Low Detail\" and \"High Detail\" modes. This approach aims to reduce cognitive strain and ensure AI assistance complements individual problem-solving strategies.", "text": "5.2.2 Design Recommendations. To improve the accessibility and usability of AI coding assistants for developers who are visually impaired, we recommend design strategies aligned with the Design for Mental Models principle by Weisz et al. [ 104 ]. This principle emphasizes the importance of aligning AI behavior with the user\u2019s expectations to balance supportive AI intervention with user autonomy. One of our recommendation is to allow developers to customize the AI\u2019s behavior to match their mental model. For instance, tools could offer adjustable modes such as \"Low Detail\" and \"High Detail,\" enabling users to control the level of information the AI provides based on task complexity or cognitive load. This flexibility ensures that AI assistance aligns with individual problem-solving preferences, reducing unnecessary cognitive strain. Customization can also resolve contradictions between the user\u2019s mental model and the often unpredictable nature of AI-generated suggestions, which may sometimes provide excessive or distracting information.", "page": 20, "bbox": []}, "seg_229": {"summary": "Summary: The proposed system includes AI timeouts as a crucial feature. These timeouts are designed to fulfill two important functions within the system.", "text": "Another essential feature that we propose is the implementation of AI timeouts , which serve two key purposes:", "page": 20, "bbox": []}, "seg_230": {"summary": "Timeouts in AI coding tools allow developers to pause AI assistance, promoting independent problem-solving and preventing over-reliance on AI.  This feature gives developers greater control over the coding process and ensures AI assistance aligns with their individual goals, as supported by Activity Theory.  By providing space for reflection, timeouts help maintain developer autonomy.", "text": "\u2022 Preserving Autonomy : Timeouts allow developers to temporarily pause AI interventions, giving them space to reflect and solve problems independently. This can prevent over-reliance on AI and help maintain control over the coding process. From an Activity Theory perspective, these timeouts address contradictions by giving users greater control over the tool, ensuring AI assistance aligns with their goals and cognitive needs.", "page": 20, "bbox": []}, "seg_231": {"summary": "Pausing AI interventions during development enhances learning by allowing developers to strengthen their problem-solving skills and mental models. This active engagement promotes deeper understanding and reinforces coding abilities. This approach aligns with Activity Theory, which emphasizes user agency in shaping interactions with tools like Copilot.", "text": "\u2022 Enhancing Learning : By pausing AI interventions, developers have the opportunity to engage with their own problem-solving strategies. This reinforces their mental model, strengthens coding skills, and promotes deeper understanding. This aligns with Activity Theory\u2019s emphasis on fostering user agency, allowing developers to actively shape their interactions with Copilot.", "page": 20, "bbox": []}, "seg_232": {"summary": "Customizable AI behavior and strategic pauses can aid visually impaired developers by maintaining a consistent mental model of the coding environment. This predictability creates a more intuitive and personalized experience, bridging the gap between user cognition and AI functionality. Ultimately, these design features improve accessibility and productivity for developers.", "text": "For developers who are visually impaired, maintaining a clear and consistent mental model of the coding environment can be essential for effective work. Customizable AI behavior and strategic pauses could help ensure interactions remain predictable and aligned with user expectations, fostering a more intuitive and personalized experience. By supporting mental model alignment, these design features bridge the gap between user cognition and AI functionality, ultimately improving accessibility and productivity.", "page": 20, "bbox": []}, "seg_233": {"summary": "Summary: A manuscript has been submitted to the ACM.", "text": "Manuscript submitted to ACM", "page": 20, "bbox": []}, "seg_235": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 21, "bbox": []}, "seg_236": {"summary": "Insufficient content for summary.", "text": "5.3 Limitations and Future Work.", "page": 21, "bbox": []}, "seg_237": {"summary": "This study explores the experiences of visually impaired developers using AI coding assistants, revealing valuable insights into the accessibility of these tools. However, the findings are limited by a small sample size of 10 male participants, which restricts the generalizability of the results. Despite this limitation, the study offers foundational understanding of the challenges and opportunities AI coding assistants present for developers with visual impairments.", "text": "Our study provides valuable insights into the experiences of developers who are visually impaired using AI coding assistants. But it also has limitations. We interviewed a small group of 10 male participants, which restricts the diversity and generalizability of our findings. Studying emerging technologies like GitHub Copilot and recruiting a niche population, such as developers who are visually impaired, often results in a limited and homogeneous sample. However, despite participants\u2019 varying levels of experience with AI coding assistants, their shared challenges and opportunities provide foundational insights into how these tools impact accessibility.", "page": 21, "bbox": []}, "seg_238": {"summary": "Summary: The study's focus on only GitHub Copilot and participants' limited experience with the tool are key limitations.  Future research should investigate a wider variety of AI coding assistants and conduct longitudinal studies to understand long-term accessibility challenges and benefits for visually impaired developers. This will provide a more comprehensive understanding of AI's impact on accessibility in coding.", "text": "Additionally, our study focused exclusively on GitHub Copilot, meaning we did not explore other AI coding assistants that may have different features and accessibility considerations. Future research should examine a broader range of AI coding tools to gain a more comprehensive understanding of accessibility challenges across different platforms. Another limitation is that participants had limited prior exposure to GitHub Copilot, meaning our findings may not fully capture the long-term benefits and challenges of using AI coding assistants. Longitudinal studies are needed to investigate how developers who are visually impaired adapt to AI-assisted environments over time, as well as any lasting accessibility barriers or professional impacts that may emerge.", "page": 21, "bbox": []}, "seg_239": {"summary": "Insufficient content for summary.", "text": "6 CONCLUSION", "page": 21, "bbox": []}, "seg_240": {"summary": "This study explored the experiences of visually impaired developers using AI coding assistants, revealing a dual impact.  While AI tools offered enhanced control and support, they also introduced new accessibility challenges and cognitive demands.  The findings underscore the critical need to prioritize accessibility in the development of AI coding tools to ensure inclusivity in the tech industry.", "text": "We examined the experiences of 10 developers who are visually impaired as they interacted with an AI coding assistant. Our study highlights the dual impact of AI coding assistants on developers who are visually impaired. While these tools enhance control and provide valuable support, they also introduce new accessibility challenges and cognitive demands. Participants\u2019 experiences underscore the need to rethink accessibility in AI-driven coding environments. As AI continues to reshape software development, the design decisions made today will determine the inclusivity of the tech industry for years to come. Our findings call on researchers, designers, and industry leaders to make accessibility a priority in the development of next-generation AI tools, ensuring they empower all developers and foster equal opportunities in the field.", "page": 21, "bbox": []}, "seg_241": {"summary": "Summary: The authors express gratitude to anonymous reviewers and software developers for their contributions to the paper and study, respectively.  They also acknowledge partial funding for this work from NSF grants 2339443 and 2403252.", "text": "ACKNOWLEDGMENTS. Special thanks to all the anonymous reviewers who helped us to strengthen the paper as well as the software developers who participated in the study. This work was partially supported by NSF grants 2339443 and 2403252.", "page": 21, "bbox": []}, "seg_243": {"summary": "Summary: This bibliographic entry refers to a paper published in 2018 by Shadi Abou-Zahra, Judy Brewer, and Michael Cooper.  The paper, titled \"Artificial intelligence (AI) for web accessibility: Is conformance evaluation a way forward?\", was featured in the Proceedings of the 15th international web for all conference, spanning pages 1-4.", "text": "[1] Shadi Abou-Zahra, Judy Brewer, and Michael Cooper. 2018. Artificial intelligence (AI) for web accessibility: Is conformance evaluation a way forward?. In Proceedings of the 15th international web for all conference . 1\u20134.", "page": 21, "bbox": []}, "seg_244": {"summary": "Summary: This research paper by Rudaiba Adnin and Maitraye Das, published in the Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility in 2024, explores how blind people use and understand generative AI tools.  The paper's title suggests it investigates the perspectives of blind individuals on generative AI as a source of knowledge.", "text": "[2] Rudaiba Adnin and Maitraye Das. 2024. \" I look at it as the king of knowledge\": How Blind People Use and Understand Generative AI Tools. In Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility . 1\u201314.", "page": 21, "bbox": []}, "seg_245": {"summary": "Summary: Ahmed et al. published a paper in the 2012 Proceedings of the International Cross-Disciplinary Conference on Web Accessibility.  Their paper, titled \"Why read if you can skim: towards enabling faster screen reading,\" explores methods for faster screen reading, suggesting skimming as a viable approach.", "text": "[3] Faisal Ahmed, Yevgen Borodin, Yury Puzis, and IV Ramakrishnan. 2012. Why read if you can skim: towards enabling faster screen reading. In Proceedings of the International Cross-Disciplinary Conference on Web Accessibility . 1\u201310.", "page": 21, "bbox": []}, "seg_246": {"summary": "Al Naqbi, Bahroun, and Ahmed published a 2024 research paper in Sustainability that comprehensively reviews the literature on enhancing work productivity through generative artificial intelligence.", "text": "[4] Humaid Al Naqbi, Zied Bahroun, and Vian Ahmed. 2024. Enhancing work productivity through generative artificial intelligence: A comprehensive literature review. Sustainability 16, 3 (2024), 1166.", "page": 21, "bbox": []}, "seg_247": {"summary": "Albusays and Ludi's 2016 paper, presented at the 9th International Workshop on Cooperative and Human Aspects of Software Engineering, explores programming challenges faced by developers with visual impairments.", "text": "[5] Khaled Albusays and Stephanie Ludi. 2016. Eliciting programming challenges faced by developers with visual impairments: exploratory study. In Proceedings of the 9th International Workshop on Cooperative and Human Aspects of Software Engineering . 82\u201385.", "page": 21, "bbox": []}, "seg_248": {"summary": "Summary: Albusays, Ludi, and Huenerfauth's 2017 paper explores the challenges blind software developers face in code navigation through interviews and observations.  This research was presented at the International ACM SIGACCESS Conference on Computers and Accessibility and published on pages 91-100.", "text": "[6] Khaled Albusays, Stephanie Ludi, and Matt Huenerfauth. 2017. Interviews and observation of blind software developers at work to understand code navigation challenges. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility . 91\u2013100.", "page": 21, "bbox": []}, "seg_249": {"summary": "Summary: Khaled L Albusays published research in 2020 at the Rochester Institute of Technology focusing on sonification as a code navigation aid.  The study investigates how sound can improve programming structure readability and understandability for non-visual users. This work explores alternative methods for accessing and comprehending code for individuals with visual impairments.", "text": "[7] Khaled L Albusays. 2020. The Role of Sonification as a Code Navigation Aid: Improving Programming Structure Readability and Understandability For Non-Visual Users . Rochester Institute of Technology.", "page": 21, "bbox": []}, "seg_250": {"summary": "Summary: Aljarallah and Dutta's 2024 Journal of Disability Research article is a systematic review focused on methods for developing computer programming skills in visually impaired students.  This research contributes to the field of accessible computer science education.", "text": "[8] Nasser Ali Aljarallah and Ashit Kumar Dutta. 2024. A Systematic Review on Developing Computer Programming Skills for Visually Impaired Students. Journal of Disability Research 3, 2 (2024), 20240018.", "page": 21, "bbox": []}, "seg_251": {"summary": "Summary: A manuscript has been submitted to the ACM. This indicates that the work is now under consideration by the Association for Computing Machinery for publication or review.", "text": "Manuscript submitted to ACM", "page": 21, "bbox": []}, "seg_253": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 22, "bbox": []}, "seg_254": {"summary": "Summary: Antonelli et al. published a paper in 2019 discussing the challenges of automatically evaluating the accessibility of rich internet applications.  This paper appeared in the Proceedings of the 37th ACM International Conference on the Design of Communication.", "text": "[9] Humberto Lidio Antonelli, Leonardo Sensiate, Willian Massami Watanabe, and Renata Pontin de Mattos Fortes. 2019. Challenges of automatically evaluating rich internet applications accessibility. In Proceedings of the 37th ACM International Conference on the Design of Communication . 1\u20136.", "page": 22, "bbox": []}, "seg_255": {"summary": "Summary: In 2018, Ameer Armaly, Paige Rodeghero, and Collin McMillan published a paper titled \"Audiohighlight: Code skimming for blind programmers.\"  This paper was presented at the IEEE International Conference on Software Maintenance and Evolution (ICSME). The publication is included in the IEEE proceedings, pages 206-216.", "text": "[10] Ameer Armaly, Paige Rodeghero, and Collin McMillan. 2018. Audiohighlight: Code skimming for blind programmers. In 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME) . IEEE, 206\u2013216.", "page": 22, "bbox": []}, "seg_256": {"summary": "Baker, Milne, and Ladner presented their 2015 paper, \"Structjumper: A tool to help blind programmers navigate and understand the structure of code,\" at the ACM Conference on Human Factors in Computing Systems.  This paper introduces Structjumper, a tool designed to aid blind programmers in comprehending code structure.", "text": "[11] Catherine M Baker, Lauren R Milne, and Richard E Ladner. 2015. Structjumper: A tool to help blind programmers navigate and understand the structure of code. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . 3043\u20133052.", "page": 22, "bbox": []}, "seg_257": {"summary": "This bibliographic entry cites a paper titled \"An activity centered approach to nonvisual computer interaction\" published in ACM Transactions on Computer-Human Interaction (TOCHI) in 2020. The authors of this paper are Mark S Baldwin, Jennifer Mankoff, Bonnie Nardi, and Gillian Hayes.", "text": "[12] Mark S Baldwin, Jennifer Mankoff, Bonnie Nardi, and Gillian Hayes. 2020. An activity centered approach to nonvisual computer interaction. ACM Transactions on Computer-Human Interaction (TOCHI) 27, 2 (2020), 1\u201327.", "page": 22, "bbox": []}, "seg_258": {"summary": "Rate limit error after 5 retries.", "text": "[13] Saidarshan Bhagat, Padmaja Joshi, Avinash Agarwal, and Shubhanshu Gupta. 2024. Accessibility evaluation of major assistive mobile applications available for the visually impaired. arXiv preprint arXiv:2407.17496 (2024).", "page": 22, "bbox": []}, "seg_259": {"summary": "Summary: This text segment is a citation for the research paper \"Vizwiz: nearly real-time answers to visual questions\" by Jeffrey P Bigham et al., published in the 2010 Proceedings of the 23rd annual ACM symposium on User interface software and technology.", "text": "[14] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. 2010. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology . 333\u2013342.", "page": 22, "bbox": []}, "seg_260": {"summary": "Summary: In 2017, Bigham et al. published a paper in the ACM SIGACCESS conference proceedings investigating the potential of speech for device control by deaf people.  Their work, titled \"On how deaf people might use speech to control devices,\" appeared in the conference proceedings.", "text": "[15] Jeffrey P Bigham, Raja Kushalnagar, Ting-Hao Kenneth Huang, Juan Pablo Flores, and Saiph Savage. 2017. On how deaf people might use speech to control devices. In Proceedings of the 19th international ACM SIGACCESS conference on computers and accessibility . 383\u2013384.", "page": 22, "bbox": []}, "seg_261": {"summary": "Summary: Bigham, Ladner, and Borodin's 2011 paper, \"The design of human-powered access technology,\" appeared in the 13th international ACM SIGACCESS conference on Computers and accessibility proceedings.  This publication explores the design of access technology that is powered by human input.", "text": "[16] Jeffrey P Bigham, Richard E Ladner, and Yevgen Borodin. 2011. The design of human-powered access technology. In The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility . 3\u201310.", "page": 22, "bbox": []}, "seg_262": {"summary": "Summary: This citation refers to a research paper by Bigham, Lin, and Savage from 2017. The paper investigates the effects of users' lack of awareness on web accessibility for blind web users and was published in the ACM SIGACCESS conference proceedings.", "text": "[17] Jeffrey P Bigham, Irene Lin, and Saiph Savage. 2017. The Effects of\" Not Knowing What You Don\u2019t Know\" on Web Accessibility for Blind Web Users. In Proceedings of the 19th international ACM SIGACCESS conference on computers and accessibility . 101\u2013109.", "page": 22, "bbox": []}, "seg_263": {"summary": "Summary: Susanne Bodker's 1989 paper, \"A human activity approach to user interfaces,\" published in Human-Computer Interaction, explores user interface design from the perspective of human activity.  This work is a significant contribution to the field of human-computer interaction.", "text": "[18] Susanne Bodker. 1989. A human activity approach to user interfaces. Human-Computer Interaction 4, 3 (1989), 171\u2013195.", "page": 22, "bbox": []}, "seg_264": {"summary": "Summary: This text segment is a bibliographic entry for Susanne Bodker's 2021 book, \"Through the interface: A human activity approach to user interface design.\"  Published by CRC Press, the book likely explores user interface design from a human-centered perspective, emphasizing activity theory.", "text": "[19] Susanne Bodker. 2021. Through the interface: A human activity approach to user interface design . CRC Press.", "page": 22, "bbox": []}, "seg_265": {"summary": "Summary: This bibliographic entry details a paper by Borodin, Bigham, Dausch, and Ramakrishnan, published in 2010, focusing on screen-reader browsing strategies.  The paper, titled \"More than meets the eye: a survey of screen-reader browsing strategies,\" appeared in the Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A).", "text": "[20] Yevgen Borodin, Jeffrey P Bigham, Glenn Dausch, and IV Ramakrishnan. 2010. More than meets the eye: a survey of screen-reader browsing strategies. In Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A) . 1\u201310.", "page": 22, "bbox": []}, "seg_266": {"summary": "Summary: Byun and Cheverst authored a publication in 2004.  Their work focuses on the utilization of context history to enable dynamic adaptations.", "text": "[21] Hee Eon Byun and Keith Cheverst. 2004. Utilizing context history to provide dynamic adaptations.", "page": 22, "bbox": []}, "seg_267": {"summary": "Insufficient content for summary.", "text": "Applied Artificial Intelligence", "page": 22, "bbox": []}, "seg_268": {"summary": "Summary: Insufficient content for summary.", "text": "18, 6 (2004),", "page": 22, "bbox": []}, "seg_270": {"summary": "Summary: This text segment is a citation for a research paper titled \"Understanding the Career Mobility of Blind and Low Vision Software Professionals\" by Yoonha Cha et al.  The paper was published in the proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering, pages 170\u2013181.", "text": "[22] Yoonha Cha, Victoria Jackson, Isabela Figueira, Stacy Marie Branham, and Andr\u00e9 Van der Hoek. 2024. Understanding the Career Mobility of Blind and Low Vision Software Professionals. In Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering . 170\u2013181.", "page": 22, "bbox": []}, "seg_271": {"summary": "Khansa Chemnad and Achraf Othman published a paper in 2024 entitled \"Digital accessibility in the era of artificial intelligence\u2014Bibliometric analysis and systematic review.\" This paper is a bibliometric analysis and systematic review on digital accessibility in the context of artificial intelligence, and it was published in Frontiers in Artificial Intelligence.", "text": "[23] Khansa Chemnad and Achraf Othman. 2024. Digital accessibility in the era of artificial intelligence\u2014Bibliometric analysis and systematic review. Frontiers in Artificial Intelligence 7 (2024), 1349668.", "page": 22, "bbox": []}, "seg_272": {"summary": "Summary: This text segment is a citation for a research paper published in ACM Transactions on Computer-Human Interaction in 2023 by Mark Chignell and colleagues. The paper, titled \"The evolution of HCI and human factors: Integrating human and artificial intelligence,\" explores the integration of human and artificial intelligence in the context of Human-Computer Interaction and human factors.", "text": "[24] Mark Chignell, Lu Wang, Atefeh Zare, and Jamy Li. 2023. The evolution of HCI and human factors: Integrating human and artificial intelligence. ACM Transactions on Computer-Human Interaction 30, 2 (2023), 1\u201330.", "page": 22, "bbox": []}, "seg_273": {"summary": "Summary: This text segment is a bibliographic entry for a journal article.  Chris Creed and Sayan Sarcar authored an article titled \"Voice coding experiences for developers with physical impairments,\" published in the Journal of Enabling Technologies, volume 18, issue 4, pages 265-275 in 2024.", "text": "[25] Chris Creed and Sayan Sarcar. 2024. Voice coding experiences for developers with physical impairments. Journal of Enabling Technologies 18, 4 (2024), 265\u2013275.", "page": 22, "bbox": []}, "seg_274": {"summary": "Summary: This bibliographic entry refers to a 2023 paper published in the Journal of Systems and Software.  The paper, titled \"Github copilot ai pair programmer: Asset or liability?\", is authored by Arghavan Moradi Dakhel and colleagues.  It explores the role of Github Copilot as an AI pair programmer.", "text": "[26] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel C Desmarais, and Zhen Ming Jack Jiang. 2023. Github copilot ai pair programmer: Asset or liability? Journal of Systems and Software 203 (2023), 111734.", "page": 22, "bbox": []}, "seg_275": {"summary": "Md Ehtesham-Ul-Haque, Syed Mostofa Monsur, and Syed Masum Billah published a paper in 2022 titled \"Grid-coding: An accessible, efficient, and structured coding paradigm for blind and low-vision programmers.\"  This paper appeared in the Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology and spanned pages 1-21.", "text": "[27] Md Ehtesham-Ul-Haque, Syed Mostofa Monsur, and Syed Masum Billah. 2022. Grid-coding: An accessible, efficient, and structured coding paradigm for blind and low-vision programmers. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology . 1\u201321.", "page": 22, "bbox": []}, "seg_276": {"summary": "Summary: This bibliographic entry cites Yrj\u00f6 Engestr\u00f6m's 1987 publication, \"An activity-theoretical approach to developmental research.\"  The book was published by Orienta-Konsultit in Helsinki.", "text": "[28] Yrj\u00f6 Engestr\u00f6m. 1987. An activity-theoretical approach to developmental research. Helsinki: Orienta-Konsultit (1987).", "page": 22, "bbox": []}, "seg_277": {"summary": "Summary: Insufficient content for summary.", "text": "[29] Y Engestr\u00f6m. 1999. Activity theory and individual and social transformation. Perspectives on activity theory/Cambridge University Press (1999).", "page": 22, "bbox": []}, "seg_278": {"summary": "Authors Claudia Flores-Saviaga, Shangbin Feng, and Saiph Savage published a paper in 2022 titled \"Datavoidant: An ai system for addressing political data voids on social media\" in the Proceedings of the ACM on human-computer interaction.  This paper introduces \"Datavoidant,\" an AI system designed to tackle political data voids on social media platforms.", "text": "[30] Claudia Flores-Saviaga, Shangbin Feng, and Saiph Savage. 2022. Datavoidant: An ai system for addressing political data voids on social media. Proceedings of the ACM on human-computer interaction 6, CSCW2 (2022), 1\u201329.", "page": 22, "bbox": []}, "seg_279": {"summary": "Summary: This text is a bibliographic entry for a paper published in the SIGCHI conference in 2008 by Krzysztof Z Gajos, Jacob O Wobbrock, and Daniel S Weld.  The paper discusses improving interface performance for motor-impaired users through automatically generated, ability-based interfaces.", "text": "[31] Krzysztof Z Gajos, Jacob O Wobbrock, and Daniel S Weld. 2008. Improving the performance of motor-impaired users with automatically-generated, ability-based interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems . 1257\u20131266.", "page": 22, "bbox": []}, "seg_280": {"summary": "Summary: Giraud, Th\u00e9rouanne, and Steiner's 2018 article in the International Journal of Human-Computer Studies investigates web accessibility for blind users.  The study demonstrates that filtering redundant and irrelevant information on websites improves usability for this user group.", "text": "[32] St\u00e9phanie Giraud, Pierre Th\u00e9rouanne, and Dirk D Steiner. 2018. Web accessibility: Filtering redundant and irrelevant information improves website usability for blind users. International Journal of Human-Computer Studies 111 (2018), 23\u201335.", "page": 22, "bbox": []}, "seg_281": {"summary": "Summary: GitHub announced the general availability of GitHub Copilot, an AI pair programmer, in a blog post on June 21, 2022.  This announcement was made public on the GitHub blog platform.", "text": "[33] GitHub. 2022. GitHub Copilot now available for everyone . https://github.blog/2022-06-21-github-copilot-now-available/ Accessed: 2023-12-06.", "page": 22, "bbox": []}, "seg_282": {"summary": "Summary: This text segment is a citation for GitHub Copilot, an AI pair programmer tool.  The citation indicates the source is GitHub's website and the information was accessed in December 2023.", "text": "[34] GitHub. 2023. GitHub Copilot \u00b7 Your AI pair programmer. https://github.com/features/copilot Accessed: December 2023.", "page": 22, "bbox": []}, "seg_283": {"summary": "Summary: Gleason et al. published a 2018 paper on crowdsourcing the installation and maintenance of indoor localization infrastructure for blind navigation.  This research appeared in the Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies.", "text": "[35] Cole Gleason, Dragan Ahmetovic, Saiph Savage, Carlos Toxtli, Carl Posthuma, Chieko Asakawa, Kris M Kitani, and Jeffrey P Bigham. 2018. Crowdsourcing the installation and maintenance of indoor localization infrastructure to support blind navigation. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2, 1 (2018), 1\u201325.", "page": 22, "bbox": []}, "seg_284": {"summary": "Summary: Gleason et al. presented their paper \"LuzDeploy: A collective action system for installing navigation infrastructure for blind people\" at the 14th International Web for All Conference in 2017.  The publication appears in the conference proceedings, pages 1-2.", "text": "[36] Cole Gleason, Dragan Ahmetovic, Carlos Toxtli, Saiph Savage, Jeffrey P Bigham, and Chieko Asakawa. 2017. LuzDeploy: A collective action system for installing navigation infrastructure for blind people. In Proceedings of the 14th International Web for All Conference . 1\u20132.", "page": 22, "bbox": []}, "seg_285": {"summary": "Summary: This text segment is a citation for a Master's thesis by Monica Hegde from Purdue University in 2023. The thesis is titled \"User Experience Study of Screen Readers for Visually Challenged Users.\"", "text": "[37] Monica Hegde. 2023. User Experience Study of Screen Readers for Visually Challenged Users . Master\u2019s thesis. Purdue University.", "page": 22, "bbox": []}, "seg_286": {"summary": "Summary: Henry, Abou-Zahra, and Brewer's 2014 publication explores the significance of accessibility in creating a universal web.  This work was presented at the 11th Web for all Conference.", "text": "[38] Shawn Lawton Henry, Shadi Abou-Zahra, and Judy Brewer. 2014. The role of accessibility in a universal web. In Proceedings of the 11th Web for all Conference . 1\u20134.", "page": 22, "bbox": []}, "seg_287": {"summary": "Summary: A manuscript has been submitted to the ACM for publication consideration.", "text": "Manuscript submitted to ACM", "page": 22, "bbox": []}, "seg_289": {"summary": "Generative AI coding assistants are being explored for their potential to assist developers, particularly those with visual impairments.  This research focuses on understanding the specific impacts, both positive and negative, that these AI tools may have on visually impaired developers in their coding practices. The study aims to shed light on the accessibility and effectiveness of generative AI in this context.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 23, "bbox": []}, "seg_290": {"summary": "Summary: This text segment cites a 2024 paper titled \"Large language models for software engineering: A systematic literature review\" published in ACM Transactions on Software Engineering and Methodology.  The paper, authored by Xinyi Hou et al., is a comprehensive study in the field of applying large language models to software engineering tasks.", "text": "[39] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 2024. Large language models for software engineering: A systematic literature review. ACM Transactions on Software Engineering and Methodology 33, 8 (2024), 1\u201379.", "page": 23, "bbox": []}, "seg_291": {"summary": "Summary: Yosef Jabareen published an article titled \"Building a conceptual framework: philosophy, definitions, and procedure\" in the International Journal of Qualitative Methods in 2009.  This article appeared in volume 8, issue 4, and spans pages 49-62.  The publication provides a resource for understanding conceptual frameworks.", "text": "[40] Yosef Jabareen. 2009. Building a conceptual framework: philosophy, definitions, and procedure. International journal of qualitative methods 8, 4 (2009), 49\u201362.", "page": 23, "bbox": []}, "seg_292": {"summary": "Summary: Sajed Jalil authored a 2023 preprint titled \"The Transformative Influence of Large Language Models on Software Development.\"  This paper is available on arXiv as preprint arXiv:2311.16429.", "text": "[41] Sajed Jalil. 2023. The Transformative Influence of Large Language Models on Software Development. arXiv preprint arXiv:2311.16429 (2023).", "page": 23, "bbox": []}, "seg_293": {"summary": "Summary: This citation refers to Philip N Johnson-Laird's 1989 publication \"Mental models.\"  It provides bibliographic information for this work.", "text": "[42] Philip N Johnson-Laird. 1989. Mental models. (1989).", "page": 23, "bbox": []}, "seg_294": {"summary": "Summary: This text is a citation for the research paper \"Slide rule: making mobile touch screens accessible to blind people using multi-touch interaction techniques\" by Shaun K Kane, Jeffrey P Bigham, and Jacob O Wobbrock, published in the 10th international ACM SIGACCESS conference on Computers and accessibility in 2008.  The paper explores methods for making mobile touch screens accessible to blind individuals using multi-touch interaction.", "text": "[43] Shaun K Kane, Jeffrey P Bigham, and Jacob O Wobbrock. 2008. Slide rule: making mobile touch screens accessible to blind people using multi-touch interaction techniques. In Proceedings of the 10th international ACM SIGACCESS conference on Computers and accessibility . 73\u201380.", "page": 23, "bbox": []}, "seg_295": {"summary": "Summary: This text segment is a bibliographic entry for the book \"Acting with technology: Activity theory and interaction design\" authored by Victor Kaptelinin and Bonnie A Nardi in 2009 and published by MIT press.  It provides publication information for this academic resource.", "text": "[44] Victor Kaptelinin and Bonnie A Nardi. 2009. Acting with technology: Activity theory and interaction design . MIT press.", "page": 23, "bbox": []}, "seg_296": {"summary": "Summary: This text segment is a citation for the book \"Activity theory in HCI: Fundamentals and reflections\" by Victor Kaptelinin and Bonnie A Nardi, published in 2012 by Morgan & Claypool Publishers.  It provides bibliographic information for this academic work.", "text": "[45] Victor Kaptelinin and Bonnie A Nardi. 2012. Activity theory in HCI: Fundamentals and reflections . Vol. 13. Morgan & Claypool Publishers.", "page": 23, "bbox": []}, "seg_297": {"summary": "Summary: This text segment is a citation for a research paper published in the 2023 CHI Conference on Human Factors in Computing Systems.  The paper, authored by Majeed Kazemitabaar et al., is titled \"Studying the effect of AI code generators on supporting novice learners in introductory programming.\"", "text": "[46] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J Ericson, David Weintrop, and Tovi Grossman. 2023. Studying the effect of AI code generators on supporting novice learners in introductory programming. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1\u201323.", "page": 23, "bbox": []}, "seg_298": {"summary": "This bibliographic entry refers to a 2005 conference paper by Kelly, Phipps, and Howell, titled \"Implementing a holistic approach to e-learning accessibility.\"  The paper was presented at the ALT-C 2005 International Conference and published in the conference proceedings, exploring frontiers of e-learning.  It is noted as being an electronic version retrieved in November 2006.", "text": "[47] B Kelly, L Phipps, and C Howell. 2005. Implementing a holistic approach to e-learning accessibility [Electronic version]. In Exploring the frontiers of e-learning: Borders, outposts and migration (ALT-C 2005 12th International Conference Research Proceedings). Retrieved November , Vol. 27. 2006.", "page": 23, "bbox": []}, "seg_299": {"summary": "Hourieh Khalajzadeh and John Grundy published a systematic literature review in \"Information and Software Technology\" in 2024.  Their work examines the accessibility of low-code approaches.", "text": "[48] Hourieh Khalajzadeh and John Grundy. 2024. Accessibility of low-code approaches: A systematic literature review. Information and Software Technology (2024), 107570.", "page": 23, "bbox": []}, "seg_300": {"summary": "Summary: Mukta Kulkarni's 2019 article, \"Digital accessibility: Challenges and opportunities,\" published in IIMB Management Review, explores the topic of digital accessibility.", "text": "[49] Mukta Kulkarni. 2019. Digital accessibility: Challenges and opportunities. IIMB Management Review 31, 1 (2019), 91\u201398.", "page": 23, "bbox": []}, "seg_301": {"summary": "Summary: In 2006, Thomas D LaToza, Gina Venolia, and Robert DeLine published a paper titled \"Maintaining mental models: a study of developer work habits.\"  This paper appeared in the Proceedings of the 28th International Conference on Software Engineering, pages 492-501. The citation provides bibliographic information for this work.", "text": "[50] Thomas D LaToza, Gina Venolia, and Robert DeLine. 2006. Maintaining mental models: a study of developer work habits. In Proceedings of the 28th international conference on Software engineering . 492\u2013501.", "page": 23, "bbox": []}, "seg_302": {"summary": "Summary: This text segment is a citation for a study by Lazar, Dudley-Sponaugle, and Greenidge published in 2004 in Computers in Human Behavior. The study investigates web accessibility and webmaster perceptions on the topic.", "text": "[51] Jonathan Lazar, Alfreda Dudley-Sponaugle, and Kisha-Dawn Greenidge. 2004. Improving web accessibility: a study of webmaster perceptions. Computers in human behavior 20, 2 (2004), 269\u2013288.", "page": 23, "bbox": []}, "seg_303": {"summary": "Summary: This citation refers to A.N. Leontiev's book \"Problems of the development of the mind.\"  Originally published in 1931, this work is referenced in 1981. It provides bibliographic information for this publication.", "text": "[52] AN Le\u00f3ntiev. 1981. Problems of the development of the mind (Published 1931).", "page": 23, "bbox": []}, "seg_304": {"summary": "Summary: This text segment is a bibliographic entry for Aleksei Nikolaevich Leont'ev's 1978 publication, \"Activity, consciousness, and personality.\" It provides essential information for referencing this work in academic contexts.", "text": "[53] Aleksei Nikolaevich Leont\u2019ev. 1978. Activity, consciousness, and personality.", "page": 23, "bbox": []}, "seg_305": {"summary": "Summary: This text segment is a citation for a research paper. Bowen Li et al. authored a paper titled \"Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study,\" which was published in the 31st International Conference on Computational Linguistics in 2025, spanning pages 7511-7531.", "text": "[54] Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, et al. 2025. Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study. In Proceedings of the 31st International Conference on Computational Linguistics . 7511\u20137531.", "page": 23, "bbox": []}, "seg_306": {"summary": "Summary: This 2024 paper by Liang, Yang, and Myers, published in the IEEE/ACM International Conference on Software Engineering, presents a large-scale survey investigating the usability of AI programming assistants. The study explores both the successes and challenges associated with these tools.", "text": "[55] Jenny T Liang, Chenyang Yang, and Brad A Myers. 2024. A large-scale survey on the usability of ai programming assistants: Successes and challenges. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering . 1\u201313.", "page": 23, "bbox": []}, "seg_307": {"summary": "Summary: Stephanie Ludi's 2015 position paper, presented at the IEEE Blocks and Beyond Workshop, addresses the topic of making block-based programming environments accessible to blind users.  The paper is part of the proceedings from the 2015 IEEE Blocks and Beyond Workshop.", "text": "[56] Stephanie Ludi. 2015. Position paper: Towards making block-based programming accessible for blind users. In 2015 IEEE Blocks and Beyond Workshop (Blocks and Beyond) . IEEE, 67\u201369.", "page": 23, "bbox": []}, "seg_308": {"summary": "Summary: Ludi, Simpson, and Merchant's 2016 paper, presented at the ACM SIGACCESS Conference, explores the use of auditory cues to aid code comprehension and navigation for visually impaired individuals within visual programming environments.  This research focuses on making visual programming more accessible through non-visual methods.", "text": "[57] Stephanie Ludi, Jamie Simpson, and Wil Merchant. 2016. Exploration of the use of auditory cues in code comprehension and navigation for individuals with visual impairments in a visual programming environment. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility . 279\u2013280.", "page": 23, "bbox": []}, "seg_309": {"summary": "Summary: This text segment is a citation for a research paper published as an arXiv preprint in 2023. The paper, authored by Ma, Wu, and Koedinger, is titled \"Is AI the better programming partner? Human-Human pair programming vs. Human-AI pAIr programming.\" This citation provides bibliographic information for this study.", "text": "[58] Qianou Ma, Tongshuang Wu, and Kenneth Koedinger. 2023. Is AI the better programming partner? Human-Human pair programming vs. Human-AI pAIr programming. arXiv preprint arXiv:2306.05153 (2023).", "page": 23, "bbox": []}, "seg_310": {"summary": "Summary: In 2012, Mealin and Murphy-Hill presented their exploratory study on blind software developers at the IEEE Symposium on Visual Languages and Human-Centric Computing.  Their paper, published by IEEE, delves into the experiences of software developers who are blind.", "text": "[59] Sean Mealin and Emerson Murphy-Hill. 2012. An exploratory study of blind software developers. In 2012 ieee symposium on visual languages and human-centric computing (vl/hcc) . IEEE, 71\u201374.", "page": 23, "bbox": []}, "seg_311": {"summary": "Summary: This text segment is a bibliographic citation for Matthew B Miles' 1994 book, \"Qualitative data analysis: An expanded sourcebook.\" It provides standard publication information including the author, title, and year of publication.", "text": "[60] Matthew B Miles. 1994. Qualitative data analysis: An expanded sourcebook. Thousand Oaks (1994).", "page": 23, "bbox": []}, "seg_312": {"summary": "Summary: This citation refers to a paper by Morris, Johnson, Bennett, and Cutrell published in 2018.  The paper, titled \"Rich representations of visual content for screen reader users,\" appeared in the CHI conference on human factors in computing systems and spans pages 1-11.", "text": "[61] Meredith Ringel Morris, Jazette Johnson, Cynthia L Bennett, and Edward Cutrell. 2018. Rich representations of visual content for screen reader users. In Proceedings of the 2018 CHI conference on human factors in computing systems . 1\u201311.", "page": 23, "bbox": []}, "seg_313": {"summary": "Summary: Mountapmbeme, Okafor, and Ludi's 2022 paper, presented at the ACM SIGACCESS Conference, introduces \"Accessible blockly,\" a block-based programming library created for individuals with visual impairments. This research focuses on making programming more accessible for this user group.", "text": "[62] Aboubakar Mountapmbeme, Obianuju Okafor, and Stephanie Ludi. 2022. Accessible blockly: An accessible block-based programming library for people with visual impairments. In Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility . 1\u201315.", "page": 23, "bbox": []}, "seg_314": {"summary": "Summary: This citation refers to a 2022 research paper published in ACM Transactions on Accessible Computing by Aboubakar Mountapmbeme, Obianuju Okafor, and Stephanie Ludi.  The paper is a literature review focused on addressing accessibility barriers in programming for individuals with visual impairments.", "text": "[63] Aboubakar Mountapmbeme, Obianuju Okafor, and Stephanie Ludi. 2022. Addressing accessibility barriers in programming for people with visual impairments: A literature review. ACM Transactions on Accessible Computing (TACCESS) 15, 1 (2022), 1\u201326.", "page": 23, "bbox": []}, "seg_315": {"summary": "Omar Moured et al. presented their paper \"Chart4blind: An intelligent interface for chart accessibility conversion\" at the 29th International Conference on Intelligent User Interfaces in 2024. This publication outlines their research on Chart4blind, an intelligent interface that converts charts to be more accessible.", "text": "[64] Omar Moured, Morris Baumgarten-Egemole, Karin M\u00fcller, Alina Roitberg, Thorsten Schwarz, and Rainer Stiefelhagen. 2024. Chart4blind: An intelligent interface for chart accessibility conversion. In Proceedings of the 29th International Conference on Intelligent User Interfaces . 504\u2013514.", "page": 23, "bbox": []}, "seg_316": {"summary": "This citation refers to a 2024 paper titled \"Tab to Autocomplete: The Effects of AI Coding Assistants on Web Accessibility\" by Peya Mowar et al.  The paper was published in the Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility, pages 1-6.  This research likely explores the impact of AI coding assistants on web accessibility based on the title.", "text": "[65] Peya Mowar, Yi-Hao Peng, Aaron Steinfeld, and Jeffrey P Bigham. 2024. Tab to Autocomplete: The Effects of AI Coding Assistants on Web Accessibility. In Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility . 1\u20136.", "page": 23, "bbox": []}, "seg_317": {"summary": "Summary: Mozannar et al. published a paper in the 2024 CHI Conference on Human Factors in Computing Systems.  Their work, titled \"Reading between the lines: Modeling user behavior and costs in AI-assisted programming,\" spans pages 1-16 of the proceedings. This paper explores user behavior and cost considerations within the context of AI-assisted programming.", "text": "[66] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2024. Reading between the lines: Modeling user behavior and costs in AI-assisted programming. In Proceedings of the CHI Conference on Human Factors in Computing Systems . 1\u201316.", "page": 23, "bbox": []}, "seg_318": {"summary": "Summary: Nguyen et al. published a paper titled \"How Beginning Programmers and Code LLMs (Mis) read Each Other\" in the 2024 CHI Conference on Human Factors in Computing Systems.  This paper appears on pages 1-26 of the conference proceedings.", "text": "[67] Sydney Nguyen, Hannah McLean Babe, Yangtian Zi, Arjun Guha, Carolyn Jane Anderson, and Molly Q Feldman. 2024. How Beginning Programmers and Code LLMs (Mis) read Each Other. In Proceedings of the CHI Conference on Human Factors in Computing Systems . 1\u201326.", "page": 23, "bbox": []}, "seg_319": {"summary": "Summary: Jakob Nielsen's 2023 publication, \"AI: First New UI Paradigm in 60 Years,\" argues that artificial intelligence represents a fundamental shift in user interface design.  Published by the Nielsen Norman Group on June 18, 2023, this work suggests AI is the most significant UI paradigm change in six decades.", "text": "[68] Jakob Nielsen. 2023. AI: First New UI Paradigm in 60 Years. Nielsen Norman Group 18, 06 (2023), 2023.", "page": 23, "bbox": []}, "seg_320": {"summary": "Sadia Nowrin, Patricia Ord\u00f3\u00f1ez, and Keith Vertanen published a paper in 2022 exploring how programmers with motor impairments utilize speech recognition.  This research was presented at the 24th International ACM SIGACCESS Conference on Computers and Accessibility.", "text": "[69] Sadia Nowrin, Patricia Ord\u00f3\u00f1ez, and Keith Vertanen. 2022. Exploring Motor-impaired Programmers\u2019 Use of Speech Recognition. In Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility .", "page": 23, "bbox": []}, "seg_321": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 23, "bbox": []}, "seg_323": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 24, "bbox": []}, "seg_324": {"summary": "Summary: In a 2023 Science publication, Shakked Noy and Whitney Zhang presented experimental evidence regarding the impact of generative artificial intelligence on productivity. Their paper, titled \"Experimental evidence on the productivity effects of generative artificial intelligence,\" appeared in volume 381, issue 6654, pages 187-192.", "text": "[70] Shakked Noy and Whitney Zhang. 2023. Experimental evidence on the productivity effects of generative artificial intelligence. Science 381, 6654 (2023), 187\u2013192.", "page": 24, "bbox": []}, "seg_325": {"summary": "Summary: Sushil K Oswal and Hitender K Oswal's 2024 IEEE ProComm paper investigates the accessibility of generative AI website builder tools for users with blindness and low vision. Their research proposes 21 best practices for designers and developers to enhance the accessibility of these tools.", "text": "[71] Sushil K Oswal and Hitender K Oswal. 2024. Examining the accessibility of generative AI website builder tools for blind and low vision users: 21 best practices for designers and developers. In 2024 IEEE International Professional Communication Conference (ProComm) . IEEE, 121\u2013128.", "page": 24, "bbox": []}, "seg_326": {"summary": "Summary: This text segment is a bibliographic entry for a publication by Linus P\u00e5hlstorp and Lukas Gwardak in 2007.  Their work, titled \"Exploring usability guidelines for rich internet applications,\" likely investigates usability principles for rich internet applications.", "text": "[72] Linus P\u00e5hlstorp and Lukas Gwardak. 2007. Exploring usability guidelines for rich internet applications. (2007).", "page": 24, "bbox": []}, "seg_327": {"summary": "Peng et al. published a 2023 preprint investigating the impact of AI on developer productivity. Their study uses GitHub Copilot as a case study to provide evidence for their findings.", "text": "[73] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. 2023. The impact of ai on developer productivity: Evidence from github copilot. arXiv preprint arXiv:2302.06590 (2023).", "page": 24, "bbox": []}, "seg_328": {"summary": "Summary: Petrie and Kheir's 2007 paper investigates the relationship between website accessibility and usability.  The study was presented at the SIGCHI conference on Human factors in computing systems.", "text": "[74] Helen Petrie and Omar Kheir. 2007. The relationship between accessibility and usability of websites. In Proceedings of the SIGCHI conference on Human factors in computing systems . 397\u2013406.", "page": 24, "bbox": []}, "seg_329": {"summary": "Summary: This text segment is a citation for a research paper titled \"Speaking with My Screen Reader: Using Audio Fictions to Explore Conversational Access to Interfaces\" by Mahika Phutane et al., published in the 25th International ACM SIGACCESS Conference on Computers and Accessibility in 2023.  The citation provides bibliographic information for this academic work.", "text": "[75] Mahika Phutane, Crescentia Jung, Niu Chen, and Shiri Azenkot. 2023. Speaking with My Screen Reader: Using Audio Fictions to Explore Conversational Access to Interfaces. In Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility . 1\u201318.", "page": 24, "bbox": []}, "seg_330": {"summary": "Summary: This blog post, written by Augustin Popa in 2023 and published on the Microsoft Visual Studio Blog, discusses how to use Copilot Chat with C++ in Visual Studio.  The article covers functionalities such as generating code and fixing functions.", "text": "[76] Augustin Popa. 2023. Using Copilot Chat with C++ in Visual Studio: Generate Code, Fix Functions, and More . https://devblogs.microsoft.com/ visualstudio/using-copilot-chat-with-c-in-visual-studio-generate-code-fix-functions-and-more/ Microsoft Visual Studio Blog.", "page": 24, "bbox": []}, "seg_331": {"summary": "Summary: Venkatesh Potluri et al. published a paper titled \"Codewalk: Facilitating shared awareness in mixed-ability collaborative software development\" in the 2022 Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility.  The paper explores methods for improving shared understanding in collaborative software development teams with varying skill levels.", "text": "[77] Venkatesh Potluri, Maulishree Pandey, Andrew Begel, Michael Barnett, and Scott Reitherman. 2022. Codewalk: Facilitating shared awareness in mixed-ability collaborative software development. In Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility . 1\u201316.", "page": 24, "bbox": []}, "seg_332": {"summary": "Summary: This text segment cites the research paper \"Codetalk: Improving programming environment accessibility for visually impaired developers\" by Venkatesh Potluri and colleagues.  Published in the 2018 CHI conference proceedings, the paper explores methods for making programming environments more accessible to visually impaired developers.", "text": "[78] Venkatesh Potluri, Priyan Vaithilingam, Suresh Iyengar, Y Vidya, Manohar Swaminathan, and Gopal Srinivasa. 2018. Codetalk: Improving programming environment accessibility for visually impaired developers. In Proceedings of the 2018 chi conference on human factors in computing systems . 1\u201311.", "page": 24, "bbox": []}, "seg_333": {"summary": "Summary: Rajaselvi et al. published a survey in 2021 focusing on programming editors designed for visually impaired individuals.  This survey explores the landscape of tools available to visually impaired programmers.", "text": "[79] Mercy Rajaselvi, F Jane Gloria, V Mohitha, and Guhan Selvarajan. 2021. A survey of programming editors for the visually impaired. Accessed: Aug 12 (2021).", "page": 24, "bbox": []}, "seg_334": {"summary": "Summary: This text segment is a bibliographic entry for the book \"Reason & rigor: How conceptual frameworks guide research\" by Sharon M Ravitch and Matthew Riggan, published by Sage Publications in 2016.  The book likely discusses the use and importance of conceptual frameworks within research methodologies.", "text": "[80] Sharon M Ravitch and Matthew Riggan. 2016. Reason & rigor: How conceptual frameworks guide research . Sage Publications.", "page": 24, "bbox": []}, "seg_335": {"summary": "Summary: Joshua Robins authored a Ph.D. dissertation in 2019 at the University of Huddersfield.  His dissertation focused on developing a \"Barrier Determination Framework\" for analyzing video game accessibility for users with visual impairments. This research contributes to understanding and improving video game accessibility for visually impaired players.", "text": "[81] Joshua Robins. 2019. Barrier Determination Framework for Video Game Analysis Regarding Users with Visual Impairments . Ph. D. Dissertation. University of Huddersfield.", "page": 24, "bbox": []}, "seg_336": {"summary": "Summary: This bibliographic entry details Hyman Rodman's 1980 publication in the Sociological Quarterly.  The article, titled \"Are conceptual frameworks necessary for theory building? The case of family sociology,\" is found in volume 21, issue 3, pages 429-441.", "text": "[82] Hyman Rodman. 1980. Are conceptual frameworks necessary for theory building? The case of family sociology. Sociological Quarterly 21, 3 (1980), 429\u2013441.", "page": 24, "bbox": []}, "seg_337": {"summary": "Summary: This citation refers to a research paper titled \"Vocal Programming for People with Upper-Body Motor Impairments\" by Lucas Rosenblatt et al., presented at the 15th International Web for All Conference (W4A) in 2018.  The paper is published by the Association for Computing Machinery and explores vocal programming as an assistive technology.", "text": "[83] Lucas Rosenblatt, Patrick Carrington, Kotaro Hara, and Jeffrey P. Bigham. 2018. Vocal Programming for People with Upper-Body Motor Impairments. In Proceedings of the 15th International Web for All Conference (Lyon, France) (W4A \u201918) . Association for Computing Machinery, New York, NY, USA, Article 30, 10 pages. https://doi.org/10.1145/3192714.3192821", "page": 24, "bbox": []}, "seg_338": {"summary": "Summary: Daniel Russo's 2024 paper, published in ACM Transactions on Software Engineering and Methodology, investigates the complexities surrounding the adoption of generative AI within the field of software engineering.  This work highlights the challenges and considerations involved in integrating this technology.", "text": "[84] Daniel Russo. 2024. Navigating the complexity of generative ai adoption in software engineering. ACM Transactions on Software Engineering and Methodology (2024).", "page": 24, "bbox": []}, "seg_339": {"summary": "Summary: Savage et al.'s 2022 publication, \"The global care ecosystems of 3D printed assistive devices,\" appeared in ACM Transactions on Accessible Computing, volume 15, issue 4, pages 1-29. This entry provides bibliographic information for a study in the field of accessible computing.", "text": "[85] Saiph Savage, Claudia Flores-Saviaga, Rachel Rodney, Liliana Savage, Jon Schull, and Jennifer Mankoff. 2022. The global care ecosystems of 3D printed assistive devices. ACM Transactions on Accessible Computing 15, 4 (2022), 1\u201329.", "page": 24, "bbox": []}, "seg_340": {"summary": "Authors Sharif, Chintalapati, Wobbrock, and Reinecke published a study in 2021 focusing on screen-reader users' experiences with online data visualizations. Their research appeared in the Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility.  This paper explores the challenges and perspectives of visually impaired users interacting with data visualizations on the web.", "text": "[86] Ather Sharif, Sanjana Shivani Chintalapati, Jacob O Wobbrock, and Katharina Reinecke. 2021. Understanding screen-reader users\u2019 experiences with online data visualizations. In Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility . 1\u201316.", "page": 24, "bbox": []}, "seg_341": {"summary": "Summary: This text segment is a bibliographic entry for the book \"Telerobotics, automation, and human supervisory control\" authored by Thomas B Sheridan and published by MIT Press in 1992.  It provides publication details for this specific academic work.", "text": "[87] Thomas B Sheridan. 1992. Telerobotics, automation, and human supervisory control . MIT press.", "page": 24, "bbox": []}, "seg_342": {"summary": "Patricia M. Shields and Nandhini Rangarajan authored the book \"A playbook for research methods: Integrating conceptual frameworks and project management\" in 2013.  This book was published by New Forums Press.", "text": "[88] Patricia M Shields and Nandhini Rangarajan. 2013. A playbook for research methods: Integrating conceptual frameworks and project management New Forums Press.", "page": 24, "bbox": []}, "seg_344": {"summary": "Summary: This 2022 paper by Shinohara, Tamjeed, McQuaid, and Barkins, published in *Proc. ACM Hum.-Comput. Interact.*, investigates the usability, accessibility, and social aspects of advanced tool use by vision-impaired graduate students.  The research is available via DOI: https://doi.org/10.1145/3555609.", "text": "[89] Kristen Shinohara, Murtaza Tamjeed, Michael McQuaid, and Dymen A. Barkins. 2022. Usability, Accessibility and Social Entanglements in Advanced Tool Use by Vision Impaired Graduate Students. Proc. ACM Hum.-Comput. Interact. 6, CSCW2, Article 551 (nov 2022), 21 pages. https://doi.org/10.1145/3555609", "page": 24, "bbox": []}, "seg_345": {"summary": "Summary: This text segment is a citation for a publication by Ann C Smith et al. in 2003.  The paper, titled \"Nonvisual tool for navigating hierarchical structures,\" was published in ACM SIGACCESS Accessibility and Computing, pages 133-139.", "text": "[90] Ann C Smith, Justin S Cook, Joan M Francioni, Asif Hossain, Mohd Anwar, and M Fayezur Rahman. 2003. Nonvisual tool for navigating hierarchical structures. ACM SIGACCESS Accessibility and Computing 77-78 (2003), 133\u2013139.", "page": 24, "bbox": []}, "seg_346": {"summary": "Summary: Mads Soegaard and Rikke Friis Dam authored \"The Encyclopedia of Human-Computer Interaction\" in 2012. This encyclopedia is a resource for the field of human-computer interaction.", "text": "[91] Mads Soegaard and Rikke Friis Dam. 2012. The encyclopedia of human-computer interaction. The encyclopedia of human-computer interaction (2012).", "page": 24, "bbox": []}, "seg_347": {"summary": "Summary: This citation refers to the Stack Overflow Developer Survey 2022.  The survey is accessible online and published by Stack Overflow.", "text": "[92] Stack Overflow. 2022. Stack Overflow Developer Survey 2022 . https://survey.stackoverflow.co/2022/ Accessed: 2023-12-06.", "page": 24, "bbox": []}, "seg_348": {"summary": "Summary: This citation refers to the Stack Overflow Developer Survey 2023, accessed online on December 6, 2023.  The survey, conducted by Stack Overflow, is a resource for information related to software developers.", "text": "[93] Stack Overflow. 2023. Stack Overflow Developer Survey 2023 . https://visualstudiomagazine.com/articles/2023/06/28/so-2023.aspx Accessed: 2023-12-06.", "page": 24, "bbox": []}, "seg_349": {"summary": "Summary: Stefik et al. published a feasibility study in 2007 on the \"WAD: wicked audio debugger.\"  This study was presented at the 15th IEEE International Conference on Program Comprehension (ICPC\u201907).", "text": "[94] Andreas Stefik, Roger Alexander, Robert Patterson, and Jonathan Brown. 2007. WAD: A feasibility study using the wicked audio debugger. In 15th IEEE International Conference on Program Comprehension (ICPC\u201907) . 69\u201380.", "page": 24, "bbox": []}, "seg_350": {"summary": "Summary: Stefik, Hundhausen, and Smith published research in 2011 concerning the design of educational infrastructure for individuals who are blind or visually impaired.", "text": "[95] Andreas M Stefik, Christopher Hundhausen, and Derrick Smith. 2011. On the design of an educational infrastructure for the blind and visually", "page": 24, "bbox": []}, "seg_351": {"summary": "Insufficient content for summary.", "text": "impaired in computer science. In", "page": 24, "bbox": []}, "seg_352": {"summary": "Summary: This text segment refers to the proceedings from the 42nd ACM technical symposium on Computer Science Education. It represents the published record of this conference focused on computer science education.", "text": "Proceedings of the 42nd ACM technical symposium on Computer science education", "page": 24, "bbox": []}, "seg_354": {"summary": "Summary: Szpiro et al. published a paper in 2016 investigating how individuals with low vision utilize computing devices.  This research, presented at the ACM SIGACCESS Conference, delves into the challenges and opportunities associated with computer accessibility for this population.", "text": "[96] Sarit Felicia Anais Szpiro, Shafeka Hashash, Yuhang Zhao, and Shiri Azenkot. 2016. How people with low vision access computing devices: Understanding challenges and opportunities. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility . 171\u2013180.", "page": 24, "bbox": []}, "seg_355": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 24, "bbox": []}, "seg_357": {"summary": "Summary: This text will likely explore the effects of generative AI coding assistants on developers with visual impairments.  It aims to analyze how these tools impact their coding experience, potentially addressing both benefits and challenges related to accessibility and productivity.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 25, "bbox": []}, "seg_358": {"summary": "Summary: This text segment is a bibliographic entry for a research article by Delphine Szymczak published in 2023.  The article focuses on audio-haptic activities involving individuals with visual impairments and technology, using Cultural-Historical Activity Theory as a framework for analysis.", "text": "[97] Delphine Szymczak. 2023. Tools in and out of sight: an analysis informed by Cultural-Historical Activity Theory of audio-haptic activities involving people with visual impairments supported by technology. (2023).", "page": 25, "bbox": []}, "seg_359": {"summary": "Summary: Tlili et al. published a systematic literature review in Frontiers in Psychology in 2021 (published in 2022).  The review explores game-based learning for learners with disabilities from the perspective of activity theory.", "text": "[98] A Tlili et al. 2021. Game-based learning for learners with disabilities\u2013what is next? A systematic literature review from the activity theory perspective. Front. Psychol. 12, 814691 (2022).", "page": 25, "bbox": []}, "seg_360": {"summary": "Emmanuel Utreras and Enrico Pontelli authored a paper in 2020 focusing on the accessibility of block-based introductory programming languages and tangible programming tools.  This work was presented at the International Conference on Computers Helping People with Special Needs and published by Springer.", "text": "[99] Emmanuel Utreras and Enrico Pontelli. 2020. Accessibility of block-based introductory programming languages and a tangible programming tool prototype. In International Conference on Computers Helping People with Special Needs . Springer, 27\u201334.", "page": 25, "bbox": []}, "seg_361": {"summary": "Summary: In their 2022 CHI conference paper, \"Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models,\" Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman investigated the usability of code generation tools powered by large language models. This publication appeared in the extended abstracts of the CHI conference on human factors in computing systems.", "text": "[100] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Chi conference on human factors in computing systems extended abstracts . 1\u20137.", "page": 25, "bbox": []}, "seg_362": {"summary": "Summary: This document is part of the Visual Studio Code documentation and provides an overview of GitHub Copilot.  It highlights the seamless integration of GitHub Copilot with the Visual Studio Code environment.  The documentation was accessed in December 2023.", "text": "[101] Visual Studio Code Documentation. 2023. GitHub Copilot Overview: Seamless Integration with Visual Studio Code . https://code.visualstudio.com/ docs/copilot/overview Accessed: 2023-12-06.", "page": 25, "bbox": []}, "seg_363": {"summary": "Summary: Insufficient content for summary.", "text": "[102] Lev S Vygotsky. 2012. Thought and language . MIT press.", "page": 25, "bbox": []}, "seg_364": {"summary": "Ruotong Wang et al. published a paper in the 2024 ACM Conference on Fairness, Accountability, and Transparency.  Their paper, titled \"Investigating and designing for trust in ai-powered code generation tools,\" explores trust in AI code generation. This publication contributes to the field of AI ethics and software engineering.", "text": "[103] Ruotong Wang, Ruijia Cheng, Denae Ford, and Thomas Zimmermann. 2024. Investigating and designing for trust in ai-powered code generation tools. In The 2024 ACM Conference on Fairness, Accountability, and Transparency .", "page": 25, "bbox": []}, "seg_365": {"summary": "Summary: In a 2024 CHI Conference publication, Weisz et al. presented their research on design principles for generative AI applications. Their paper explores key considerations for human-centered design in this domain.", "text": "[104] Justin D Weisz, Jessica He, Michael Muller, Gabriela Hoefer, Rachel Miles, and Werner Geyer. 2024. Design Principles for Generative AI Applications. In Proceedings of the CHI Conference on Human Factors in Computing Systems .", "page": 25, "bbox": []}, "seg_366": {"summary": "Summary: This text segment cites a 2023 research paper titled \"Natural language generation and understanding of big code for AI-assisted programming: A review\" by Wong et al.  The paper was published in Entropy, volume 25, issue 6, and is identified as article number 888.", "text": "[105] Man-Fai Wong, Shangxin Guo, Ching-Nam Hang, Siu-Wai Ho, and Chee-Wei Tan. 2023. Natural language generation and understanding of big code for AI-assisted programming: A review. Entropy 25, 6 (2023), 888.", "page": 25, "bbox": []}, "seg_367": {"summary": "Jason Yu and Cheryl Qi published an article in the Review of Business in 2024 examining the impact of generative AI on employment and labor productivity.  Their research, appearing in volume 44, issue 1, explores this timely topic within the context of business.", "text": "[106] Jason Yu and Cheryl Qi. 2024. The Impact of Generative AI on Employment and Labor Productivity. Review of Business 44, 1 (2024).", "page": 25, "bbox": []}, "seg_368": {"summary": "Summary: Zhang, Sun, and Findlater's 2023 paper, published in the ACM SIGACCESS Conference, explores the digital content creation needs of blind and low vision individuals. The research delves into understanding the specific challenges and requirements of this population in the context of creating digital content.", "text": "[107] Lotus Zhang, Simon Sun, and Leah Findlater. 2023. Understanding Digital Content Creation Needs of Blind and Low Vision People. In Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility .", "page": 25, "bbox": []}, "seg_369": {"summary": "Albert Ziegler and colleagues published a paper titled \"Productivity assessment of neural code completion\" in the 6th ACM SIGPLAN International Symposium on Machine Programming in 2022. This paper was presented at the 6th ACM SIGPLAN International Symposium on Machine Programming and is available on pages 21-29 of the proceedings.", "text": "[108] Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Productivity assessment of neural code completion. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming . 21\u201329.", "page": 25, "bbox": []}, "seg_370": {"summary": "Summary: Zong et al. published a paper in Computer Graphics Forum in 2022 focusing on \"rich screen reader experiences for accessible data visualization.\"  This work appears in Volume 41 of the journal, published by Wiley Online Library.", "text": "[109] Jonathan Zong, Crystal Lee, Alan Lundgard, JiWoong Jang, Daniel Hajas, and Arvind Satyanarayan. 2022. Rich screen reader experiences for accessible data visualization. In Computer Graphics Forum , Vol. 41. Wiley Online Library.", "page": 25, "bbox": []}, "seg_371": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 25, "bbox": []}}