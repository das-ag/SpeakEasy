{"seg_0": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 1, "bbox": []}, "seg_1": {"summary": "Summary: This text segment lists the authors and their affiliations. Claudia Flores-Saviaga, Kashif Imteyaz, and Saiph Savage are affiliated with Northeastern University in the USA, while Benjamin V. Hanrahan and Steven Clarke are affiliated with Microsoft in the USA and United Kingdom, respectively.", "text": "CLAUDIA FLORES-SAVIAGA, Northeastern University, USA BENJAMIN V. HANRAHAN, Microsoft, USA KASHIF IMTEYAZ, Northeastern University, USA STEVEN CLARKE, Microsoft, United Kingdom SAIPH SAVAGE, Northeastern University, USA", "page": 1, "bbox": []}, "seg_3": {"summary": "Summary: Figure 1 provides an overview of research focused on understanding how developers with visual impairments interact with AI coding assistants.  This research aims to analyze the specific ways visually impaired developers use and engage with AI coding assistants.", "text": "Fig. 1. Overview of our research to analyze how developers who are visually impaired interact with AI coding assistants.", "page": 1, "bbox": []}, "seg_4": {"summary": "A study exploring the use of AI coding assistants by developers with visual impairments found that while beneficial, these tools also presented accessibility challenges, such as overwhelming suggestions and difficulty switching between AI-generated and user code.  Despite these issues, participants were optimistic about the potential of AI assistants, highlighting the need for activity-centered design to improve inclusivity and effectiveness for developers with visual impairments.  This approach can lead to more intuitive and accessible AI tools for software development.", "text": "The rapid adoption of generative AI in software development has impacted the industry, yet its effects on developers with visual impairments remain largely unexplored. To address this gap, we used an Activity Theory framework to examine how developers with visual impairments interact with AI coding assistants. For this purpose, we conducted a study where developers who are visually impaired completed a series of programming tasks using a generative AI coding assistant. We uncovered that, while participants found the AI assistant beneficial and reported significant advantages, they also highlighted accessibility challenges. Specifically, the AI coding assistant often exacerbated existing accessibility barriers and introduced new challenges. For example, it overwhelmed users with an excessive number of suggestions, leading developers who are visually impaired to express a desire for \u201cAI timeouts.\u201d Additionally, the generative AI coding assistant made it more difficult for developers to switch contexts between the AI-generated content and their own code. Despite these challenges, participants were optimistic about the potential of AI coding assistants to transform the coding experience for developers with visual impairments. Our findings emphasize the need to apply activity-centered design principles to generative AI assistants, ensuring they better align with user behaviors and address specific accessibility needs. This approach can enable the assistants to provide more intuitive, inclusive, and effective experiences, while also contributing to the broader goal of enhancing accessibility in software development.", "page": 1, "bbox": []}, "seg_5": {"summary": "Insufficient content for summary.", "text": "ACM Reference Format:", "page": 1, "bbox": []}, "seg_6": {"summary": "Summary: This research paper, authored by Flores-Saviaga, Hanrahan, Imteyaz, Clarke, and Savage, investigates the impact of generative AI coding assistants on developers with visual impairments.  Published in March 2025, the 25-page study is accessible via the provided DOI link.", "text": "Claudia Flores-Saviaga, Benjamin V. Hanrahan, Kashif Imteyaz, Steven Clarke, and Saiph Savage. 2025. The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired. 1, 1 (March 2025), 25 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn", "page": 1, "bbox": []}, "seg_7": {"summary": "Summary: This text segment lists the authors of a publication, along with their affiliations at Northeastern University and Microsoft, and their respective email addresses.  It provides contact information for each author.", "text": "Authors\u2019 addresses: Claudia Flores-Saviaga, floressaviaga.c@northeastern.edu, Northeastern University, USA; Benjamin V. Hanrahan, benhanrahan@ microsoft.com, Microsoft, USA; Kashif Imteyaz, imteyaz.k@northeastern.edu, Northeastern University, USA; Steven Clarke, stevencl@microsoft.com, Microsoft, United Kingdom; Saiph Savage, s.savage@northeastern.edu, Northeastern University, USA.", "page": 1, "bbox": []}, "seg_8": {"summary": "Summary: This text segment is a copyright notice from the Association for Computing Machinery (ACM) regarding permitted uses of their work.  It grants free permission for personal and classroom use, provided it is not for profit and includes proper citation.  Any other use, such as commercial redistribution or republication, requires explicit permission and potentially a fee, obtainable from ACM.", "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2025 Association for Computing Machinery.", "page": 1, "bbox": []}, "seg_9": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 1, "bbox": []}, "seg_10": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 1, "bbox": []}, "seg_13": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 2, "bbox": []}, "seg_14": {"summary": "Insufficient content for summary.", "text": "1 INTRODUCTION", "page": 2, "bbox": []}, "seg_15": {"summary": "Generative AI is transforming software development through coding assistants like GitHub Copilot, which offer functionalities such as auto-completion and code generation.  This technological shift raises an important question regarding the impact of these AI tools on developers with visual impairments. The text segment highlights the need to understand how this emerging technology affects accessibility and inclusivity in coding.", "text": "The integration of generative artificial intelligence (AI) into software development is fundamentally transforming coding practices [ 55 ]. AI coding assistants, such as GitHub Copilot [ 34 ], provide functionalities like auto-completion, code generation, and test generation [ 76 ], which have the potential to revolutionize how developers work [ 73 ]. Amidst this surge of new capabilities driven by AI, an important question emerges: How do these generative AI coding assistants impact developers with visual impairments?", "page": 2, "bbox": []}, "seg_16": {"summary": "AI's integration into coding offers opportunities to improve accessibility for developers with visual impairments by automating tasks and providing intelligent support.  However, if AI coding assistants are not designed with accessibility in mind, they risk creating new obstacles, especially for screen reader users.  Therefore, careful consideration of accessibility is crucial in the development of AI-powered coding tools.", "text": "The convergence of AI and accessibility in coding presents both tremendous opportunities and intricate challenges [ 55 , 67 ]. On one hand, AI has the potential to make coding more accessible for developers with visual impairments by reducing manual coding tasks and providing intelligent assistance [ 89 ]. On the other hand, if these AI coding assistants are not designed with accessibility in mind, they may inadvertently create new barriers or worsen existing ones [ 67 , 103 ]. This risk is especially high if the AI coding assistants depend heavily on visual cues or employ interaction methods incompatible with screen readers [ 61 , 86 , 109 ].", "page": 2, "bbox": []}, "seg_17": {"summary": "The accessibility challenges in AI assistants mirror those in web accessibility, particularly for visually impaired users, due to dynamic content and complex interactions.  Traditional accessibility guidelines may not fully address these issues in AI coding assistants, where AI-generated suggestions and interfaces can create new hurdles for visually impaired developers.  Further research is needed to understand and address these difficulties to make AI coding assistants truly accessible and user-friendly for all developers.", "text": "In this context, it is important to understand that the relationship between accessibility and usability in AI assistants mirrors longstanding challenges in web accessibility [ 38 , 51 ], particularly for visually impaired users [ 14 , 43 ]. Similar to rich internet applications [ 9 , 72 ], AI assistants often introduce dynamic content and complex interactions that traditional accessibility approaches may not fully address [ 39 , 47 ]. For instance, Petrie and Kheir [ 74 ] found that existing accessibility guidelines fail to capture many of the usability problems encountered by visually impaired users when interacting with dynamic content. This gap could be especially exacerbated in AI coding assistants [ 2 , 65 , 71 ], where AI-generated suggestions, real-time code generation, and sophisticated user interfaces can present new hurdles [ 39 , 41 ]. Just as screen readers struggle with dynamic web content, AJAX updates, and automatic refreshes [ 20 ], developers who are visually impaired may experience similar difficulties when interacting with rapidly changing AI-generated code suggestions and interface elements [ 54 ]. Consequently, simply following basic accessibility rules might not be enough to make AI coding assistants truly easy to use. We might need to rethink how to make these assistants more accessible for everyone. The challenge is that we do not fully understand the difficulties and benefits that visually impaired developers experience when using AI coding assistants. Closing this knowledge gap is important\u2014not just for accessibility, but also for making AI-assisted coding tools more user-friendly and effective for all developers, including those with different levels of vision.", "page": 2, "bbox": []}, "seg_18": {"summary": "Summary: This study investigates the unique challenges and opportunities that AI-assisted coding tools present for developers with visual impairments.  The research is driven by a question focused on understanding these challenges and opportunities.", "text": "Our study addresses this research gap by investigating what unique challenges and opportunities AI-assisted coding tools present for developers with visual impairments. Our research is driven by the following research question:", "page": 2, "bbox": []}, "seg_19": {"summary": "Summary: This research question explores the challenges that AI coding tools present to visually impaired developers.  It also investigates the potential of these tools to create new opportunities and improve the work of developers with visual impairments.", "text": "\u2022 RQ1: What challenges do AI-assisted coding tools pose for developers who are visually impaired, and what opportunities do they offer to empower and enhance their work?", "page": 2, "bbox": []}, "seg_20": {"summary": "This study uses the Activity Theory framework to investigate how visually impaired developers interact with the AI coding assistant GitHub Copilot. Researchers observed ten developers with varying experience levels as they completed a coding task using Copilot and conducted interviews afterwards. The study aimed to identify both the challenges and opportunities these developers encountered when using the AI assistant.", "text": "To study this question, we use the Activity Theory framework [ 44 ], which helps analyze how tools influence human activity and identifies contradictions that occur when a tool\u2019s design does not fit well with users\u2019 workflows and needs [ 12 ]. We applied this framework to examine a qualitative study we conducted with 10 visually impaired developers of varying experience levels. These developers used an AI coding assistant, GitHub Copilot, to complete a coding task. During the study, we observed their real-time interactions with the AI assistant, noting both challenges and opportunities. After the task, we conducted interviews to gain deeper insights into their experiences and the difficulties they faced while using the AI coding assistant.", "page": 2, "bbox": []}, "seg_21": {"summary": "Summary: This study reveals that AI coding assistants have a dual impact, enhancing coding efficiency while simultaneously posing new accessibility challenges.  Based on these findings, the authors propose a roadmap for developing the next generation of accessible AI tools. This research contributes to the understanding of the complex relationship between AI in coding and accessibility.", "text": "Our findings reveal a complex landscape where AI coding assistants can both improve coding efficiency and introduce new accessibility challenges. Based on our findings, we outline a roadmap for the next generation of accessible AI Manuscript submitted to ACM", "page": 2, "bbox": []}, "seg_23": {"summary": "Summary: This text segment will explore how generative AI coding assistants affect developers who are visually impaired. It will likely examine the specific ways these tools impact their coding experience.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 3, "bbox": []}, "seg_24": {"summary": "Summary: This text segment discusses research focused on the experiences of visually impaired developers using AI coding assistants. The goal of this research is to derive key design insights that can improve accessibility in AI-driven software development. Ultimately, these insights are intended to reshape how accessibility is approached in the development of AI-driven software.", "text": "coding assistants. By analyzing the experiences of visually impaired developers using AI-assisted coding tools, we derive key design insights that can reshape how accessibility is approached in AI-driven software development.", "page": 3, "bbox": []}, "seg_25": {"summary": "Insufficient content for summary.", "text": "The contributions of this paper are twofold:", "page": 3, "bbox": []}, "seg_26": {"summary": "Summary: This research offers a thorough examination of the accessibility of AI coding assistants, specifically looking at the challenges and advantages for developers who are visually impaired. The study is based on practical experiences and perspectives from visually impaired developers themselves.", "text": "\u2022 We conduct a comprehensive analysis of the accessibility challenges and benefits of AI coding assistants, using real-world insights from developers who are visually impaired.", "page": 3, "bbox": []}, "seg_27": {"summary": "Summary: This segment introduces design recommendations aimed at improving the accessibility and inclusivity of AI coding assistants.  The recommendations are specifically targeted towards developers with visual impairments.", "text": "\u2022 We propose a set of design recommendations to make AI coding assistants more accessible and inclusive for developers with visual impairments.", "page": 3, "bbox": []}, "seg_28": {"summary": "Insufficient content for summary.", "text": "2 RELATED WORK", "page": 3, "bbox": []}, "seg_29": {"summary": "Insufficient content for summary.", "text": "2.1 Accessibility and Tools for Developers who are Visually Impaired", "page": 3, "bbox": []}, "seg_30": {"summary": "Studies have consistently shown that visually impaired developers face significant challenges in coding. These challenges span multiple areas, including code navigation, comprehension, editing, debugging, and skimming, as identified by Mountapmbeme et al. Further research by Albusays and Ludi highlights ongoing difficulties in code navigation, diagrams, debugging, and UI layouts, leading to a preference for text editors over IDEs due to accessibility concerns.", "text": "Accessibility in general has been a subject of ongoing research [ 17 , 35 , 36 , 85 ]. Several studies have shed light on the significant challenges developers who are visually impaired face with coding and the techniques these programmers employ to overcome these challenges. Mountapmbeme et al. [ 63 ] categorized these challenges into five main areas: code navigation, code comprehension, code editing, code debugging, and code skimming. Albusays and Ludi [ 5 ] found persistent challenges in code navigation, accessing diagrams, debugging, and UI layout. Their study highlighted a strong preference for text editors over IDEs due to accessibility issues, reduced complexity, and better compatibility with assistive technologies.", "page": 3, "bbox": []}, "seg_31": {"summary": "Visually impaired developers often favor text editors over IDEs due to accessibility limitations, leading to unique coding practices.  Challenges include navigating code structure and debugging with visually oriented tools. To mitigate these issues, researchers have developed tools like StructJumper for code navigation and audio-based debuggers like Wicked Audio Debugger and CodeTalk, as well as collaborative tools like CodeWalk and Grid-Coding to improve coding environment accessibility.", "text": "Further studies by Mealin and Murphy-Hill [ 59 ] and Baker et al. [ 11 ] explored specific tools and techniques that developers who are visually impaired use. Mealin and Murphy-Hill found that many developers who are visually impaired rely on text editors rather than IDEs due to accessibility issues employing unique practices like \u201cout-of-context editing,\u201d where blocks of code are copied, edited separately, and pasted back. This preference for text editors has been corroborated by other researchers [ 6 , 63 ], who attribute it to specific challenges such as navigating line-by-line, understanding indentation, and managing nested code structures [ 56 , 99 ]. To address this issue, Baker et al. [ 11 ] created StructJumper, a plugin that generates a hierarchical tree structure of code for easier navigation. Debugging, presents another significant challenge for developers who are visually impaired, largely due to the reliance on visual interfaces in debugging tools, which screen readers struggle to interpret effectively [ 5 , 78 ]. These challenges have led the development of tools like Wicked Audio Debugger (WAD) [ 94 ] , a tool that provides audio descriptions of programs during execution; CodeTalk [ 78 ], a Visual Studio plugin that introduces \u201cTalkPoints\u201d for audio-based debugging; and CodeWalk, a tool by Potluri et al. [ 77 ], which facilitates accessible, remote, synchronous code review and refactoring activities by tethering collaborators\u2019 cursors with the host of a Live Share session. Additionally, Haque et al. [ 27 ] introduced Grid-Coding, a paradigm designed to improve the accessibility of coding environments by representing code in a structured 2D grid, allowing developers who are visually impaired to navigate and edit code more effectively.", "page": 3, "bbox": []}, "seg_32": {"summary": "Summary: Conversational interfaces, especially speech-based ones, show promise in improving comprehension and navigation. Studies have explored their effectiveness in reducing cognitive load through voice commands and their application in development environments using audio cues for tasks like debugging and file navigation.", "text": "Conversational interfaces have also shown promise [ 15 ], Ludi et al. [ 57 ], found that speech-based cues generally provided the best performance for comprehension and navigation tasks. Phutane et al. [ 75 ] explored the use of voice commands to reduce cognitive load. Meanwhile, Stefik et al. [ 95 ] created Sodbeans, an IDE that relies on audio cues for debugging, while Smith et al. [ 90 ] developed a tool to allow developers to navigate the tree structure of files in Eclipse.", "page": 3, "bbox": []}, "seg_33": {"summary": "Summary: Existing research on accessibility for visually impaired developers may not fully address the new challenges and opportunities presented by generative AI coding tools like GitHub Copilot.  The impact of these tools on accessible programming remains largely unexamined as most current studies predate their emergence.", "text": "While previous research has significantly advanced accessibility for developers who are visually impaired, these findings may not fully apply to the new landscape shaped by generative AI-assisted coding tools such as GitHub Copilot. The introduction of these tools brings new challenges and opportunities in accessible programming, an area that remains largely unexamined. The majority of current studies were conducted before the emergence of generative", "page": 3, "bbox": []}, "seg_34": {"summary": "Summary: A manuscript has been submitted to the ACM, likely for review and potential publication.", "text": "Manuscript submitted to ACM", "page": 3, "bbox": []}, "seg_36": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 4, "bbox": []}, "seg_37": {"summary": "Summary:  A significant knowledge gap exists concerning the effects of AI coding tools on developers with visual impairments.  This lack of understanding hinders our ability to assess the impact and possibilities of these tools for visually impaired programmers.", "text": "AI in coding environments, resulting in a critical knowledge gap regarding the impact and potential of these advanced tools for developers who are visually impaired.", "page": 4, "bbox": []}, "seg_38": {"summary": "Insufficient content for summary.", "text": "2.2 AI-assisted Coding Environments", "page": 4, "bbox": []}, "seg_39": {"summary": "AI-assisted coding tools like GitHub Copilot are shown to increase developer productivity and speed up programming tasks. However, studies also reveal challenges, as developers can struggle with understanding, editing, and debugging AI-generated code, impacting task effectiveness.  The tool's utility is also dependent on the developer's expertise level, as less experienced developers may find it harder to assess AI suggestions.", "text": "The integration of AI into software development tools has significantly impacted coding practices, with AI-assisted coding assistants like GitHub Copilot showing promise in improving developer productivity. Ziegler et al. [ 108 ] found that Copilot increases users\u2019 feelings of productivity, with almost a third of its proposed code completions being accepted. In a controlled experiment, Peng et al.[ 73 ] demonstrated that software developers using Github Copilot were able to complete programming tasks significantly faster than those without such assistance. However, Vaithilingam et al. [ 100 ] noted that while most participants preferred using Copilot in daily programming tasks, they often faced difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. This was confirmed by Dakhel et al [ 26 ], who noted that the effectiveness of tools like Github Copilot depends on the developer\u2019s level of expertise, as less experienced developers often lack the necessary skills to effectively evaluate AI-generated code suggestions.", "page": 4, "bbox": []}, "seg_40": {"summary": "Summary: According to a survey by Liang et al., developers primarily use AI programming assistants to reduce keystrokes, speed up task completion, and aid in syntax recall. The survey also indicated that developers are utilizing these tools for a significant portion of their coding, with a median of 30.5% of code being generated with AI assistance.", "text": "A recent survey of developers by Liang et al. [ 55 ] revealed that the primary motivations for using AI programming assistants include reducing keystrokes, finishing programming tasks quickly, and recalling syntax. Developers reported that a median of 30.5% of their code was written with help from tools like Copilot.", "page": 4, "bbox": []}, "seg_41": {"summary": "Studies have explored how developers interact with AI coding assistants, identifying modes like \"acceleration\" and \"exploration\" and comparing human-AI to human-human pair programming.  These studies reveal insights into collaboration dynamics, but a key gap remains in understanding the impact of AI coding assistants on developers with accessibility needs. Further research is needed to address this underrepresented perspective.", "text": "Studies like those by Barke et al.[ 66 ] and Wu et al. [ 58 ] provide insight into the varying modes of interaction with AI coding assistants. Barke et al. [ 66 ] identified \u201cacceleration mode\u201d and \u201cexploration mode\u201d as two broad categories of Copilot use, while Wu et al. [ 58 ] compared human-human pair programming with human-AI pair programming, highlighting differences in collaboration dynamics. However, there is still a significant gap in understanding how these AI tools impact developers with accessibility needs.", "page": 4, "bbox": []}, "seg_42": {"summary": "Insufficient content for summary.", "text": "2.3 Activity Theory", "page": 4, "bbox": []}, "seg_43": {"summary": "A visually impaired software developer uses Github Copilot as a tool to assist with coding tasks. This highlights the use of the tool by a specific user group for a particular purpose.", "text": "Tool ( Github Copilot ) Subject (Software developer who is visually impaired) Object ( Coding task )", "page": 4, "bbox": []}, "seg_44": {"summary": "Summary: Fig. 2 illustrates the relationship between visually impaired software developers (subject), coding tasks (object), and GitHub Copilot (mediating tool). This relationship is presented within the Activity Theory framework.", "text": "Fig. 2. Diagram illustrating the relationship between software developers who are visually impaired (subject), coding tasks (object), and GitHub Copilot (mediating tool) within the Activity Theory framework.", "page": 4, "bbox": []}, "seg_45": {"summary": "Summary: Activity Theory is a conceptual framework used to guide research and problem-solving.  It centers on the concept of \"activity,\" defined as goal-directed actions mediated by artifacts to achieve specific objectives. These actions are carried out through routinized operations.", "text": "Activity Theory is a type of \u201cconceptual frame- work\u201d [ 80 , 82 , 88 , 91 ]. A conceptual framework is an analytical tool or structure used to organize and guide research, projects, or problem-solving efforts [ 40 , 60 ]. The foundational concept of Activity The- ory is the \u201cactivity\u201d, a series of goal directed actions [ 53 ] that aim to achieve specific objectives [ 52 ]. These actions are mediated by artifacts, the instru- ments through which individuals interact with their objectives. Actions themselves are performed via routinized operations, in which individuals are not conscious of or focused on these operations. This process is illustrated in Figure 2 . For example, devel- opers who are visually impaired (the subjects) may have specific goals (objects) related to completing", "page": 4, "bbox": []}, "seg_46": {"summary": "Summary: In the context of programming tasks, an AI programming tool is defined as a mediating artifact, essentially functioning as a tool itself.", "text": "programming tasks. In this context, an AI programming tool acts as the mediating artifact (tool). Manuscript submitted to ACM", "page": 4, "bbox": []}, "seg_48": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 5, "bbox": []}, "seg_49": {"summary": "Summary: B\u00f8dker introduced Activity Theory as a foundational framework for HCI, emphasizing the dynamic interplay between users, their goals, and the tools they use, advocating for tool evolution to meet user needs.  Within this theory, contradictions between users, goals, and tools are seen as drivers of change, while misalignments are identified as localized usability issues that may not require systemic changes. Activity Theory, therefore, guides the design of adaptable interactive systems by addressing these tensions and mismatches.", "text": "B\u00f8dker proposed Activity Theory as a foundational framework for HCI [ 18 ], emphasizing its potential to guide the design of interactive systems by focusing on the dynamic interplay between users, their goals, and the tools users employ (interactive systems) [ 19 ]. B\u00f8dker emphasized that tools should evolve over time to meet users\u2019 needs and adapt to the activities they mediate[ 18 ], highlighting the importance of designing systems that adjust to changing workflows and contexts to remain effective[ 19 ]. Within B\u00f8dker\u2019s definition of Activity Theory for HCI [ 18 ], contradictions are discrepancies or tensions that arise between the tools, the goals, and the users, that must be addressed [ 28 ]. These contradictions are not obstacles but drivers of change, pointing out areas where tools or processes need to evolve to better meet user needs [ 29 ]. Within this space, Activity Theory also introduces the concept of misalignments , which are localized issues or practical mismatches. Unlike contradictions , misalignments hinder usability and effectiveness for end-users but may not necessitate systemic changes.", "page": 5, "bbox": []}, "seg_50": {"summary": "This paper uses Activity Theory to investigate the design of AI-assisted coding tools for visually impaired developers.  Activity Theory is presented as a suitable framework due to its established history in studying technologies designed for diverse groups, including individuals with disabilities. Prior research has successfully applied Activity Theory to analyze various technologies for people with disabilities, such as tactile devices and audio-haptic tools.", "text": "In this paper, we use Activity Theory as a framework to examine the design of AI-assisted coding tools for developers who are visually impaired. We chose this approach because Activity Theory has been widely used for decades to study tools and technologies designed for diverse groups, including individuals with disabilities [ 12 , 18 , 28 , 44 , 45 , 81 , 97 ]. For example, Baldwin et al. [ 12 ] applied Activity Theory to investigate the design of tactile devices and enhanced auditory tools, examining whether these technologies align with the unique workflows of users who have no or low-vision. Similarly, Szymczak [ 97 ] applied Activity Theory to analyze how audio-haptic technologies mediated interactions and supported the goals of individuals who are visually impaired, focusing on how these technologies could assist users in interacting with 2D representations, such as maps and drawings. Tlili et al. [ 98 ], also applied Activity Theory to review the use of game-based learning for learners with disabilities and identify inconsistencies in stakeholder involvement, variability in the use of educational technology, and difficulties in standardizing performance measures. Robins [ 81 ] applied Activity Theory to analyze and design accessibility in video games, focusing on the interaction between visually impaired players, game goals, and mediating tools like audio-haptic technologies and game mechanics.", "page": 5, "bbox": []}, "seg_51": {"summary": "Summary: The table presents data for ten male participants with either low vision or no vision who participated in a study.  These participants possess diverse coding experience and have utilized various AI coding tools such as GitHub Copilot and ChatGPT.  Python and C# were the preferred programming languages for the study task, which was successfully completed by all participants.", "text": "P# Gender Age Exp.(yrs) Vision Status AI-Coding Tool Experience Coding Proficiency Pref. Lang. For Study Task Completed P1 Male 32 8 No vision GitHub Copilot C#, JavaScript, Python, Ruby Python Yes P2 Male 34 12 Low vision GitHub Copilot PHP, C#, JavaScript C# Yes P3 Male 38 9 Low vision CodeWhisperer, Codellama, ChatGPT C++, Python Python Yes P4 Male 43 22 Low vision ChatGPT JavaScript, TypeScript, C# C# Yes P5 Male 26 6 No vision GitHub Copilot, ChatGPT Python Python Yes P6 Male 36 17 Low vision GitHub Copilot PHP, Ruby, Swift, Go, Python, JavaScript, Kotlin Python Yes P7 Male 58 23 No vision GitHub Copilot Python, SQL, PowerShell Python Yes P8 Male 54 32 No vision ChatGPT C, R, Ruby, TypeScript, Swift, Python, Kotlin Python Yes P9 Male 42 4 No vision ChatGPT JavaScript, PHP, Python, HTML, CSS Python Yes P10 Male 24 2 No vision Claude, ChatGPT, Gemini C#, HTML, PHP C# Yes", "page": 5, "bbox": []}, "seg_52": {"summary": "Insufficient content for summary.", "text": "Table 1. Participant Demographics, Vision Status, Experience with AI Coding Assistants, and Programming Background.", "page": 5, "bbox": []}, "seg_53": {"summary": "This paper utilizes Activity Theory to study the impact of AI-assisted coding tools, specifically GitHub Copilot, on visually impaired developers.  The analysis investigates how these tools mediate developer tasks and identifies both systemic contradictions and usability misalignments.  Ultimately, the research aims to provide design recommendations to enhance AI coding tools to better meet the needs of developers with visual impairments.", "text": "Building on this foundation, our paper applies Activity Theory to examine how AI-assisted coding tools, particularly GitHub Copilot, mediate and influence the tasks of developers who are visually impaired. Activity Theory provides a particularly useful analysis framework since the introduction of AI coding assistants may operationalize some actions that have not yet become routine for developers. Additionally, we use Activity Theory to guide design recommendations aimed at improving these tools for this population. To achieve this, we analyze both contradictions \u2014systemic tensions within the activity system\u2014and misalignments \u2014localized issues that disrupt usability. By addressing these challenges, we ensure AI tools meet developers\u2019 needs.", "page": 5, "bbox": []}, "seg_54": {"summary": "Summary: Manuscript submission to ACM has occurred.", "text": "Manuscript submitted to ACM", "page": 5, "bbox": []}, "seg_56": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 6, "bbox": []}, "seg_57": {"summary": "Summary: This text segment lists various chat functionalities.  It includes features such as inline chat and ghost text suggestions, as well as different types of chat window implementations, specifically floating and embedded panes.", "text": "a.Inline chat b. Ghost text suggestions c. Chat Window (floating element) d. Chat window (embedded pane)", "page": 6, "bbox": []}, "seg_58": {"summary": "Summary: GitHub Copilot offers multiple interaction interfaces to assist users, including an inline chat for quick commands, dynamic ghost text suggestions as users type, a floating chat window for temporary interactions, and an embedded pane chat for more extensive and ongoing conversations. These interfaces cater to different user needs and interaction styles.", "text": "Fig. 3. GitHub Copilot interaction interfaces. (a) Inline chat window for quick command input and AI engagement. (b) Ghost text suggestions dynamically generated as the user types. (c) Floating chat window for temporary interactions, ideal for quick-access queries. (d) Embedded pane chat window for ongoing, more extensive conversations with AI, allowing for sustained reference and deeper coding assistance.", "page": 6, "bbox": []}, "seg_59": {"summary": "Insufficient content for summary.", "text": "2.4 Copilot", "page": 6, "bbox": []}, "seg_60": {"summary": "Summary: GitHub Copilot's interface includes an inline chat feature, enabling developers to interact with the AI coding assistant directly within the code editor. This method allows for seamless and quick communication without interrupting the coding process.", "text": "The GitHub Copilot interface provides multiple ways for developers to interact with its AI-driven coding assistant. One primary method is through its inline chat (Fig. 3 a), where users can type commands or questions directly within the code editor. This allows for seamless, quick interactions without disrupting the coding workflow.", "page": 6, "bbox": []}, "seg_61": {"summary": "Developers can interact with Copilot through a chat window that can be either a floating element or an embedded pane within the interface.  Floating windows are suitable for quick queries, while embedded panes are better for extended interactions and referencing past exchanges. This provides developers with flexible interaction styles to suit their workflow.", "text": "Additionally, developers can engage with Copilot using the chat window , which can appear either as a floating element (Fig. 3 c) or as an embedded pane within the interface (Fig. 3 d). An embedded pane refers to a UI component that is fixed within the main window, allowing for continuous visibility and interaction without obstructing other elements on the screen. The floating element is useful for quick, temporary queries, while the embedded pane is better suited for extended interactions where users may need to frequently reference past exchanges. This flexibility enables developers to choose an interaction style that best fits their workflow and preferences.", "page": 6, "bbox": []}, "seg_62": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 6, "bbox": []}, "seg_64": {"summary": "This text segment discusses the effects of generative AI coding assistants on developers who are visually impaired.  It likely explores how these tools impact their coding experience, accessibility, and productivity.  The content will likely delve into both the benefits and challenges presented by AI assistance for this specific group of developers.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 7, "bbox": []}, "seg_65": {"summary": "Summary: Copilot offers ghost text suggestions, providing real-time, semi-transparent code completions as developers type, which can be accepted, modified, or ignored.  Additionally, developers can request code completions by writing natural language comments describing the desired functionality, which Copilot then translates into code snippets. These features aim to enhance developer efficiency.", "text": "Another key interaction method is through ghost text suggestions (Fig. 3 b). As the developer types, Copilot continuously analyzes the context and generates relevant code suggestions in real-time. These suggestions appear as semi-transparent text within the editor, allowing developers to seamlessly integrate them into their code. Developers can choose to accept, modify, or ignore these suggestions based on their needs. Beyond real-time suggestions, developers can also request code completions by writing natural language comments that describe the desired functionality. Copilot then processes these descriptions and generates corresponding code snippets, helping developers implement features more efficiently.", "page": 7, "bbox": []}, "seg_66": {"summary": "Summary: GitHub Copilot is a Visual Studio Code extension, not a standalone editor, and its chat windows are features of the extension itself, not default VS Code functionality.  These chat interfaces enable users to engage with AI-generated coding suggestions directly within their VS Code environment.", "text": "It is important to note that GitHub Copilot is not a standalone code editor but an extension designed to work within Visual Studio Code. The chat windows, including both the floating and embedded pane versions, are features introduced by the GitHub Copilot extension and are not part of Visual Studio Code by default. These chat interfaces allow users to interact directly with AI-generated coding suggestions within their coding environment.", "page": 7, "bbox": []}, "seg_67": {"summary": "Researchers selected GitHub Copilot as the primary AI coding assistant for their study due to its widespread use at the time of research.  Visual Studio Code was chosen as the development environment because of its popularity, accessibility, and seamless integration with GitHub Copilot. This setup allowed the researchers to investigate the influence of these tools on the coding experience of developers who are visually impaired.", "text": "GitHub Copilot was selected as the primary AI coding assistant for this study because, at the time of research, it was the most widely used tool of its kind [ 33 , 92 ]. We chose Visual Studio Code as the development environment due to its popularity and accessibility, as well as its seamless integration with GitHub Copilot [ 93 , 101 ]. This setup enabled us to explore how these tools influence the coding experience of developers who are visually impaired.", "page": 7, "bbox": []}, "seg_68": {"summary": "Insufficient content for summary.", "text": "2.5 Research Gap", "page": 7, "bbox": []}, "seg_69": {"summary": "Summary:  While accessibility has improved for visually impaired developers using traditional coding tools, AI coding assistants like GitHub Copilot present new, under-researched challenges.  Existing research mainly focuses on traditional tools, neglecting the unique complexities of AI-assisted coding, highlighting the need to examine and ensure these new AI tools are inclusive.", "text": "Despite progress in accessibility for developers who are visually impaired [ 13 , 23 , 65 ], generative AI coding assistants like GitHub Copilot introduce new challenges and opportunities that remain under-explored. Most existing research focuses on traditional coding tools [ 1 , 49 ], overlooking complexities unique to AI-assisted coding. Our study examines these accessibility issues to ensure AI assistants like GitHub Copilot are inclusive and support equal access for visually impaired developers.", "page": 7, "bbox": []}, "seg_71": {"summary": "Researchers conducted a study to understand the experiences and challenges faced by visually impaired developers when using GitHub Copilot. Utilizing Activity Theory, the study analyzed the interaction between these developers and the AI tool in coding tasks. The findings revealed design limitations of Copilot in meeting the specific needs of visually impaired developers, alongside the identification of potential opportunities.", "text": "To study the relationship between AI coding assistants and accessibility, we conducted a study with developers who are visually impaired. Our goal was to understand their experiences, challenges, and strategies when using an AI coding assistant, specifically GitHub Copilot. Using Activity Theory as our framework, we analyzed how Copilot (tool) mediates the interaction between visually impaired developers (participants) and their coding tasks (object). Our analysis highlights the contradictions and misalignments that occur when Copilot\u2019s design falls short of meeting the unique needs of developers who are visually impaired, while also uncovering new opportunities that emerge in this environment.", "page": 7, "bbox": []}, "seg_72": {"summary": "Insufficient content for summary.", "text": "3.1 Participant Recruitment", "page": 7, "bbox": []}, "seg_73": {"summary": "Summary: This study recruited ten visually impaired male software developers with varying levels of vision and professional experience (2-32 years) to participate in research.  Participants were sourced through specialized networks and had some familiarity with AI coding assistants. The study acknowledges the limited recruitment pool but justifies the sample size by referencing similar HCI studies on underrepresented groups.", "text": "We recruited 10 software developers who were visually impaired, including 4 with low vision and 6 with no vision, with experience ranging from 2 to 32 years in the field (see Table 1 ). Participants were sourced through professional networks, accessibility-focused online forums, and organizations supporting professionals in tech with visual impairments. To participate, developers needed some familiarity with AI coding assistants, though extensive experience with Copilot was not required. All participants identified as male. Given the specialized nature of developers who are visually impaired, our recruitment pool was naturally limited. However, our sample size aligns with prior HCI studies on underrepresented populations [ 25 , 30 , 69 , 79 , 83 ]. Despite these constraints, our study provides valuable insights into the accessibility", "page": 7, "bbox": []}, "seg_74": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 7, "bbox": []}, "seg_76": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 8, "bbox": []}, "seg_77": {"summary": "Summary: This text segment refers to an underrepresented group and mentions challenges and opportunities they face.  Participants from this group received $50 USD as compensation for their time, suggesting involvement in a study or event.", "text": "challenges and opportunities for this underrepresented group. Each participant received $50 USD (or its equivalent in local currency) as compensation for their time.", "page": 8, "bbox": []}, "seg_78": {"summary": "Insufficient content for summary.", "text": "3.2 Study setup", "page": 8, "bbox": []}, "seg_79": {"summary": "Summary: This study was conducted remotely, allowing participants to utilize their preferred accessible setups.  Each session was approximately 90 minutes long and followed four sequential phases.", "text": "We conducted our study remotely, ensuring that participants could use their preferred accessible setup. Each session lasted approximately 90 minutes and followed four sequential phases:", "page": 8, "bbox": []}, "seg_80": {"summary": "Summary: This section of the study focuses on participant backgrounds and their experience with AI coding assistants.  Researchers collected information on participants' professional and technical backgrounds and their prior use of tools like GitHub Copilot.", "text": "(1) Participant Background and Copilot Experience: We asked participants to describe their professional and technical backgrounds and share their prior experience with AI coding assistants like GitHub Copilot.", "page": 8, "bbox": []}, "seg_81": {"summary": "Summary: Participants attended a training session to learn about GitHub Copilot and its features. The session also detailed how Copilot works with accessible development environments and assistive technologies that participants were already familiar with. This training was designed to prepare participants to effectively use Copilot for upcoming coding tasks.", "text": "(2) Training Session: We provided participants with a training session that introduced GitHub Copilot and its key features. The session also covered how Copilot integrates with accessible development environments and assistive technologies participants were already using. This ensured that all participants had a solid understanding of how to interact with Copilot before starting the coding tasks.", "page": 8, "bbox": []}, "seg_82": {"summary": "Participants in the study engaged in practical coding and debugging exercises.  These tasks were performed using GitHub Copilot within the Visual Studio Code environment. This hands-on component aimed to assess participant interaction with the tool in realistic software development scenarios.", "text": "(3) Hands-on Coding and Debugging Tasks: We instructed participants to complete a coding task followed by a debugging task using GitHub Copilot in Visual Studio Code.", "page": 8, "bbox": []}, "seg_83": {"summary": "Summary: Participants were tasked with writing a program to calculate the days until a user's next birthday, requiring specific date input formats and class implementation with unit tests.  They were instructed to use the \"think-aloud\" protocol and utilize Copilot while completing the programming task. This programming assignment was designed following prior research methods.", "text": "\u2022 Programming Task: Following prior work [ 46 ], we asked participants to write a program that calculates the number of days until the user\u2019s next birthday. The program required a birthday string input in either DDMMYY or DDMMYYYY format and a class implementation to handle date calculations. We also asked participants to write unit tests for their class. Throughout the task, we encouraged them to use the \"think-aloud\" protocol, verbalizing their thought process while interacting with Copilot.", "page": 8, "bbox": []}, "seg_84": {"summary": "Insufficient content for summary.", "text": "We selected this task because:", "page": 8, "bbox": []}, "seg_85": {"summary": "Summary: This tool integrates familiar programming concepts like string manipulation, date calculations, and object-oriented principles.  These features ensure relevance and applicability to participants' standard coding practices.", "text": "\u2013 It incorporates common programming concepts such as string manipulation, date calculations, and object- oriented principles, making it relevant to participants\u2019 typical coding workflows.", "page": 8, "bbox": []}, "seg_86": {"summary": "Summary: The subject matter does not necessitate specific programming framework expertise. This simplifies the process of recruiting participants.", "text": "\u2013 It does not require specialized knowledge of specific programming frameworks, simplifying participant recruitment.", "page": 8, "bbox": []}, "seg_87": {"summary": "Participants participated in a debugging task where they had to find and correct errors in code produced by Copilot.  This session involved them discussing the debugging process as they worked to fix the code.", "text": "\u2022 Debugging Task: We then asked participants to engage in a debugging session, where they identified and fixed errors in the code generated by Copilot. During this session, we prompted them to discuss:", "page": 8, "bbox": []}, "seg_88": {"summary": "Summary: Insufficient content for summary.", "text": "\u2013 Their usual approach to evaluating code accuracy.", "page": 8, "bbox": []}, "seg_89": {"summary": "Insufficient content for summary.", "text": "\u2013 The methods they used to handle encountered issues.", "page": 8, "bbox": []}, "seg_90": {"summary": "Insufficient content for summary.", "text": "\u2013 Whether Copilot simplified or complicated their debugging process.", "page": 8, "bbox": []}, "seg_91": {"summary": "Summary: Following the programming tasks, semi-structured interviews were conducted where participants detailed their code organization, adjustments made due to Copilot, and challenges encountered.  Participants also reflected on Copilot's impact on task complexity, highlighting instances of its helpfulness or difficulty.  Further details about the interview questions can be found in the Appendix.", "text": "(4) Post-Interview: After completing the programming and debugging tasks, we conducted a semi-structured interview with each participant. We asked them to explain their approach to organizing their code and describe any adjustments they made based on Copilot\u2019s suggestions. They also shared the challenges they encountered and the solutions they implemented. Additionally, we encouraged them to reflect on how Copilot influenced the complexity of their tasks, providing examples of when it was particularly helpful or challenging. Our Appendix contains further details on the post-study interview questions.", "page": 8, "bbox": []}, "seg_92": {"summary": "Summary: Figure 1 illustrates the study setup with a diagram.", "text": "A diagram illustrating our study setup is shown in Figure 1 .", "page": 8, "bbox": []}, "seg_93": {"summary": "Insufficient content for summary.", "text": "3.3 Data Collection and Analysis", "page": 8, "bbox": []}, "seg_94": {"summary": "The researchers collected data through interviews, recording and transcribing both participant verbal responses and audio cues from assistive technologies.  They then analyzed this data using thematic coding to identify key patterns and insights. This methodology allowed for a comprehensive understanding of the participant experience.", "text": "We recorded and transcribed all interviews, capturing both verbal responses and relevant audio cues from participants\u2019 screen readers and other assistive technologies. We analyzed the data using thematic coding, focusing on significant Manuscript submitted to ACM", "page": 8, "bbox": []}, "seg_96": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 9, "bbox": []}, "seg_97": {"summary": "This study analyzed the experiences of visually impaired developers using GitHub Copilot by qualitatively coding interview transcripts to identify key themes and patterns.  Researchers focused on critical incidents to understand how Copilot impacted accessibility and aligned with or diverged from non-visual coding strategies. Activity Theory was used to further analyze the opportunities and challenges encountered by these developers when using the AI coding assistant.", "text": "events that participants experienced while using GitHub Copilot. Two of us independently coded the transcripts, identifying emerging themes and patterns. We paid special attention to pivotal moments that either enhanced or hindered participants\u2019 coding processes. These critical incidents provided concrete examples of how Copilot mediated accessibility, revealing specific ways in which the tool aligned with or diverged from the non-visual coding strategies used by developers who are visually impaired. To deepen our analysis, we integrated Activity Theory , which allowed us to examine the opportunities and contradictions that visually impaired developers encountered when interacting with an AI coding assistant. In particular, Activity Theory helped us explore within our themes:", "page": 9, "bbox": []}, "seg_98": {"summary": "Researchers applied Activity Theory to study how visually impaired developers used GitHub Copilot for coding tasks.  The study mapped the interactions within this activity system to understand how Copilot impacted developer workflows and to identify any resulting challenges or misalignments. This framework helped analyze the dynamics between developers, Copilot, and their coding activities.", "text": "(1) Activity-Centric Lens: how developers who are visually impaired ( subjects ) used GitHub Copilot ( tool ) to complete their coding tasks ( object ). By applying Activity Theory, we mapped the observed dynamics within the activity system, identifying key interactions and tensions among its components. This framework helped us understand how Copilot influenced developers\u2019 workflows and where misalignments or challenges emerged.", "page": 9, "bbox": []}, "seg_99": {"summary": "Visually impaired developers encounter challenges due to systemic contradictions in AI tools like Copilot.  Specifically, context switching and dynamic behaviors within these tools disrupt the sequential workflows crucial for these developers. These disruptions necessitate extra navigation adaptations to maintain productivity.", "text": "(2) Systemic Contradictions: challenges that developers who are visually impaired faced, particularly those arising from systemic contradictions . For example, some participants struggled with context switching imposed by the AI, while others had difficulty navigating Copilot\u2019s dynamic behaviors, which disrupted the sequential workflows that are critical for developers who are visually impaired. These disruptions required additional navigation adaptations to maintain productivity.", "page": 9, "bbox": []}, "seg_100": {"summary": "Summary: The text segment discusses adaptive solutions to identified challenges and mentions the application of the Activity Theory framework. This framework provides insights into how AI assistants can be improved to better suit the workflows of visually impaired developers. The ultimate goal is to create more inclusive and efficient coding environments.", "text": "(3) Adaptive Solutions: possible adaptive solutions to the challenges we identified. By applying the Activity Theory framework, we gained insights into how AI assistants could be better aligned with the specific workflows of developers who are visually impaired, ultimately fostering more inclusive and efficient coding environments.", "page": 9, "bbox": []}, "seg_101": {"summary": "Summary: This study used Activity Theory to analyze how visually impaired developers interact with AI coding assistants like Copilot. The analysis revealed key accessibility themes and design elements within Copilot that either helped or hindered these developers' workflows.  Ultimately, the findings advocate for a re-evaluation of accessibility standards in AI-assisted coding environments.", "text": "By integrating Activity Theory into our thematic analysis, we gained a deeper understanding of how developers who are visually impaired engage with AI coding assistants. This approach allowed us to identify key accessibility themes and design elements of Copilot that either supported or hindered participants\u2019 work processes. Our findings contribute to the broader goal of rethinking accessibility paradigms in AI-assisted coding environments.", "page": 9, "bbox": []}, "seg_103": {"summary": "Summary: Research involving interviews and observations has identified key themes regarding the intricate connection between AI coding assistants, accessibility requirements, and coding methods. These themes shed light on the challenges and opportunities encountered by visually impaired developers when utilizing AI-assisted coding tools such as GitHub Copilot. This research directly addresses the question of how these tools impact accessibility and coding practices for this specific group of developers.", "text": "Through our interviews and observations, we identified key themes that highlight the complex relationship between AI coding assistants, accessibility needs, and coding practices. These themes address our research question by revealing the challenges and opportunities that developers who are visually impaired encounter when using AI-assisted coding tools like GitHub Copilot.", "page": 9, "bbox": []}, "seg_104": {"summary": "This section presents themes from interviews and critical incidents to illustrate how generative AI coding assistants impact visually impaired developers, both positively and negatively.  Framed within Activity Theory, the findings reveal systemic misalignments between users, tools, and coding environments. The study aims to identify opportunities to improve the accessibility and inclusivity of AI coding assistants.", "text": "In this section, we present these themes. For each theme, we include illustrative quotes and describe critical incidents from our interviews. These incidents provide insight into the lived experiences of our participants, offering concrete examples of how generative AI coding assistants can both empower and hinder developers who are visually impaired. We frame our findings within the Activity Theory framework to contextualize these challenges as systemic misalignments between users, their tools, and the coding environments they work in. This approach allows us to examine how AI coding assistants mediate the development process and where breakdowns occur, helping us identify opportunities for more accessible and inclusive AI coding assistants.", "page": 9, "bbox": []}, "seg_105": {"summary": "Summary: This section, labeled as Research Question 1 (RQ1), explores the challenges and opportunities that AI coding assistants present for developers who are visually impaired. It aims to understand the specific implications of these tools for this user group.", "text": "4.1 RQ1: Challenges and Opportunities of AI Coding Assistants for Developers Who Are Visually Impaired", "page": 9, "bbox": []}, "seg_106": {"summary": "AI coding assistants were found to enhance the sense of control for visually impaired developers.", "text": "4.1.1 AI and Control . Our interviews revealed that AI coding assistants contributed to a dynamic sense of control among developers who are visually impaired. This enhanced control manifested in several positive ways. Participants", "page": 9, "bbox": []}, "seg_107": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 9, "bbox": []}, "seg_109": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 10, "bbox": []}, "seg_110": {"summary": "Summary: AI assistants are shifting developers' focus towards strategic control in coding by enabling them to delegate routine tasks. This allows developers to concentrate on high-level decisions and code structure, ultimately increasing their control over the development process. For example, Copilot assists with tedious tasks like generating documentation, freeing developers to focus on more strategic aspects of coding.", "text": "described experiencing a shift toward strategic control over the coding process. Rather than manually performing every task, they found that AI assistant allowed them to delegate routine or less engaging tasks, enabling them to focus on high-level decision-making and overall code structure. This ability to offload work gave them greater control in shaping their development process while maintaining oversight of their projects. For instance, P7 highlighted how Copilot eased their workload and gave them more control over their coding process by handling tasks they found tedious, such as generating docstrings\u2014special multiline comments in programming that explain the functionality of a function, class, or module:", "page": 10, "bbox": []}, "seg_111": {"summary": "Summary: The developer reports a positive experience with Copilot, particularly for its ability to automate tasks they find unenjoyable, such as generating documentation.  They appreciate Copilot's assistance with these less desirable aspects of development, even while planning to write the majority of their code themselves.", "text": "\u201cOverall, my experience with Copilot is positive, especially because it helps me with tasks I don\u2019t enjoy. I will write the majority of my own code [...], but where it [Copilot] can help me, I\u2019ll definitely let it [Copilot] do it. I love having it generate docstrings for me. That\u2019s cool, because frankly, I don\u2019t like writing documentation. I\u2019d rather code and let it do the heavy lifting for me...\u201d - P7, Developer with no vision.", "page": 10, "bbox": []}, "seg_112": {"summary": "Summary: AI empowers developers by automating routine tasks, freeing them to focus on complex challenges and increasing their workflow control.  One participant compared using AI to piloting an aircraft, highlighting this enhanced control.", "text": "This experience demonstrates how AI empowers developers by taking over tedious tasks they prefer to avoid. By automating routine work, AI enables developers to maintain greater control over their workflow and focus on more complex and engaging coding challenges. Similarly, participant P1, who had prior experience with AI coding assistants, compared coding with AI to piloting an aircraft, emphasizing how these assistants increase their sense of control:", "page": 10, "bbox": []}, "seg_113": {"summary": "Summary: This developer likens using Copilot to pair programming and flying a plane, where Copilot handles the repetitive coding tasks, similar to an autopilot.  The developer's role then shifts to a higher-level perspective, focusing on the overall direction and identifying potential problems, much like a pilot monitoring a plane's course.", "text": "\u201cI sort of compare it [Copilot] to pair programming almost, where [...] I\u2019m sort of driving, but the other person is doing all the sort of drudgery work. And what I\u2019m really mostly doing now is almost like what a pilot does when they\u2019re flying a plane. They\u2019re [...] not flying the plane physically for most of it. Most of it, the plane\u2019s flying itself, but the pilot does have to keep an eye on. Are we still going in the right direction? What\u2019s that big thing coming towards us really fast? Should we avoid that?\u201d - P1, Developer with no vision.", "page": 10, "bbox": []}, "seg_114": {"summary": "Summary: Activity Theory suggests AI tools like Copilot mediate the coding process, transforming how developers work. Copilot acts as an intermediary, shifting developer control and enabling them to concentrate on strategic, high-level decisions instead of lower-level coding tasks. This reallocation of focus is exemplified by developers P7 and P1.", "text": "From the perspective of Activity Theory, these findings illustrate the transformative role of AI tools as mediators in the coding process. In this context, Copilot functions as an intermediary that actively shapes a developer\u2019s control over their work, allowing developers to reallocate their focus toward higher-level strategic decisions, as demonstrated by P7 and P1.", "page": 10, "bbox": []}, "seg_115": {"summary": "Visually impaired developers can utilize AI coding assistants in a supervisory control model, where they oversee and guide the AI's output, correcting mistakes and ensuring alignment with their coding objectives. This approach shifts the developer's role from manual code writing to strategically guiding and refining AI-generated code, similar to a pilot managing an aircraft's automated systems.  This allows developers to focus on high-level project direction while delegating implementation details to the AI.", "text": "This shift in control also aligns with the concept of supervisory control [ 24 , 87 ], in which humans oversee and direct automated systems rather than executing tasks manually. In this case, the developers who are visually impaired guide the AI coding assistants by monitoring their outputs, making corrections when necessary, and ensuring alignment with their broader coding goals. Just as a pilot monitors and manages an aircraft\u2019s automated systems while remaining responsible for high-level navigation and safety decisions, developers can delegate routine implementation details to Copilot while maintaining oversight and control over the overall project direction. This dynamic shifts the developer\u2019s role from manually writing every line of code to curating, refining, and strategically guiding the AI\u2019s output.", "page": 10, "bbox": []}, "seg_116": {"summary": "Summary: In this evolving coding paradigm, developer control shifts from writing individual lines of code to a more strategic role of guiding the overall direction and making critical decisions. Developers now oversee AI-generated suggestions, ensuring they align with the broader project architecture and goals, while still retaining ultimate authority. This represents a move towards a more abstract and high-level form of control.", "text": "In this new paradigm, control manifests as the ability to guide the overall direction of the code, make critical decisions, and intervene when necessary. While the developer retains ultimate authority over the coding process, the nature of that control shifts to a more abstract and strategic level. Rather than focusing solely on writing individual lines of code, developers can now oversee and direct AI-generated suggestions to ensure they align with the broader project architecture and goals.", "page": 10, "bbox": []}, "seg_117": {"summary": "Summary: The increasing control developers have with AI tools demands they acquire new skills. These skills include prompt engineering to guide AI outputs, evaluating the quality of AI-generated code, and maintaining a high-level perspective on software design.", "text": "However, this shift in control also requires developers to develop new skills, such as crafting effective prompts to generate useful AI outputs, quickly assessing the quality and relevance of the AI-generated code, and maintaining a high-level understanding of the overall software design. As highlighted by P1:", "page": 10, "bbox": []}, "seg_118": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 10, "bbox": []}, "seg_120": {"summary": "This text segment likely discusses the effects of generative AI coding assistants on developers who are visually impaired. It probably explores how these AI tools influence the workflows, accessibility, and productivity of visually impaired programmers.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 11, "bbox": []}, "seg_121": {"summary": "Summary: A developer using Copilot found that the AI assistant made a mistake in a day counting function by not considering different years, a type of error they had observed previously. This experience underscores the need for careful prompting and user vigilance when working with AI coding assistants to avoid such oversights.", "text": "\u201c...when I was checking that initial version of the day counting function, I wasn\u2019t sure if it [Copilot] realized that it wasn\u2019t always going to be the same year, and it [Copilot] didn\u2019t realize that in this case. But I\u2019ve seen it make that kind of mistake before [...] You just need to be careful with your prompts [communication with the AI assistant].\u201d - P1, Developer with no vision.", "page": 11, "bbox": []}, "seg_122": {"summary": "AI-assisted coding is changing the role of visually impaired developers, allowing them to focus on strategic oversight rather than manual coding tasks.  This shift presents a challenge to design accessible AI coding environments that empower these developers with control over their workflows.  Ensuring accessibility in these new environments is crucial for visually impaired developers to maintain command over their coding processes.", "text": "This shift in AI-assisted coding, where developers focus on strategic oversight rather than manually completing every coding task, marks an evolution in how developers who are visually impaired maintain control over their coding workflows [ 8 ]. The challenge lies in designing AI coding environments that empower developers with this level of control while ensuring accessibility.", "page": 11, "bbox": []}, "seg_123": {"summary": "Summary: Designers of AI coding environments must recognize the distinct needs of visually impaired developers, who rely on predictable and structured interactions rather than the visual cues preferred by sighted developers.  Integrating generative AI, which is inherently unpredictable, presents a challenge to creating accessible coding environments that meet these needs.  Therefore, careful design is crucial to balance AI assistance with the accessibility requirements of visually impaired developers.", "text": "To achieve this, designers of AI coding environments must recognize that the needs of developers who are visually impaired often differ from those of sighted developers [ 8 , 27 , 107 ]. While sighted developers frequently rely on visual cues and dynamic, exploratory interfaces, visually impaired developers usually require predictable, structured interactions with clear, interpretable feedback [ 63 ]. This requirement is not merely a preference but a necessity that allows them to effectively understand, navigate, and maintain control over their workflow [ 48 , 62 ]. Now, a key consideration in designing AI coding environments for visually impaired developers is enabling them to anticipate the AI\u2019s actions and seamlessly integrate its assistance into their coding tasks [ 64 ]. However, generative AI inherently introduces a degree of unpredictability, making this integration complex. As a result, developing AI-driven coding environments that balance strategic oversight with accessibility is not a trivial task and requires careful, thoughtful design.", "page": 11, "bbox": []}, "seg_124": {"summary": "AI-assisted interfaces like Copilot introduce context switching challenges due to their dynamic suggestions, forcing developers to frequently shift focus between writing code and reviewing AI outputs. This disruption is particularly problematic for visually impaired developers, as it interferes with their structured navigation strategies and makes it harder to maintain workflow continuity.", "text": "4.1.2 Context Switching Difficulties in AI-assisted Interfaces . Although Copilot provided a sense of control and empowerment, it also introduced new challenges related to context switching. Our findings highlight that the dynamic nature of AI-generated suggestions and the frequent need to shift focus between writing code and reviewing AI outputs created disruptions. For developers who are visually impaired, this shift interfered with the structured navigation strategies they had developed for traditional coding environments [ 8 ], making it harder to maintain workflow continuity.", "page": 11, "bbox": []}, "seg_125": {"summary": "The AI assistant's unexpected view changes disrupted developer workflows by forcing them to switch focus and lose context.  For example, interacting with code suggestions automatically shifted the view, often without notification. This made it challenging for developers to maintain their flow and seamlessly integrate AI-generated code.", "text": "The AI assistant frequently triggered unexpected view changes, forcing developers to shift their focus to different tasks or sections of the coding environment. For example, as developers interacted with Copilot\u2019s suggestions, the system would automatically switch their view to the newly generated code. This disrupted their workflow, especially because they were not always notified about these sudden shifts. As a result, developers struggled to maintain context, making it even more challenging to navigate and integrate AI-generated code seamlessly.", "page": 11, "bbox": []}, "seg_126": {"summary": "Summary: Context switching in AI-assisted coding environments creates significant accessibility barriers for visually impaired developers, especially those using screen readers.  This underscores the necessity for AI coding assistants to be designed with improved accessibility to better support screen reader users' reliance on sequential navigation.", "text": "The challenges of context switching in AI-assisted coding environments pose a significant accessibility barrier for developers who are visually impaired. These difficulties highlight the need for AI coding assistants to be designed with greater attention to the needs of screen reader users, who rely on sequential navigation [ 37 , 109 ].", "page": 11, "bbox": []}, "seg_127": {"summary": "Summary: User P9 faced usability issues with Copilot's embedded chat feature.  After posing a question, P9 had trouble locating and navigating to the answer provided by Copilot within the text. This difficulty hindered workflow and increased mental effort.", "text": "This challenge became evident with P9, who encountered difficulties after typing a question in the embedded chat window. When Copilot generated a response, he struggled to locate and navigate to the text where the answer was displayed, disrupting his workflow and adding unnecessary cognitive load:", "page": 11, "bbox": []}, "seg_128": {"summary": "Summary: The user, a developer with no vision (P9), is confused about how to navigate the interface to find the AI assistant's response. They are unsure if they need to use the up and down keys and suspect the focus might still be on their initial question, preventing them from seeing the AI's answer.  This indicates a potential usability issue for users with no vision in accessing the AI's output.", "text": "\u201cOkay, so do I need, I\u2019m just, I\u2019m trying to understand if we need to press the up and down [the participant repeatedly pressed keys to locate the AI assistant\u2019s response]. Seems like it still, the focus is still on my question that I typed [and not on the AI\u2019s response]...\u201d - P9, Developer with no vision.", "page": 11, "bbox": []}, "seg_129": {"summary": "Summary: Blind users rely on stable context for efficient screen reader use, but AI coding introduces frequent and abrupt context switching. This can negatively impact the workflow of blind users who depend on consistent context.", "text": "P1 emphasized that blind users depend on maintaining a stable and consistent context to work efficiently, largely due to how screen readers are designed. However, the frequent and abrupt context switching introduced by AI coding", "page": 11, "bbox": []}, "seg_130": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 11, "bbox": []}, "seg_132": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 12, "bbox": []}, "seg_133": {"summary": "Assistants hinder visually impaired developers by disrupting the continuous workflow necessary for orientation and task management in coding. This disruption negatively impacts their ability to code effectively.", "text": "assistants disrupts this continuity, making it difficult for developers who are visually impaired to stay oriented and manage their coding tasks effectively:", "page": 12, "bbox": []}, "seg_134": {"summary": "Summary: Screen readers present information sequentially, meaning blind users can only access one element at a time, such as a single window or line of text. This sequential nature contrasts with visual interfaces where sighted users can perceive multiple elements simultaneously.  This limitation is a key aspect of how blind individuals interact with digital content.", "text": "\u201cA blind person really only has one thing they can see at any given time. It would be that one window, one line of text, one line of whatever it is [...] because screen readers, that\u2019s just how they work. You can only see one thing at a time. They work sequentially and not parallel.\u201d - P1, Developer with no vision.", "page": 12, "bbox": []}, "seg_135": {"summary": "Summary: The user, P5, found that navigating between coding and Copilot's chat window required too many keystrokes, which exacerbated context-switching issues. This excessive effort for minor transitions caused task disruption and frustration, hindering workflow and focus during coding.", "text": "P5 explained that the context-switching issue worsened due to the many keystrokes required to navigate between different contexts, such as reaching the Copilot chat window after writing code. The need for multiple keystrokes or clicks for minor transitions caused him to lose track of his original task, leading to frustration. This additional effort further amplified the challenge, making it harder to maintain workflow and stay focused on coding tasks:", "page": 12, "bbox": []}, "seg_136": {"summary": "Summary: A developer with no vision expresses frustration with coding due to the heavy reliance on memorization and frequent need to reference files.  Using Copilot, while helpful, also introduces frustration because navigating to and from the Copilot chat window disrupts their train of thought and causes them to lose context, requiring them to re-examine code. This context switching makes it harder to remember their original coding intentions.", "text": "\u201c...when I\u2019m coding, I have to memorize so much... It\u2019s a lot of, I\u2019m going to reference this file again because I can\u2019t 100% remember what I said...I was going to do. And you know when I\u2019m going to Copilot, when I\u2019m pressing F6 a couple of times and I\u2019m pressing tab a couple of times, that creates a little frustration almost, because I want to get back to the task [The participant used multiple key presses to navigate to the Copilot chat window and then return to their code]. And that frustration makes it hard to remember what I was thinking about, to begin with. And then...I forget the context...now I have to go back and check again.\u201d - P5, Developer with no vision.", "page": 12, "bbox": []}, "seg_137": {"summary": "Unexpected AI responses in AI-assisted coding environments worsen context switching challenges.  For example, a developer with no vision, P5, found that navigating the coding interface triggered unintended AI actions, which further complicated navigation.", "text": "The challenges of context switching in AI-assisted coding environments were further exacerbated by unexpected AI responses that caused sudden and confusing shifts in context. P5, a developer with no vision, described a particularly frustrating experience where merely navigating through the coding interface unintentionally triggered AI actions, making navigation even more difficult:", "page": 12, "bbox": []}, "seg_138": {"summary": "Summary: A developer with no vision, P5, describes accidentally accepting unwanted Copilot suggestions due to the UI design.  The issue arises because Copilot immediately suggests code when the cursor moves, and visually impaired developers often use the tab key for navigation, making accidental acceptance easy. This highlights a usability problem for developers who navigate interfaces with the tab key.", "text": "\u201cDid I accidentally just accept [accept the AI\u2019s suggestion] I don\u2019t, I think I hit tab. See, this is part of the problem too. It\u2019s so easy to accidentally accept a suggestion because when you move the cursor, Copilot immediately is like, oh, I have a suggestion for this and I\u2019m just going to suggest it to you. But you know, again, the way a lot of us [developers who are visually impaired] navigate the UI is usually with the tab key. And so it\u2019s so easy to just accidentally insert a suggestion\u201d - P5, Developer with no vision.", "page": 12, "bbox": []}, "seg_139": {"summary": "Summary: Users of screen magnification software encountered issues with the AI assistant because it would sometimes display information outside of their zoomed-in view. This meant that users were often unaware of suggestions provided by the AI assistant as the content was not visible on their screen.  This poses a challenge for accessibility when using AI assistants with screen magnification.", "text": "Similarly, P2 highlighted a significant challenge for users who rely on screen magnification: the AI assistant sometimes displayed information in areas of the screen that were out of view. Because these users were zoomed into a specific section of the screen, they often remained unaware when the AI assistant provided suggestions, as the content appeared outside their visible area:", "page": 12, "bbox": []}, "seg_140": {"summary": "Summary: A developer with low vision reports that zooming in on their screen to improve visibility obstructs their view of AI assistant suggestions, which are placed in an area outside of their zoomed-in view. This makes it difficult to access the AI assistant's features and suggests that for users who utilize screen zoom, the AI assistant's interface should be positioned more centrally. The user implies that the current placement hinders accessibility for zoomed-in users.", "text": "\u201c...If I, for example, zoom in like this [the participant zoomed into a specific screen area], I often don\u2019t get any information if there\u2019s some programming error or something going on in this area [the participant pointed to another part of the screen]. When I look at this [the zoomed-in area] and I just remember every time I have to go this way [The participant pointed to the unseen screen area where Copilot placed suggestions.] Sometimes I don\u2019t see that [AI suggestion] if I zoom in. So for people who are using Zoom, it will be better that it [AI assistant] will show up in the middle of the program because I don\u2019t see it [AI assistant] when it\u2019s in this area [zoomed-in area of the screen]. If there\u2019s something I have to install, for example, to run anything.\u201d - P2, Developer with low vision.", "page": 12, "bbox": []}, "seg_141": {"summary": "Summary: This text segment indicates that the study's participant experiences corroborate previous research on the difficulties visually impaired developers face when context-switching.  Specifically, the findings are presented as an extension of the work conducted by Albusays et al. [6].", "text": "These participant experiences align with and extend prior research on context-switching challenges for developers who are visually impaired. Our findings build upon the work of Albusays et al. [ 6 ], who conducted interviews and Manuscript submitted to ACM", "page": 12, "bbox": []}, "seg_143": {"summary": "Insufficient content for summary.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 13, "bbox": []}, "seg_144": {"summary": "Summary: A study on blind software developers found that AI-assisted coding environments exacerbate code navigation difficulties compared to traditional IDEs.  While previous research focused on challenges in traditional IDEs, this study reveals that AI introduces additional interface elements and interaction modes that complicate navigation and workflow.  This increased complexity stems from the need for frequent context switching between manual code and AI-generated content, disrupting established navigation strategies for visually impaired developers.", "text": "observations with blind software developers to examine code navigation difficulties in traditional IDEs. Their study identified the challenges blind developers face when moving between different sections of a program. Our research expands on these insights by showing how AI-assisted coding environments amplify these difficulties by introducing additional interface elements and new interaction modes, further complicating navigation and workflow continuity. While Albusays et al. [ 6 ] focused on traditional IDEs, our study reveals that AI-assisted coding environments introduce an additional layer of complexity by requiring developers to frequently switch contexts within the same application. This added complexity exacerbates the already challenging task of code navigation for developers who are visually impaired. The dynamic nature of AI-generated suggestions and the frequent need to shift focus between manually written code and AI-generated content disrupt the mental models and navigation strategies that blind developers have developed.", "page": 13, "bbox": []}, "seg_145": {"summary": "Summary: Activity Theory suggests that context switching challenges for screen reader users of AI coding assistants arise from contradictions in user-AI interaction. This is mainly because AI coding assistants are designed for visual interfaces, which is incompatible with the nonvisual navigation methods employed by screen reader users.  Further factors also contribute to this mismatch.", "text": "From an Activity Theory perspective, the challenges of context switching reveal contradictions in the interaction between the user and the AI coding assistant. These contradictions likely arise because AI coding assistants are primarily designed for visual interaction, which conflicts with the sequential, nonvisual navigation methods used by screen reader users. However, several additional factors contribute to this misalignment:", "page": 13, "bbox": []}, "seg_146": {"summary": "Summary: Navigating different contexts within AI-assisted environments requires multiple keystrokes. This necessity could potentially slow down user workflows and make context switching less efficient.", "text": "\u2022 The need for multiple keystrokes to move between different contexts within the AI-assisted environment.", "page": 13, "bbox": []}, "seg_147": {"summary": "Summary: Unexpected behaviors from AI systems are disrupting the established navigation patterns of developers who are visually impaired. This interference poses challenges for these developers in their work.", "text": "\u2022 Unexpected AI behaviors that interfere with established navigation patterns of developers who are visually impaired.", "page": 13, "bbox": []}, "seg_148": {"summary": "Summary: Screen magnification can lead to information being displayed outside of the visible screen area. This makes the information inaccessible to the user who is relying on magnification.", "text": "\u2022 Information being displayed in inaccessible areas due to screen magnification.", "page": 13, "bbox": []}, "seg_149": {"summary": "Summary: The design of the tool is not suitable for visually impaired developers. This incompatibility negatively impacts their concentration and ability to work efficiently.", "text": "This mismatch between the tool\u2019s design and the needs of developers who are visually impaired disrupts their ability to stay focused and maintain a smooth workflow.", "page": 13, "bbox": []}, "seg_150": {"summary": "AI coding assistants, despite offering greater control and efficiency, can overwhelm developers with constant suggestions, leading to cognitive strain.  This constant stream of AI-generated \"ghost text\" creates a tension between leveraging AI and maintaining mental focus. Consequently, developers express a need for \"AI timeouts,\" periods of coding without AI intervention, to regain control and focus.", "text": "4.1.3 AI Timeouts . While AI coding assistants gave developers greater control over their code and reduced the need for manually completing every task, they also introduced cognitive challenges. Participants specifically expressed a need for moments of disconnection from the AI. The constant stream of AI-generated suggestions\u2014especially the frequent appearance of ghost text \u2014created a trade-off between maintaining control and mental focus. This tension led to a key theme in our study: participants\u2019 desire for what we call \"AI timeouts\" \u2014periods of uninterrupted coding without AI intervention. For instance, participant P1, a braille display user, stressed the importance of having control over when AI suggestions appear. They found that frequent AI interventions disrupted their thought process, stating:", "page": 13, "bbox": []}, "seg_151": {"summary": "Summary: A developer with no vision states that they turn off speech functionality when ghost text appears. This allows them time to think and process the information presented.", "text": "\u201cWhat I usually do when this happens [when ghost text is presented] is I\u2019ll turn speech off for a bit so I can think....\u201d - P1, Developer with no vision.", "page": 13, "bbox": []}, "seg_152": {"summary": "Participant P1's strategy of turning off speech to think exemplifies the problem of information overload for screen-reader users.  Research by Ahmed et al. [3] indicates that users with visual impairments often experience cognitive overload because they must listen to content to assess its relevance, a challenge in non-visual interfaces. This highlights the difficulty screen-reader users face in efficiently processing information.", "text": "Participant P1\u2019s strategy of turning off speech to think highlights the issue of information overload , a challenge explored by Ahmed et al. [ 3 ] in their research on non-visual interfaces. Ahmed et al. explain that screen-reader users often struggle to determine the relevance or importance of content without first listening to at least some of it. This necessity frequently results in cognitive overload for users who are visually impaired.", "page": 13, "bbox": []}, "seg_153": {"summary": "AI-assisted coding environments can overwhelm users with continuous AI-generated suggestions, particularly through speech output, as illustrated by one user turning off speech to manage the influx of information. This strategy of creating an \"AI timeout\" allows users to pause and process information effectively.  This observation aligns with research suggesting that slower-paced interactions can improve usability.", "text": "In AI-assisted coding environments, this issue becomes even more pronounced. P1\u2019s experience illustrates how the continuous stream of AI-generated suggestions, conveyed through speech output, can overwhelm cognitive capacity. By choosing to \u201cturn speech off for a bit,\u201d P1 effectively creates a temporary barrier\u2014an AI timeout \u2014allowing them to pause, process information, and make decisions without the constant influx of new suggestions. This aligns with findings by Bigham et al. [ 16 ], who suggest that slower-paced interactions can enhance usability for visually impaired users.", "page": 13, "bbox": []}, "seg_154": {"summary": "Summary: A manuscript has been submitted to ACM for review.", "text": "Manuscript submitted to ACM", "page": 13, "bbox": []}, "seg_156": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 14, "bbox": []}, "seg_157": {"summary": "Summary: AI timeouts are not just for managing ghost text suggestions, but also for enabling periods of coding without any AI interference.  Participants, like P7, highlighted the need for extended uninterrupted coding sessions free from AI input. This allows developers to focus and work without AI distractions.", "text": "The concept of AI timeouts extends beyond merely managing ghost text suggestions. Some participants expressed a need for extended periods of uninterrupted coding without any AI input. For instance, P7 emphasized the importance of having the option to engage in uninterrupted coding sessions, stating:", "page": 14, "bbox": []}, "seg_158": {"summary": "Summary: This developer desires a coding mode that allows for uninterrupted coding within the IDE, even if it results in buggy code initially.  They prioritize getting their thoughts down and plan to address errors later using shortcuts, rather than having AI assistance interrupt their flow during the initial coding process.  Essentially, they want a \"get it done now, fix it later\" mode without AI interference.", "text": "\u201cWell, hey, can we get a mode that says, hey, I just want to get all my code done. I just want to write code. I don\u2019t care right now. I don\u2019t care how buggy it is right now. I really don\u2019t care. But I want to stay in the IDE because when I\u2019m done, I\u2019m going to use a shortcut and I\u2019m going to go through, I\u2019m going to find all my errors and I\u2019ll fix them, but I don\u2019t want it [AI assistant] to get in the way [...] But let me just get my thoughts out. \u201d - P7, Developer with no vision.", "page": 14, "bbox": []}, "seg_159": {"summary": "Summary: This text segment explores the issue of cognitive dissonance arising when AI assistance operates too quickly for the user's cognitive pace.  Participant P7's experience illustrates this point, suggesting a need for AI timeouts to avoid disrupting the user's thought process, while participant P1 offers a contrasting view of AI as a learning catalyst.  The segment highlights differing user experiences with AI assistance speed.", "text": "This experience highlights the potential for cognitive dissonance when AI assistance operates at a faster pace than the developer\u2019s thought process. Although the AI\u2019s suggestion may have been useful, it momentarily disrupted P7\u2019s train of thought, creating a mismatch between the AI\u2019s proactive assistance and the participant\u2019s cognitive workflow. While P7\u2019s experience underscores the need for AI timeouts, it is important to note that not all participants shared this sentiment. P1\u2019s reflection, for example, illustrates a different perspective\u2014one where AI acts as a catalyst for learning and expanding problem-solving approaches:", "page": 14, "bbox": []}, "seg_160": {"summary": "Summary: The developer was surprised by the AI assistant's approach to parsing and converting the date in their coding task, as it preempted a step they had planned to do later.  Initially, the developer focused on extracting the date and then calculating a year later, but the AI assistant first converted the date into a standard format, which the developer realized was a logical and efficient step.  This experience highlighted the AI assistant's ability to anticipate necessary steps in the coding process.", "text": "\u201cI think it [AI assistant] was a couple steps ahead of me now and again because I was sort of just expecting, okay, I need to get that date [date requested in their coding task], and then I\u2019m going to need to somehow get a year later out of it. And I didn\u2019t actually really consider that you\u2019d have to sort of parse that date and convert it into an actual date first. [...] So when it [AI assistant] made a constructor and just transformed it into month, day, year, I\u2019m like, okay, well, that\u2019s not immediately what I had in mind, but on second thought, that does make a lot of sense. [...] I probably would have done that as a next step myself, but it just sort of preempted me on there.\u201d - P1, Developer with no vision.", "page": 14, "bbox": []}, "seg_161": {"summary": "Summary: This text highlights the connection between AI coding assistants and Vygotsky's Zone of Proximal Development, emphasizing that AI should guide users to more advanced solutions.  It stresses the importance of dynamically adjusting AI intervention to balance cognitive challenge and prevent user overwhelm.  Adaptive AI assistance is crucial for creating an enriching coding experience that supports problem-solving without causing excessive cognitive load.", "text": "This experience aligns with Vygotsky\u2019s concept of the Zone of Proximal Development [ 102 ], which suggests that learning is most effective when guidance bridges the gap between what a learner can do independently and what they can achieve with support. In this case, the AI guided P1 toward a more advanced solution. However, maintaining a balance between beneficial cognitive challenges and overwhelming disruptions is crucial. While this instance demonstrated a positive outcome, it highlights the need for AI systems that can dynamically adjust their level of intervention based on the user\u2019s expertise and cognitive load. By incorporating adaptive assistance, AI coding assistants can create a more effective and enriching coding experience\u2014one where developers are both supported and challenged in a way that enhances their problem-solving abilities without causing excessive cognitive overload.", "page": 14, "bbox": []}, "seg_162": {"summary": "Summary: P4's response reveals that AI assistants can promote the early adoption of best coding practices among developers. Additionally, AI timeouts can be beneficial by providing developers with time to consider various aspects of their code before applying AI suggestions. These insights highlight the potential of AI to improve coding workflows and developer reflection.", "text": "Conversely, P4\u2019s response to how AI suggestions influenced their planning highlights two key insights. First, AI assistants can encourage developers to adopt best coding practices earlier in the development process. Second, AI timeouts could play a role in giving developers the necessary time to reflect on different aspects of their program before implementing AI-generated suggestions:", "page": 14, "bbox": []}, "seg_163": {"summary": "Summary: A developer with low vision appreciated the AI assistant suggesting guard clauses, which are best practices often considered during unit testing.  Inspired by Bob Martin's unit testing hat analogy, the developer indicated that the AI helped them think about these important edge cases earlier in the development process than they normally would.  The developer welcomed this proactive assistance from the AI.", "text": "\u201c...[I would] not have remembered to put those guard clauses in until I got, like, my unit testing hat on [the participant was pointing to AI-generated guard clauses, a best practice for handling edge cases]. I\u2019ve seen some videos of like, I think Uncle Bob, Bob Martin did some unit testing videos where he literally had a hat that he would flip around and it was two hats put together and he\u2019d be like, coder, unit test, or coder and those kind of things of like, oh, I wasn\u2019t there yet [was not thinking about guard clauses yet], but if you [AI assistant] want to help me get there early, that\u2019s fine. That\u2019s great.\u201d - P4, Developer with low vision.", "page": 14, "bbox": []}, "seg_164": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 14, "bbox": []}, "seg_166": {"summary": "This text segment discusses the impact of generative AI coding assistants on developers who are visually impaired.  It likely explores how these tools affect their coding practices and workflows, potentially highlighting both benefits and challenges.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 15, "bbox": []}, "seg_167": {"summary": "Summary: AI has the potential to merge traditional coding phases by offering proactive suggestions, as seen with guard clauses eliminating the need for timeouts between coding and testing.  However, developers have varied workflow preferences, necessitating flexible AI timeouts that allow control over AI interventions rather than complete disengagement.  This adaptability is crucial for accommodating diverse coding styles and needs, particularly for developers with visual impairments who may rely on structured routines.", "text": "This reflection highlights how AI can blur the traditional boundaries between different coding phases, potentially reducing the need for certain AI timeouts. In this case, the AI\u2019s proactive suggestions for guard clauses eliminated the need for a \u201ctimeout\u201d between the coding and testing phases by integrating best practices early in the development process. However, this scenario also emphasizes the importance of \u201cflexible AI timeouts\u201d. While P4 valued the early inclusion of guard clauses, other developers might prefer to maintain distinct mental phases in their coding workflow. In this context, AI timeouts do not necessarily mean disengaging from AI assistance entirely but rather enabling developers to control when and how AI interventions occur. By allowing developers to schedule AI timeouts or receive advanced suggestions at specific points in their workflow, AI-assisted coding tools can accommodate different coding styles and personal preferences. This flexibility could be particularly beneficial for developers who are visually impaired, as they may have structured routines or mental models for managing distinct phases of the coding process [ 42 , 50 ]. Providing adaptive AI interaction could enhance productivity without disrupting established workflows.", "page": 15, "bbox": []}, "seg_168": {"summary": "Summary: This text highlights the importance of carefully managing AI timeouts to achieve a balance.  The goal is for AI assistance to be beneficial and supportive, as illustrated by P3's experience, without becoming intrusive or hindering the user.  Therefore, a balanced approach is crucial to optimize AI's helpfulness and minimize potential disruptions.", "text": "P3\u2019s experience further illustrates the need for a balanced approach to AI timeouts, ensuring that AI assistance is helpful without becoming disruptive:", "page": 15, "bbox": []}, "seg_169": {"summary": "Summary: A developer with low vision finds the AI assistant very helpful, especially for tasks like datetime manipulation, which they often struggle to remember.  They appreciate the AI assistant's ability to understand the context of their programming language and quickly provide solutions, saving them time spent searching documentation. This allows them to bypass the need to repeatedly consult online resources.", "text": "\u201cHonestly, I think it\u2019s [AI assistant] super helpful because like, I\u2019m going to be honest, like you were saying before, I always forget datetime manipulation. [...] So having the ability to just say what I want, have it [AI assistant] understand the context of the programming language I\u2019m in and help me kind of get there quicker without having to kind of start searching around the web, figuring out the reading documentation again for the datetime methods. Like I have to all the time.\u201d - P3, developer with low vision.", "page": 15, "bbox": []}, "seg_170": {"summary": "Developers find AI useful for tasks like syntax recall and code manipulation, providing helpful support during coding.  However, for complex problem-solving and creative tasks, developers may prefer to temporarily disable AI to work independently. This suggests a nuanced approach to AI adoption, leveraging its strengths while maintaining autonomy for certain tasks.", "text": "As the quote illustrates, developers may choose to keep the AI when it provides helpful support, such as recalling correct syntax or assisting with complex code manipulations. However, for tasks requiring deep problem-solving or creative thinking, they may prefer to use AI timeouts to independently work through solutions.", "page": 15, "bbox": []}, "seg_171": {"summary": "The concept of \"AI timeouts\" emerges from contradictions in developer workflows when using tools like Copilot.  While intended to assist coding, Copilot's constant interventions can disrupt developers who need uninterrupted periods for processing and structuring information. This highlights the need for AI systems to be more adaptable, calibrating their assistance based on individual user needs for periods of focused work.", "text": "The concept of \u201cAI timeouts\u201d reflects contradictions within the activity system, arising from misalignments between the subject (developers), the object (coding tasks), and the tool (Copilot). While Copilot serves as a mediating artifact designed to facilitate coding, its dynamic and sometimes intrusive interventions create tensions that can disrupt developers\u2019 workflows. This contradiction likely arises because the tool\u2019s design assumes that continuous AI assistance improves productivity, whereas developers might actually need uninterrupted periods to process and structure information. These tensions highlight the necessity of adaptable AI systems that calibrate their level of intervention based on the user\u2019s needs.", "page": 15, "bbox": []}, "seg_172": {"summary": "Summary:  The study found that AI coding assistants enhance coding efficiency for visually impaired developers, despite challenges like context switching. This efficiency improvement is attributed to two main factors.  These findings highlight a key benefit of AI tools in coding accessibility.", "text": "4.1.4 AI-Assisted Coding Efficiency . While our study highlighted challenges like context switching, it also revealed key benefits of AI coding assistants, particularly in enhancing coding efficiency for developers who are visually impaired. This efficiency improvement stemmed from two main factors:", "page": 15, "bbox": []}, "seg_173": {"summary": "Summary: Copilot offers proactive code generation, minimizing manual coding efforts.  Additionally, it serves as an accessible and consistently available coding partner, providing immediate and non-judgmental assistance.", "text": "(1) Copilot\u2019s Proactive Code Generation , which anticipated and generated relevant code, reducing manual effort. (2) Copilot as an Accessible, Always-Available Coding Partner , providing instant non-judgmental support.", "page": 15, "bbox": []}, "seg_174": {"summary": "Proactive code generation in the AI assistant significantly boosts user productivity by anticipating their coding needs.  Participants noted that the AI proactively generated code to resolve issues or introduce new features, sometimes even before the user explicitly considered them.  For example, one participant described the AI intelligently parsing and formatting a date, which streamlined their workflow.", "text": "Proactive Code Generation . Participants highlighted how the AI assistant boosted their productivity by anticipating their needs\u2014proactively generating code that resolved existing issues or introduced features they had not initially considered. For instance, P7 described a moment when the AI assistant demonstrated foresight by automatically parsing and transforming a date into a usable format. Upon reflection, P7 realized that this step aligned perfectly with their next intended action, streamlining their workflow and reducing the need for manual adjustments:", "page": 15, "bbox": []}, "seg_175": {"summary": "Summary: A manuscript has been submitted to ACM.", "text": "Manuscript submitted to ACM", "page": 15, "bbox": []}, "seg_177": {"summary": "Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 16, "bbox": []}, "seg_178": {"summary": "Summary: A developer using an AI assistant for a coding task realized the AI was more efficient by parsing the user-provided date and converting it into a date object, a step the developer had not initially considered.  The developer acknowledged the AI's approach was logical and something they would have likely done later, but the AI preempted them in the process. This highlights the AI's ability to anticipate necessary steps in coding tasks.", "text": "\u201cIt [AI assistant] was a couple steps ahead of me. [The participant\u2019s coding task involved calculating the number of days until a birthday provided by an end-user.] I was initially focused on getting the date and calculating a year later, but I didn\u2019t actually really consider that you\u2019d have to sort of parse that date [date given by the end-user] and convert it into an actual date first [converting to an actual date was something the generative AI did for them]. That makes perfect sense in hindsight [...] So when it [AI assistant] made it a constructor and just transformed it into month, day, year, I\u2019m like, okay, well, that\u2019s not immediately what I had in mind, but on second thought, that does make a lot of sense. [...] I probably would have done that as a next step myself, but it just sort of preempted me on there.\u201d - P7, Developer with no vision.", "page": 16, "bbox": []}, "seg_179": {"summary": "Summary: Participant P1 valued the AI's proactive code generation capabilities.  They specifically highlighted the AI's ability to anticipate their development steps and align with their intended coding goals. This proactive nature of the AI was seen as a significant benefit.", "text": "Similarly, P1 emphasized their appreciation for the AI\u2019s ability to proactively generate code, particularly in how it anticipated their next steps and aligned with their intentions as a developer:", "page": 16, "bbox": []}, "seg_180": {"summary": "Summary: This developer is experimenting with Copilot's \"auto-modify\" feature and is impressed with the AI's code generation capabilities.  Upon reviewing the AI-generated code, the developer expresses astonishment, stating it is \"scarily good\" and exactly what they would have done themselves. This highlights the perceived accuracy and usefulness of the AI tool in coding tasks.", "text": "\u201cI\u2019m gonna try something else. I\u2019m actually very curious [...] I\u2019ll auto modify that [The participant used a Copilot command to \u2019auto-modify\u2019 their code, allowing the AI to proactively edit and expand their initial work]. That sounds good [The participant was reviewing the code generated by the AI\u201d]. There are days until your next birthday. Yeah, format days until next birthday [The participant continued reviewing the code generated by the AI.] That [code generated by the AI] is scarily good. Actually that [AI generated code] is exactly what I was going to do.\u201d - P1, Developer with no vision.", "page": 16, "bbox": []}, "seg_181": {"summary": "Developers with visual impairments face challenges in traditional coding environments, requiring them to navigate complex code structures and documentation. AI-driven code generation offers a promising solution by proactively predicting coding needs and generating relevant snippets. This technology can significantly reduce cognitive load and improve efficiency for visually impaired developers by minimizing the need for extensive manual code exploration.", "text": "Prior research has documented the challenges that developers with visual impairments face when navigating and understanding code structures in traditional development environments [ 5 , 11 , 78 ]. Given these challenges, we argue that AI\u2019s proactive code generation holds potential for this population. Developers who are visually impaired often need to traverse multiple documentation pages or code files to construct appropriate solutions\u2014a process that is not only time-consuming but also more prone to errors when relying on screen readers [ 6 , 20 ]. In this context, AI\u2019s ability to predict coding needs and generate relevant code snippets is especially valuable, as it can reduce cognitive load and enhance efficiency for developers who are visually impaired.", "page": 16, "bbox": []}, "seg_182": {"summary": "Participants viewed the AI assistant as a supportive and always-available coding partner that enhanced their productivity. They appreciated its reliability in providing quick answers and relevant code snippets, likening it to a knowledgeable colleague ready to assist with coding challenges.", "text": "AI as an Accessible, Always-Available Coding Partner. Some participants described the AI assistant as a supportive \u2019buddy\u2019 that guided them through their coding tasks, providing assistance and enhancing their productivity. They appreciated having an AI-powered companion they could rely on for coding assistance, providing quick answers and relevant code snippets when needed. For example, Participant P3 likened the AI assistant to a knowledgeable colleague who is readily available to offer solutions and assist with coding challenges:", "page": 16, "bbox": []}, "seg_183": {"summary": "Summary: A developer with low vision describes using an AI coding assistant as a helpful \"buddy\" for quickly answering coding questions.  They specifically mention using it to recall functions and methods, such as those needed for date-related coding tasks. This highlights the AI assistant's role in providing immediate coding support.", "text": "\u201c[with the AI coding assistant] you have that kind of buddy to kind of ask real quick, you know, what was that? You know, what was the daytime method that I needed to use to convert that particular string into the right thing? [The participant refers to asking the AI about functions and methods needed for the date-related coding task]\u201d - P3, Developer with low vision.", "page": 16, "bbox": []}, "seg_184": {"summary": "Summary: Participants generally found the AI assistant helpful for software development tasks.  While all agreed on its benefits, the extent to which they relied on the assistant differed, with some individuals using it more extensively than others.  For example, participant P6 used the AI assistant to complete almost all of their assigned work.", "text": "All participants agreed that the AI assistant was beneficial in helping them complete their software development tasks. However, the degree of reliance on the assistant varied among individuals. Some, like P6, depended on it extensively, using it to complete nearly their entire assigned workload:", "page": 16, "bbox": []}, "seg_185": {"summary": "Summary: A study participant utilized Copilot, an AI assistant, to complete a coding task by simply copying and pasting the task instructions into the chat interface. The AI provided a nearly complete implementation of the task, requiring only very minor adjustments, and even handled complex details like date formats correctly. This suggests the AI assistant was highly effective in generating code based on task descriptions.", "text": "\u201cI just copy and pasted the whole task [instructions for a coding task assigned in our study] into chat [Copilot\u2019s chat interface], and it [AI assistant] gave me the whole implementation that we had to do. Very, very minor tweaks left to do. I mean, the date formats worked out, right? [The participant was assigned a coding task", "page": 16, "bbox": []}, "seg_186": {"summary": "Summary: Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 16, "bbox": []}, "seg_188": {"summary": "Summary: This text segment discusses the effects of generative AI coding assistants on developers with visual impairments.  It will likely explore the ways in which these tools impact their coding practices and professional experiences.", "text": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired", "page": 17, "bbox": []}, "seg_189": {"summary": "Summary: A developer with low vision used an AI assistant to help with coding related to date formats.  The developer indicated that the AI assistant automated most of the coding process, requiring them only to add comments and make minor adjustments. This highlights the AI's capability to handle complex coding tasks efficiently.", "text": "involving date formats.] Essentially we just added comments and tinkered with the code [...] But otherwise, it [AI assistant] just did everything for us.\u201d - P6, Developer with low vision.", "page": 17, "bbox": []}, "seg_190": {"summary": "Summary: Participants with limited experience using AI assistants recognized the potential of these tools to speed up coding tasks.  Their participation in the study increased their interest in exploring AI technology further.", "text": "Even participants with limited experience using AI assistants, such as P9, acknowledged its potential to accelerate coding tasks. Their experience in our study sparked greater interest in exploring the technology further:", "page": 17, "bbox": []}, "seg_191": {"summary": "Summary: The developer is interested in further exploring AI coding assistants and believes they have potential for quickly writing functions and completing tasks.  They highlight the time-saving benefits of using these tools for developers.", "text": "\u201cI would love to try and explore more [about AI coding assistants]. But, it [the AI coding assistant] has definitely a potential [...] just doing quick work, especially writing quick functions and doing also all those tasks [...] It\u2019s [the AI assistant is] bringing a lot of time saving.\u201d - P9, Developer with no vision.", "page": 17, "bbox": []}, "seg_192": {"summary": "Generative AI assistants offer significant benefits to visually impaired developers by providing constant, non-judgmental support, thus increasing their independence and efficiency in software development, particularly in workplace settings where asking for help might be challenging. This technology can make software development more accessible for these developers by reducing reliance on coworkers for assistance.", "text": "For developers who are visually impaired, this aspect of AI assistance could be particularly valuable, as it offers constant, non-judgmental support without requiring face-to-face interaction. This can be especially beneficial in workplace environments where they may feel hesitant to frequently ask colleagues for help [ 6 ]. Generative AI assistants have the potential to make software development more accessible and efficient, enabling developers to work more independently without relying on coworkers for assistance.", "page": 17, "bbox": []}, "seg_193": {"summary": "Summary: From an Activity Theory perspective, Copilot transforms coding by acting as an active collaborator, not just a passive tool. This AI assistant fosters a dynamic interaction where AI suggestions proactively guide problem-solving, functioning as a readily available \"buddy\" to reduce cognitive load and enhance developer confidence.", "text": "From the perspective of Activity Theory, Copilot serves as a mediating artifact that reshapes the coding activity by shifting its role from a passive tool to an active collaborator. Within the framework of Activity Theory, the interaction between the subject (developer), object (coding task), and tool (AI assistant) is no longer a linear process but a dynamic, adaptive exchange. Instead of developers following a rigid, step-by-step workflow, Copilot enables a more fluid interaction where AI-generated suggestions proactively shape the problem-solving process. Acting as a readily available and non-judgmental coding companion, Copilot can function as a \u201cbuddy\u201d that developers can turn to at any moment to clarify uncertainties, propose solutions, and reduce cognitive load. By integrating AI as an active collaborator within the activity system, Copilot can help developers navigate complex programming tasks with greater confidence and independence.", "page": 17, "bbox": []}, "seg_194": {"summary": "Insufficient content for summary.", "text": "5 DISCUSSION", "page": 17, "bbox": []}, "seg_195": {"summary": "AI coding assistants offer significant advantages for visually impaired developers, including enhanced capabilities and control. However, these tools also present accessibility challenges, particularly in context switching and managing AI suggestions.  The text emphasizes the need for a new approach to accessibility in AI coding environments and proposes design recommendations to make these tools beneficial for all developers.", "text": "Our findings show that AI coding assistants provide powerful capabilities, new opportunities, and greater control for developers who are visually impaired, but they also introduce new accessibility challenges. Issues such as context switching and managing AI-generated suggestions highlight the need for a new approach to accessibility in AI coding environments. Building on Nielsen\u2019s argument that generative AI has introduced a new paradigm for HCI [ 68 ] and the design principles for generative AI tools outlined by Weisz et al. [ 104 ], we propose design recommendations to ensure AI coding assistants are accessible and beneficial for all developers.", "page": 17, "bbox": []}, "seg_196": {"summary": "Summary: This section likely discusses how AI coding assistants manage control flow and context switching during code generation and interaction with developers.  Effective management of these aspects is crucial for the usability and efficiency of AI coding assistants in complex coding tasks.  The discussion will likely delve into the specific techniques and challenges related to AI control and context within these tools.", "text": "5.1 AI Control and Context-Switching Management in AI Coding Assistants", "page": 17, "bbox": []}, "seg_197": {"summary": "AI coding assistants offer visually impaired developers the advantage of focusing on strategic thinking; however, they also present challenges.  Specifically, navigating between static code and dynamic AI windows causes frustration and disrupts workflow due to constant context switching, hindering cognitive focus. This mixed experience highlights areas for improvement in AI assistant design for visually impaired developers.", "text": "Our study highlights the mixed experiences of developers who are visually impaired when using AI coding assistants. While these assistants empower developers by allowing them to focus on higher-level strategic thinking, they also introduce new challenges. Participants, such as P5 and P9, reported difficulties in navigating between their static code and the dynamic windows used for AI interactions. This constant context switching led to frustration, disrupting their workflow and breaking their cognitive focus.", "page": 17, "bbox": []}, "seg_198": {"summary": "Visually oriented Integrated Development Environments (IDEs) present significant challenges for visually impaired developers, often resulting in a loss of control due to their reliance on visual interfaces.  Screen readers, used by these developers, further complicate matters by requiring sequential code reading, which hinders context switching.  Tools like StructJumper and CodeTalk have been created to mitigate these navigation issues in static coding.", "text": "Previous research highlights that the visually oriented nature of Integrated Development Environments (IDEs) presents challenges for developers who are visually impaired, often leading to a loss of control [ 6 , 22 , 96 ]. P1 also noted that screen readers require users to read code sequentially, making context switching particularly difficult [ 5 \u2013 7 , 59 , 90 ]. Several tools, such as StructJumper and CodeTalk, have been designed to aid navigation in traditional static coding", "page": 17, "bbox": []}, "seg_199": {"summary": "Insufficient content for summary.", "text": "Manuscript submitted to ACM", "page": 17, "bbox": []}, "seg_201": {"summary": "Summary: Insufficient content for summary.", "text": "Flores-Saviaga, et al.", "page": 18, "bbox": []}}