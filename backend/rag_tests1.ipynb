{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized model: gemini-2.0-flash-thinking-exp-01-21\n",
      "\n",
      "--- Invoking with simple prompt: ---\n",
      "Explain the concept of Chain-of-Thought prompting in 1-2 sentences.\n",
      "\n",
      "--- Response ---\n",
      "Chain-of-Thought prompting improves reasoning in large language models by prompting them to generate intermediate reasoning steps, like showing their work, before arriving at a final answer.  This is achieved by demonstrating step-by-step reasoning in the prompt examples, which encourages the model to apply a similar thought process to new questions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "MODEL_NAME = \"gemini-2.0-flash-thinking-exp-01-21\"\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables. \"\n",
    "                     \"Please set it in your .env file or system environment.\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "        model=MODEL_NAME,\n",
    "        google_api_key=api_key, # Can omit if GOOGLE_API_KEY env var is set\n",
    "        temperature=0.7,       # Adjust creativity (0.0 - 1.0)\n",
    "        # top_p=0.9,           # Optional: nucleus sampling\n",
    "        # top_k=40,            # Optional: top-k sampling\n",
    "        # max_output_tokens=1024 # Optional: Limit response length\n",
    "    )\n",
    "print(f\"Successfully initialized model: {MODEL_NAME}\")\n",
    "\n",
    "# Example 1: Simple invocation with a string prompt\n",
    "try:\n",
    "    prompt = \"Explain the concept of Chain-of-Thought prompting in 1-2 sentences.\"\n",
    "    print(f\"\\n--- Invoking with simple prompt: ---\\n{prompt}\")\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\\n--- Response ---\")\n",
    "    print(response.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during invocation: {e}\")\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            print(f\"Successfully loaded JSON from {file_path}\")\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to convert JSON data to documents\n",
    "def json_to_documents(json_data, metadata=None):\n",
    "    if not metadata:\n",
    "        metadata = {}\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    # Handle different JSON structures\n",
    "    if isinstance(json_data, list):\n",
    "        # For array of objects like Huridocs output\n",
    "        for i, item in enumerate(json_data):\n",
    "            if isinstance(item, dict):\n",
    "                # Extract text if available\n",
    "                text = item.get('text', str(item))\n",
    "                # Create metadata with item-specific info\n",
    "                item_metadata = metadata.copy()\n",
    "                item_metadata.update({\n",
    "                    'index': i,\n",
    "                    'page_number': item.get('page_number', 'unknown'),\n",
    "                    'type': item.get('type', 'unknown')\n",
    "                })\n",
    "                documents.append(Document(page_content=text, metadata=item_metadata))\n",
    "    elif isinstance(json_data, dict):\n",
    "        # For single objects\n",
    "        for key, value in json_data.items():\n",
    "            if isinstance(value, str):\n",
    "                item_metadata = metadata.copy()\n",
    "                item_metadata['key'] = key\n",
    "                documents.append(Document(page_content=value, metadata=item_metadata))\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                # Recursively process nested structures\n",
    "                nested_docs = json_to_documents(value, {**metadata, 'parent_key': key})\n",
    "                documents.extend(nested_docs)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Function to create a vector store from documents\n",
    "def create_vector_store(documents):\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create embeddings using Google's embedding model\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = FAISS.from_documents(splits, embeddings)\n",
    "    print(f\"Created vector store with {len(splits)} document chunks\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Function to query the vector store\n",
    "def query_json_data(vector_store, query_text, k=3):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = vector_store.similarity_search(query_text, k=k)\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Create prompt template\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an assistant that answers questions based on the provided context.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer the question based only on the provided context. If the context doesn't contain \n",
    "    the information needed to answer the question, say \"I don't have enough information to \n",
    "    answer this question based on the provided context.\"\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create chain\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Execute chain\n",
    "    response = chain.invoke({\"context\": context, \"question\": query_text})\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"source_documents\": retrieved_docs\n",
    "    }\n",
    "\n",
    "sample_file = \"./huridocs_output/095937acc2bd0f790c11b8a3e80a7e8d2c99e555dfb49edf3599a1533dcbc19c.json\"\n",
    "json_data = load_json_file(sample_file)\n",
    "documents = json_to_documents(json_data, {\"source_file\": sample_file})\n",
    "vector_store = create_vector_store(documents)\n",
    "\n",
    "def query_print_results(query, vector_store):\n",
    "    print(f\"\\n--- Querying: {query} ---\")\n",
    "    result = query_json_data(vector_store, query)\n",
    "\n",
    "    print(\"\\n--- Response ---\")\n",
    "    print(result[\"response\"])\n",
    "\n",
    "    print(\"\\n--- Sources ---\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"]):\n",
    "        print(f\"Source {i+1}:\")\n",
    "        print(f\"Content: {doc.page_content[:100]}...\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "query_print_results(\"What's in this document?\", vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Example usage\n",
    "# if os.path.exists(\"./huridocs_output\"):\n",
    "#     # Load a sample JSON file\n",
    "#     sample_file = \"./huridocs_output/bf6f80ae6c08aa62fd2de9f1d14f2606110fcc5ce0d5cd019c19a766bf3558f1.json\"\n",
    "#     json_data = load_json_file(sample_file)\n",
    "    \n",
    "#     if json_data:\n",
    "#         # Convert to documents\n",
    "#         documents = json_to_documents(json_data, {\"source_file\": sample_file})\n",
    "#         print(f\"Created {len(documents)} documents from JSON data\")\n",
    "        \n",
    "#         # Create vector store\n",
    "#         vector_store = create_vector_store(documents)\n",
    "        \n",
    "        # # Query example\n",
    "        # query = \"What is the title of this document?\"\n",
    "        # print(f\"\\n--- Querying: {query} ---\")\n",
    "        # result = query_json_data(vector_store, query)\n",
    "        \n",
    "        # print(\"\\n--- Response ---\")\n",
    "        # print(result[\"response\"])\n",
    "        \n",
    "        # print(\"\\n--- Sources ---\")\n",
    "        # for i, doc in enumerate(result[\"source_documents\"]):\n",
    "        #     print(f\"Source {i+1}:\")\n",
    "        #     print(f\"Content: {doc.page_content[:100]}...\")\n",
    "        #     print(f\"Metadata: {doc.metadata}\")\n",
    "#             print()\n",
    "# else:\n",
    "#     print(\"Huridocs output directory not found. Please adjust the path to your JSON files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON from ./huridocs_output/095937acc2bd0f790c11b8a3e80a7e8d2c99e555dfb49edf3599a1533dcbc19c.json\n",
      "Created vector store with 7 document chunks\n"
     ]
    }
   ],
   "source": [
    "# sample_file = \"./huridocs_output/bf6f80ae6c08aa62fd2de9f1d14f2606110fcc5ce0d5cd019c19a766bf3558f1.json\"\n",
    "sample_file = \"./huridocs_output/095937acc2bd0f790c11b8a3e80a7e8d2c99e555dfb49edf3599a1533dcbc19c.json\"\n",
    "json_data = load_json_file(sample_file)\n",
    "documents = json_to_documents(json_data, {\"source_file\": sample_file})\n",
    "vector_store = create_vector_store(documents)\n",
    "\n",
    "def query_print_results(query, vector_store):\n",
    "    print(f\"\\n--- Querying: {query} ---\")\n",
    "    result = query_json_data(vector_store, query)\n",
    "\n",
    "    print(\"\\n--- Response ---\")\n",
    "    print(result[\"response\"])\n",
    "\n",
    "    print(\"\\n--- Sources ---\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"]):\n",
    "        print(f\"Source {i+1}:\")\n",
    "        print(f\"Content: {doc.page_content[:100]}...\")\n",
    "        print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Querying: What's in this document? ---\n",
      "\n",
      "--- Response ---\n",
      "Answer: This document contains text about Lorem Ipsum, and it seems to include elements such as lists, tables, and figures. It is titled \"Document Big Centered Title\".\n",
      "\n",
      "--- Sources ---\n",
      "Source 1:\n",
      "Content: Some text. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum ha...\n",
      "Metadata: {'source_file': './huridocs_output/095937acc2bd0f790c11b8a3e80a7e8d2c99e555dfb49edf3599a1533dcbc19c.json', 'index': 1, 'page_number': 1, 'type': 'Text'}\n",
      "Source 2:\n",
      "Content: ● LIST ● TABLE ● FIGURE...\n",
      "Metadata: {'source_file': './huridocs_output/095937acc2bd0f790c11b8a3e80a7e8d2c99e555dfb49edf3599a1533dcbc19c.json', 'index': 6, 'page_number': 1, 'type': 'List item'}\n",
      "Source 3:\n",
      "Content: Document Big Centered Title...\n",
      "Metadata: {'source_file': './huridocs_output/095937acc2bd0f790c11b8a3e80a7e8d2c99e555dfb49edf3599a1533dcbc19c.json', 'index': 0, 'page_number': 1, 'type': 'Section header'}\n"
     ]
    }
   ],
   "source": [
    "query_print_results(\"What's in this document?\", vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speakeasy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
