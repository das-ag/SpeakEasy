{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3/envs/speakeasy_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - langchain\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-arm64::certifi-2025.1.3~ --> conda-forge/noarch::certifi-2025.1.31-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# !conda install -y jupyter notebook ipykernel \n",
    "# !conda install -y langchain -c conda-forge\n",
    "# %pip install -U langchain-google-genai\n",
    "# %pip install langchain_community\n",
    "# %pip install faiss-cpu OR %pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized model: gemini-2.0-flash-thinking-exp-01-21\n",
      "\n",
      "--- Invoking with simple prompt: ---\n",
      "Explain the concept of Chain-of-Thought prompting in 1-2 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/speakeasy_env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response ---\n",
      "Chain-of-Thought prompting encourages large language models to generate intermediate reasoning steps before arriving at a final answer, mimicking human thought processes and improving performance on complex tasks.  By explicitly showing the model *how* to think, it can solve problems requiring multi-step reasoning more effectively.\n",
      "\n",
      "--- Invoking with messages: ---\n",
      "[SystemMessage(content='You are a helpful assistant that provides concise explanations.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/speakeasy_env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response ---\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "MODEL_NAME = \"gemini-2.0-flash-thinking-exp-01-21\"\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables. \"\n",
    "                     \"Please set it in your .env file or system environment.\")\n",
    "\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=MODEL_NAME,\n",
    "        google_api_key=api_key, # Can omit if GOOGLE_API_KEY env var is set\n",
    "        temperature=0.7,       # Adjust creativity (0.0 - 1.0)\n",
    "        # top_p=0.9,           # Optional: nucleus sampling\n",
    "        # top_k=40,            # Optional: top-k sampling\n",
    "        # max_output_tokens=1024 # Optional: Limit response length\n",
    "        convert_system_message_to_human=True # Often useful for Gemini\n",
    "    )\n",
    "    print(f\"Successfully initialized model: {MODEL_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing model '{MODEL_NAME}': {e}\")\n",
    "    print(\"This could be due to several reasons:\")\n",
    "    print(\"1. The experimental model name is incorrect or deprecated.\")\n",
    "    print(\"2. Your API key does not have access to this specific experimental model.\")\n",
    "    print(\"3. The model is temporarily unavailable or restricted by region.\")\n",
    "    print(\"Consider trying a stable model like 'gemini-1.5-flash-latest' or 'gemini-1.5-pro-latest'.\")\n",
    "    exit() # Exit if initialization fails\n",
    "\n",
    "# --- Use the Model (Example Invocations) ---\n",
    "\n",
    "# Example 1: Simple invocation with a string prompt\n",
    "try:\n",
    "    prompt = \"Explain the concept of Chain-of-Thought prompting in 1-2 sentences.\"\n",
    "    print(f\"\\n--- Invoking with simple prompt: ---\\n{prompt}\")\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\\n--- Response ---\")\n",
    "    print(response.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during invocation: {e}\")\n",
    "\n",
    "\n",
    "# Example 2: Invocation with message history (System + Human)\n",
    "try:\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that provides concise explanations.\"),\n",
    "        HumanMessage(content=\"What is the capital of France?\"),\n",
    "    ]\n",
    "    print(f\"\\n--- Invoking with messages: ---\")\n",
    "    print(messages)\n",
    "    response = llm.invoke(messages)\n",
    "    print(\"\\n--- Response ---\")\n",
    "    print(response.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during invocation with messages: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON from ./huridocs_output/bf6f80ae6c08aa62fd2de9f1d14f2606110fcc5ce0d5cd019c19a766bf3558f1.json\n",
      "Created 106 documents from JSON data\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Load and Query JSON Data with LangChain and Google LLM ---\n",
    "import os\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Function to load JSON data from a file\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            print(f\"Successfully loaded JSON from {file_path}\")\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to convert JSON data to documents\n",
    "def json_to_documents(json_data, metadata=None):\n",
    "    if not metadata:\n",
    "        metadata = {}\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    # Handle different JSON structures\n",
    "    if isinstance(json_data, list):\n",
    "        # For array of objects like Huridocs output\n",
    "        for i, item in enumerate(json_data):\n",
    "            if isinstance(item, dict):\n",
    "                # Extract text if available\n",
    "                text = item.get('text', str(item))\n",
    "                # Create metadata with item-specific info\n",
    "                item_metadata = metadata.copy()\n",
    "                item_metadata.update({\n",
    "                    'index': i,\n",
    "                    'page_number': item.get('page_number', 'unknown'),\n",
    "                    'type': item.get('type', 'unknown')\n",
    "                })\n",
    "                documents.append(Document(page_content=text, metadata=item_metadata))\n",
    "    elif isinstance(json_data, dict):\n",
    "        # For single objects\n",
    "        for key, value in json_data.items():\n",
    "            if isinstance(value, str):\n",
    "                item_metadata = metadata.copy()\n",
    "                item_metadata['key'] = key\n",
    "                documents.append(Document(page_content=value, metadata=item_metadata))\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                # Recursively process nested structures\n",
    "                nested_docs = json_to_documents(value, {**metadata, 'parent_key': key})\n",
    "                documents.extend(nested_docs)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Function to create a vector store from documents\n",
    "def create_vector_store(documents):\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create embeddings using Google's embedding model\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = FAISS.from_documents(splits, embeddings)\n",
    "    print(f\"Created vector store with {len(splits)} document chunks\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Function to query the vector store\n",
    "def query_json_data(vector_store, query_text, k=3):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = vector_store.similarity_search(query_text, k=k)\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Create prompt template\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an assistant that answers questions based on the provided context.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer the question based only on the provided context. If the context doesn't contain \n",
    "    the information needed to answer the question, say \"I don't have enough information to \n",
    "    answer this question based on the provided context.\"\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create chain\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Execute chain\n",
    "    response = chain.invoke({\"context\": context, \"question\": query_text})\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"source_documents\": retrieved_docs\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if os.path.exists(\"./huridocs_output\"):\n",
    "    # Load a sample JSON file\n",
    "    sample_file = \"./huridocs_output/bf6f80ae6c08aa62fd2de9f1d14f2606110fcc5ce0d5cd019c19a766bf3558f1.json\"\n",
    "    json_data = load_json_file(sample_file)\n",
    "    \n",
    "    if json_data:\n",
    "        # Convert to documents\n",
    "        documents = json_to_documents(json_data, {\"source_file\": sample_file})\n",
    "        print(f\"Created {len(documents)} documents from JSON data\")\n",
    "        \n",
    "        # Create vector store\n",
    "        vector_store = create_vector_store(documents)\n",
    "        \n",
    "        # Query example\n",
    "        query = \"What is the title of this document?\"\n",
    "        print(f\"\\n--- Querying: {query} ---\")\n",
    "        result = query_json_data(vector_store, query)\n",
    "        \n",
    "        print(\"\\n--- Response ---\")\n",
    "        print(result[\"response\"])\n",
    "        \n",
    "        print(\"\\n--- Sources ---\")\n",
    "        for i, doc in enumerate(result[\"source_documents\"]):\n",
    "            print(f\"Source {i+1}:\")\n",
    "            print(f\"Content: {doc.page_content[:100]}...\")\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"Huridocs output directory not found. Please adjust the path to your JSON files.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speakeasy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
